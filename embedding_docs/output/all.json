[
  {
    "id": 1,
    "content": "\n\nINTRODUCTION | Espresso \nOverview \nEspresso is a global confirmation layer designed to provide Layer 2 (L2) chains with fast and \nreliable transaction \nconfirmations backed by Byzantine Fault Tolerance (BFT) consensus. \nKey Features \nFast Confirmations: \n- Utilizes HotShot consensus protocol to confirm transactions within seconds. - Enables high-\nspeed asset movement across \nintegrated chains with minimal risk. \nDecentralized Sequencing: \n- 100 nodes will operate the Espresso Network in a decentralized manner during Mainnet 0. - \nRollups can opt for transaction \nsequencing by these nodes instead of a centralized sequencer. - Plans to scale the number of \nnodes and enable permissionless proof-of-stake participation post-Mainnet 0. \nLow-Cost Data Availability: \n- Offers a cheaper alternative for data availability compared to Ethereum. - Provides strong data \navailability guarantees to enhance scalability for applications. \nIntegration Support \nEspresso facilitates integration with various stacks, including: \n- Arbitrum Orbit Chains \n- Cartesi Applications \n- OP Stack Chains \n- Polygon CDK \nRollup Types Supported \n- ZK Rollup \n- Optimistic Rollup \nAdditional Information \nEspresso's confirmation layer enhances cross-chain composability by providing near-instant \naccess to reliable \ninformation across all connected chains. \nIt helps prevent sequencer equivocation, protects against reorgs, and reduces finality risk in \nintent-based systems. \nThe network is currently in its Mainnet 0 release, supporting various applications. \nDocumentation and Resources \n\nFor detailed integration guides and further information on the inner workings of the Espresso \nNetwork, refer to the documentation section. \nConclusion: \nEspresso is designed to enhance transaction speed, reliability, and scalability for various \nblockchain applications, making it a versatile solution for developers and chains looking for \nefficient confirmation and data availability solutions. \n \nEspresso in the Modular Stack \nHow Espresso can optionally be used for data availability and sequencing \nThe Espresso Network has been designed with modularity in mind. We have seen that \ndevelopers are best able to innovate when they have flexibility around designing their stack.  \nEspresso offers several benefits for chain operators and their developers to choose from: \n• Confirmations: All chains that leverage Espresso benefit from fast, reliable \nconfirmations—replacing the need for users, bridges, and beyond to depend on \npreconfirmations that come from centralized sequencers. \n• Data availability: All chains using Espresso also benefit from highly efficient data \navailability offered by the Espresso Network. However, many of the chains that are using \nEspresso also choose to leverage another form of DA, such as EigenDA, Celestia, Avail, \nor Ethereum itself. We have designed Espresso to respect and to be additively \ncompatible with these choices.  \n• Decentralized sequencing: Chains integrating Espresso may additionally use it as a \ndecentralized sequencing layer, leveraging the network to determine the order of \ntransactions on the chain. However, this is not required: chains that use their own \nsovereign sequencers to determine transaction ordering can still benefit from \nEspresso’s confirmations. A few examples of how Ethereum rollups can use the \nEspresso network include: \n• A standard rollup or validium that uses the L1 or an alternative data availability solution, \nleverages its own centralized sequencer, and settles to Ethereum may leverage \nEspresso for confirmations. \n• A based rollup that uses Ethereum for DA, relies on the Ethereum proposer for \nsequencing, and looks to the L1 for settlement may also use Espresso for fast, reliable \nconfirmations. \n• A validium that leverages its own centralized sequencer can use Espresso for data \navailability and also for more robust confirmations than the preconfirmations that its \nsequencer could offer. \n• A rollup or validium that wants to decentralize its sequencing without using the L1 \nproposer may use the Espresso leader as its sequencer for any given round and use \nEspresso for confirmations – while using its own choice of data availability. \n• A rollup that uses Espresso for DA, sequencing, and confirmations is what we like to call \na caffeinated chain. \n\nWhile we only cover Ethereum rollups here, this also applies to sovereign rollups and beyond.  \n ",
    "filename": "1.intro.pdf"
  },
  {
    "id": 2,
    "content": "\n\nReading Confirmations from the Espresso Network \nThis guide introduces a Go project that reads from the Espresso Network and monitors for \nspecific transactions, filtering them by value and sender address. You can imagine this project \nas being a bot that alerts you when your wallet gets drained. \nOverview \nThe hackathon-example repository provides a foundation for building applications that interact \nwith the Espresso Network. You can use this project as a starting point for developing \napplications or more sophisticated monitoring and analysis tools. \nPrerequisites \nBefore proceeding with this guide, you'll need: \n• A running Arbitrum Orbit chain integrated with the Espresso Network, either locally or in \nthe cloud \n• Access to a Caffeinated node, which serves as your interface to the Espresso Network \nFor convenience, the Caffeinated node setup is available in the same espresso-build-\nsomething-real repository as the previous guide, under the caff-node directory. If you've already \ndeployed your rollup following the earlier instructions, you're ready to proceed with this \nmonitoring project. \nThe Caffeinated Node \nThe Caffeinated node is a full node and your interface to the Espresso Network. It processes \nHotshot blocks, maintains state, and provides a JSON-RPC endpoint for monitoring \ntransactions. The monitoring tool we'll build uses these capabilities to track specific \ntransactions based on sender address and value. \nSetup Instructions \nNavigate to the espresso-build-something-real repository and follow these steps to configure \nand run your caffeinated node locally: \n1. Update the config \nUpdate the caff-node/config/caff_node.json file with: \nCopy \n{ \n  \"chain\": { \n      \"id\": 1000000, \n      ... \n  }, \n  ... \n  \"next-hotshot-block\": 9999, \n\n  \"namespace\": 1000000, \n  \"parent-chain-node-url\": \"WEBSOCKET_ARBITRUM_SEPOLIA_RPC_URL\", \n}    \n• Namespace and Chain ID: Both should match your rollup chain ID. Update both \nvalues. \n• Next Hotshot Block: \no Find the latest Hotshot block height at \nhttps://explorer.decaf.testnet.espresso.network/. Update this value just before \nrunning the Caffeinated node to avoid resyncing the entire chain. \n• Parent Chain Node URL: Enter your Arbitrum Sepolia or mainnet RPC URL (must be a \nWebSocket URL). \n          Note: We recommend using a different RPC URL for the caff node to avoid hitting rate limits \n(free tier should be enough). \n2. Set Up and Run \n1. Copy Configuration Files: \no Copy the l2_chain_info.json file into the caff-node/config folder \no Copy the database folder from the repository root to the caff-node directory. \n          Note: Make sure the node is not running when you do this. \n2. Start the Services: \no First, run docker compose up at the root of the repository. \no Then, run docker compose up in the caff-node folder. \nSetting Up the Example Project \nInstructions can be found on the README to set this project up but we will go over the steps \nhere in more detail. \n1. Clone the Repository \nCopy \ngit clone https://github.com/EspressoSystems/hackathon-example --recursive \ncd hackathon-example \nIf you've already cloned the repository without the --recursive flag (e.g., through your IDE), you \ncan fetch the go-ethereum submodule with: \nCopy \ngit submodule update --init --recursive \nThe reason we are doing this is because we want to use a forked version of the go-ethereum \nlibrary from offchainlabs to interact with the caffeinated node. \n\n2. Install Dependencies \nCopy \ngo mod tidy \n3. Configure the Application \nYou should configure the following settings in the /config/config.json file: \nCopy \n{ \n    \"caff_node_url\": \"http://YOUR_HOST:8550\", \n    \"polling_interval\": 1, \n    \"value\": 1, \n    \"from\": \"0x0000000000000000000000000000000000000000\" \n} \no caff_node_url: Change YOUR_HOST to either: \n▪ localhost if running locally. \n▪ Your EC2 instance's IPv4 address if running on the cloud. \no polling_interval: \n▪ Controls how frequently the application checks for new transactions. \n▪ Adjust based on your needs, but keep it low enough to avoid missing \ntransactions. \no value: Set the minimum transaction value (in wei) that you want to monitor. \no from: Enter the Ethereum address you want to monitor transactions from. \n          Note: The docker setup exposes the Caffeinated node on port 8550.           Note: The code \ndivides the polling value by 2 to determine the actual polling interval. \n4. Run the Application At the root of the repository, run the following command: \nCopy \ngo run . \nThis will start the application and monitor the Espresso Network for transactions that match \nyour rollup and the specified criteria. \n          Note: This monitoring tool simply logs matching transactions to the console. You can extend \nit to perform more complex actions when transactions are detected. \n5. Send Transactions \nNow that you have both your rollup and Caffeinated node running, you can send test \ntransactions to your rollup. You have two options: \n\no Use a simple command-line transaction: \nCopy \ncast send ANY_ADDRESS --value 1 --private-key YOUR_PRIVATE_KEY_WITH_FUNDS --rpc-url \nhttp://127.0.0.1:8547 \no Use the transaction generator script in the espresso-build-something-real/tx-\ngenerator directory, which can continuously generate test transactions. See the \nREADME in that repository for detailed instructions. \nThese transactions will be processed by your rollup and should appear in the monitoring tool's \noutput if they match your configured criteria. \n          Note: The block number shown in the logs is not the latest block height of the espresso \nnetwork but the latest block processed by your rollup. \nTroubleshooting \nIf you've followed the guide carefully and paid attention to the notes, you shouldn't encounter \nmany issues. However, here are solutions to some common problems: \nNormal Warning Messages \nThe following logs are normal and not cause for concern: \nCopy \n2025-02-27 13:29:11 WARN [02-27|18:29:11.645] unable to get next message               err=\"no \nmessage found\" \n2025-02-27 13:29:13 WARN [02-27|18:29:13.273] failed to fetch the transactions         err=\"no \nmajority consensus reached\" \nThese messages simply indicate that the node is listening to the rollup but there are no new \ntransactions to process. \nTransaction Detection Messages \nThe following logs indicate that the node is correctly listening to the rollup and has detected a \nnew transaction: \nCopy \n2025-03-06 INFO [03-06|21:38:50.671] Added message to queue message=487 \n... \n2025-03-06 INFO [03-06|21:38:50.765] Now processing hotshot block \"block \nnumber\"=2,056,058 \n... \n2025-03-06 INFO [03-06|21:38:52.099] Initial State lastBlockHash=6033cf..9f3bef \nlastBlockStateRoot=35ca02..3749dc \n\n2025-03-06 INFO [03-06|21:38:52.103] Produced block block=751811..23907a \nblockNumber=487 receipts=2 \nSyncing to the Latest Hotshot Block \nIf you see many logs like these at startup, your Caffeinated node is simply syncing to the latest \nHotshot block: \nCopy \n2025-02-27 11:22:32 INFO [02-27|16:22:32.835] No transactions found in the hotshot block \n\"block number\"=1,850,985 \n2025-02-27 11:22:32 INFO [02-27|16:22:32.835] Now processing hotshot block             \"block \nnumber\"=1,850,986 \nSolution: To reduce syncing time, update the next-hotshot-block value in caff-\nnode/config/caff_node.json to the latest block height from the espresso explorer before starting \nthe node. \nRestarting Docker Containers \nIf you stop and restart both your rollup and Caffeinated node, you might need to recopy the \ndatabase folder to your /caff-node folder. Failing to do this can cause your rollup and \nCaffeinated node to fall out of sync. \n          Note: This can result in transactions not being detected by the monitoring tool. \nRate Limiting Issues \nIf you're experiencing rate limiting from your RPC provider, you can adjust polling intervals in the \nconfiguration files: \nCopy \n// In caff-node/config/caff_node.json \n\"caff-node-config\": { \n    \"hotshot-polling-interval\": \"1ms\", \n    \"retry-time\": \"2s\", \n}, \n \n// In config/full_node.json \n\"staker\": { \n    \"staker-interval\": \"120s\", \n    \"make-assertion-interval\": \"120s\", \n} \n\"parent-chain-reader\": { \n\n    \"poll-interval\": \"120s\" \n} \nIncreasing these intervals will reduce the frequency of RPC calls, helping you stay within rate \nlimits. \nMonitoring Issue \nIf you don't see logs indicating successful block processing, your Caffeinate node might not be \nproperly connected to or in sync with the rollup. This often happens when the database \ndirectory isn't properly configured. You should see logs like: \nCopy \n2025/03/01 10:00:00 Searching for transaction at last processed block number: 123456 \nIf these logs are missing, check that: \n• Your repository structure is correct. \n• The docker-compose.yml file is properly configured and indented. \n• Your caffeinated node is properly syncing with the rollup. \nUpdate for Listening to Mainnet \nTo ensure your setup is ready for Arbitrum and Espresso mainnet, you'll need to make the \nfollowing changes in addition to those described in the Deploying Your Rollup on Mainnet \nsection in the previous guide: \n1. Update Caffeinated Node Configuration: In caff-node/config/caff_node.json, update \nthe following values: \nCopy \nparent-chain-node-url: \"WEBSOCKET_ARBITRUM_MAINNET_RPC_URL\", \nespresso-tee-verifier-addr: \"0xE68c322e548c3a43C528091A3059F3278e0274Ed\", \nhotshot-url: \"https://query.main.net.espresso.network/v0\" \n          Note: Make sure to use a WebSocket URL (starting with wss://) for your Arbitrum mainnet \nRPC provider. \n2. Update Hotshot Block Height: Find the latest Hotshot block height at the mainnet \nespresso explorer and update the next-hotshot-block value in your configuration. \nDeploying your caffeinated node on the cloud \nSimilarly to the preceding guide, the next step would be to deploy your caffeinated node on the \ncloud. You can follow the instructions in the Cloud Configuration section of the previous guide \nfor building your EC2 instance and installing the required docker configs. Here are the steps to \nmigrate your caffeinated node to the cloud: \n\n1. Start off from the Set Up and Run section of this guide. The steps on how to run your \nrollup in the cloud have been covered in the previous guide. You can use the same rollup \nconfig files. \n2. Create and configure the caff-node directory structure on your EC2 instance: \nCopy \n# Create directory structure \nssh -i ~/.ssh/your-key.pem ec2-user@YOUR_HOST \"mkdir -p ~/rollup/caff-\nnode/{config,database,wasm}\" \n \n# Set permissions for directories that need write access \nssh -i ~/.ssh/your-key.pem ec2-user@YOUR_HOST \"sudo chown -R 1000:1000 ~/rollup/caff-\nnode/database ~/rollup/caff-node/wasm\" \n          Note: \no The config directory remains owned by ec2-user for configuration files \no database and wasm directories need write permissions for the Docker container \nuser (1000:1000) \n3. Transfer the config files to your EC2 instance: From the root of the repository, run the \nfollowing command: \nCopy \nscp -i ~/.ssh/your-key.pem -r ./caff-node/config/* ec2-user@YOUR_HOST:~/rollup/caff-\nnode/config/ \n4. Copy the database from your rollup to the caff-node directory: From outside the EC2 \ninstance, run the following command: \nCopy \nssh -i ~/.ssh/your-key.pem ec2-user@YOUR_HOST \"cp -r ~/rollup/database ~/rollup/caff-\nnode/database\" \nOr from inside the EC2 instance, run the following command: \nCopy \ncp -r ~/rollup/database ~/rollup/caff-node/database \n          Note: Make sure you sync and stop the rollup service before copying the database. This step \nis crucial and is often the cause of issues. \n5. Enable CloudWatch Logging (Optional): \nIf you want to monitor your caffeinated node logs in CloudWatch: \n\no Create a log group for your caffeinated node in the CloudWatch console, similar \nto how you created log groups in the Setting up CloudWatch logging section of \nthe previous guide. \no Modify your caff-node/docker-compose.yml file to include CloudWatch logging: \nCopy \n version: '2.2' \n services: \n caff_node: \n    image: ghcr.io/espressosystems/nitro-espresso-integration/nitro-\nnode@sha256:bf63374a00a5d6676ca39af79ac4b0f053128cb7438bcdaa746dba6656c12658 \n    container_name: caff_node \n    ports: \n       - \"8550:8547\" \n       - \"8551:8548\" \n       - \"8552:8549\" \n    command: --conf.file /config/caff_node.json \n    volumes: \n       - ./config:/config \n       - ./wasm:/home/user/wasm/ \n       - ./database:/home/user/.arbitrum \n    logging: \n      driver: \"awslogs\" \n      options: \n        awslogs-region: \"us-east-2\" # Update to your EC2 instance's region \n        awslogs-group: \"caff-node-logs\" \n        awslogs-stream: \"caff-node\" \n          Note: Make sure to update the awslogs-region to match your EC2 instance's region. \nWhether you enabled logging or not, you can now transfer the docker-compose.yml file inside \nyour instance. From outside the EC2 instance, run the following command: \nCopy \nscp -i ~/.ssh/your-key.pem ./caff-node/docker-compose.yml ec2-\nuser@YOUR_HOST:~/rollup/caff-node/ \n\n          Note: Your rollup folder structure should be very similar to the repository structure of the \nespresso-build-something-real repository.           Note: You can always update the docker-\ncompose.yml file using nano or by copying again the file from the repository. \n6. Connect to your EC2 instance and start the services: \nCopy \nssh -i ~/.ssh/your-key.pem ec2-user@YOUR_HOST \ncd rollup \ndocker-compose up -d \ncd caff-node \ndocker-compose up -d \n          Note: Do not forget to update the latest hotshot block height in the caff-\nnode/config/caff_node.json from the espresso explorer to avoid resyncing the entire chain.           \nNote: You can stop the services by running docker compose down in the rollup and caff-node \ndirectories.           Note: You can remove the -d flag to run see the logs of the containers in the \nterminal. \nRun the monitoring application \nOnce your caffeinated node is running on the cloud, update the caff_node_url in the hackathon-\nexample repository's config to point to your EC2 instance's public IP address. Then try running \nthe application with the updated config. \nConfiguring Network Access \nIf you encounter connection timeouts when trying to access your EC2 instance (e.g., dial tcp \nYOUR_HOST:8550: i/o timeout), you need to open the required ports in your EC2 security group: \n1. In the AWS EC2 Console, navigate to your instance \n2. Select the \"Security\" tab in the bottom panel \n3. Click on the security group ID (e.g., \"sg-0123456789abcdef\") \n4. In the security group page, select the \"Inbound rules\" tab \n5. Click \"Edit inbound rules\" \n6. Add two new rules: \no Type: Custom TCP, Port range: 8547, Source: Custom 0.0.0.0/0 (or your IP for \nbetter security) \no Type: Custom TCP, Port range: 8550, Source: Custom 0.0.0.0/0 (or your IP for \nbetter security) \n7. Click \"Save rules\" \nThis enables external connections to your Caffeinated node (port 8550) and rollup node (port \n8547). \n\nArbitrum Nitro Integration Overview \nTL;DR - Chains and Rollup-as-a-Service (”RaaS”) providers can leverage the Espresso Network \nto provide users with faster, more secure confirmations on their transactions. The Espresso \nNetwork also provides chain operators with information about the state of their own chain and \nthe states of other chains, all of which is important for improved UX and ultimately, cross-chain \ncomposability. This document outlines the steps for chains/RaaS to integrate with the Espresso \nNetwork. \nIntegration at a glance - Integrating with the Espresso Network requires minimal changes to \nArbitrum Nitro's existing rollup design, and the key change is running the Arbitrum Nitro batch \nposter inside a Trusted Execution Environment (”TEE”). See here for an explanation of this \ndesign and see here for trust assumptions that partners should be aware of when integrating. \nToday, Espresso has developed software to run the batch poster using SGX in Microsoft Azure. \nWe plan to release a version compatible with AWS Nitro (not to be confused with Arbitrum Nitro) \nby EOQ1’25. \nPrerequisites & Requirements (1-3 days*) \n*Assumes familiarity with TEEs. Teams may want to allocate up to 7 days to upskill and deploy \nthis technology for the first time. \nWe will host a kick-off call to walk through your technical architecture, including what cloud \nproviders you work with. In an effort to expedite the integration process, we ask that you please \nshare with Espresso: \n1. A config file that we can work from. Note: This is only needed if Espresso is running the \nbatch poster. \n2. Direct access to your RPC node (instead of through an intermediary). Note: This is only \nneeded if Espresso is running the batch poster. \n3. Confirmation that you are able to run the TEE within your existing infrastructure. Note: \nEspresso is able to run it on your behalf; however we prefer to support you to run it using \nyour own infrastructure. \n4. Confirmation of the integrating chain's current Arbitrum Nitro version. \n5. Other pre-requisites include understanding how a TEE operates, if you aren't already \nfamiliar: \no Docs specific to AWS Nitro Enclaves: \n▪ Review Hello Enclaves sample application \n▪ Install AWS Nitro on Linux OR \n▪ Install AWS Nitro on Windows \no Docs specific to SGX: \n▪ Guide to determine which hardware/cloud providers support SGX \n▪ Guide to enable a prover using SGX (note this links to Taiko’s docs, as \nTaiko currently uses SGX) \n\nRun deployments (2-3 weeks) \nWe require that teams run a testnet and mainnet deployment; however chains and RaaS \nproviders have the opportunity to also run internal devnet deployments beforehand (this is \nrecommended if it's our first time working together). This process will allow us time to fix any \nissues and bugs that may occur during testing and will ultimately ensure a more seamless \ntransition to mainnet. An illustrative integration timeline may look like this (see diagram below): \n• An internal deployment (3-4 days) is optional but is recommended for new architecture \nor a RaaS provider that has not previously integrated with Espresso. \n• A devnet deployment (3-7 days) is optional but is recommended for new architecture or \na RaaS provider that has not previously integrated with Espresso. \n• A testnet deployment (5-7 days) is required, and we look to run a testnet for 5-7 days \nwith no incidents before we consider deploying to mainnet. \n• A mainnet deployment would follow a successful testnet deployment. \n \nMigration Flow (1-3 hours) \n1. Run the batch poster inside the TEE (e.g. AWS Nitro or SGX) \n2. Deploy the EspressoTEEVerifier contract \n3. Stop the batch poster node and copy the batch poster’s databases files to the TEE \n4. Perform the sequencer inbox migration \n5. New batch poster starts catching up messages and building up state \nSupport (0.5-1 hour per week) \n\n• Espresso engineers will support your chain and RaaS provider as you navigate the \nintegration process through relevant devops services, async tech support, and calls to \nrun through integration steps. \n• Espresso has a 24x7 support model and tracks the health and frequency of batch \nposting to ensure that there are no delays. If something isn't working, Espresso will \nautomatically trigger an on-call procedure to investigate the incident. We recommend \nthat our integrated chains also monitor these health metrics and have an incident \nmanagement procedure. \nFAQ \n1. How much does it cost a chain or RaaS provider to integrate with Espresso? \n1. Fees are paid by the builder of each block. Initially, in Mainnet 0.0/Decaf 0.0, \nEspresso will run a simple builder and cover fees. Fees will not be collected from \nrollups using Espresso or their users. There will be no third party builders during \nMainnet 0.0/Decaf 0.0. \n2. What is Espresso’s latency for blocks? How long does it take blocks to reach finality? \n1. Espresso confirmations are faster than waiting for Ethereum finality, which takes \n12-15 minutes. Using Espresso, the current latency for blocks is 2-9s (1-3s with \ntransactions, 8s with no transactions) with HotShot finality reached after 3 \nconsecutive blocks, typically in 6-20s (slower when blocks are empty). There are \nfuture roadmap improvements to decrease latency and time to finality (including \nan adjustment to finalize after 2 consecutive blocks, rather than 3, which should \nresult in a 20-30% reduction in latency). \n3. What is Espresso’s throughput per second? \n1. During some recent heavy load testing, we achieved ~100 TPS with small \ntransactions (~200 bytes each). We currently have a 1MB limit on the block size, \nwhich performed well under the same load testing. We plan to use our Proof of \nStake upgrade in March 2025 to significantly improve network throughput and \nlatency with (i) an increase to the block limit, and (ii) adding erasure coding via \nVerifiable Information Dispersal. \nUsing TEE with Nitro \nThe Espresso Network is a confirmation layer that provides chains with information about the \nstate of their own chain and the states of other chains, which is important for cross-chain \ncomposability. Espresso confirmations can be used in addition to the soft confirmations from a \ncentralized sequencer, are backed by the security of the Espresso Network, and are faster than \nwaiting for Ethereum finality (12-15 minutes). \nPurpose: This document describes how the Espresso Network provides fast confirmations to \nArbitrum Orbit chains. Espresso has developed a TEE based integration, which is ready for chain \noperators and rollup-as-a-service providers to implement. There is some assumed familiarity \nwith the Arbitrum Nitro stack. \nHow it works: In a regular chain, the transaction lifecycle will look something like this: A user \ntransacts on an Arbitrum chain. The transaction is processed by the chain’s sequencer, which \n\nprovides a soft-confirmation to the user, and the transactions are packaged into a block. The \nsequencer is responsible for collecting these blocks, compressing, and submitting them to the \nbase layer. If the base layer is Arbitrum One or Ethereum, then the transaction will take at least \n12-15 minutes to finalize, or longer depending on how frequently the sequencer posts to the \nbase layer. In this transaction lifecycle, the user must trust that the chain’s sequencer provided \nan honest soft-confirmation and will not act maliciously. There are limited ways to verify that the \nsequencer and batcher acted honestly or did not censor transactions. \nHowever, if the chain is integrated with the Espresso Network: The sequencer provides a soft-\nconfirmation to the user, while the transactions are also sent to the Espresso Network to \nprovide a stronger confirmation secured by BFT consensus. A software component of the \nsequencer called the batch poster (or “batcher”) is run inside a TEE and must honor the \nEspresso Network confirmation. It cannot change the ordering or equivocate. This gives a strong \nguarantee that the transaction will ultimately be included and finalized by the base layer. \n• Implication: The user must trust that the chain’s sequencer provided an honest soft-\nconfirmation; however the Espresso Network provides a stronger confirmation that \nkeeps the sequencer accountable and prevents the sequencer from equivocating or \nacting maliciously. The initial implementation of the batch poster is permissioned and \nthe user must trust that it will not reorder blocks produced by the sequencer. \nIntegration considerations: **Integrating with the Espresso Network requires minimal changes \nto Arbitrum Nitro’s existing rollup design. The key change is running the Arbitrum Nitro batch \nposter (which collects multiple transactions, organizes them into batches, and compresses the \ndata to post to the base layer) inside a Trusted Execution Environment (“TEE”). Espresso’s \npreferred TEE for this integration is built using Intel SGX, and we plan to support other \napproaches in the future. \nEffort required: We would kick off the integration process with a full walkthrough of your \narchitecture. Espresso will provide a config file that chains and RaaS providers can leverage \nduring integration. We prefer to have direct access to your node (instead of through an \nintermediary). \nGoal \nThe Espresso Network’s fast confirmation layer provides users with faster, more secure \nconfirmations on their transactions as well as chain operators with information about the state \nof their own chain and the states of other chains, all of which is important for improved UX and \nultimately, cross-chain composability. \nThe goal of this document is to integrate the Espresso Network’s fast confirmation layer with \nminimal changes to Arbitrum Nitro's existing rollup design. By leveraging Espresso’s fast \nconfirmations, a rollup accepts a batch only after it has been finalized by the Espresso Network. \nThis integration ensures that each batch processed by the rollup is consistent with the blocks \nfinalized via the Espresso Network. \nThis approach involves running an Arbitrum Nitro node with only the batcher enabled, operating \nin a TEE environment (such as Intel SGX). The batcher signs the combined hash of blob hashes \nand add this signature to the L1 transaction calldata. \nAssumption \n\nThis document assumes that the batch poster sends only blob transactions to L1. However, in \nsome cases, it does post the data in calldata. The flow remains the same in this scenario, \nexcept that the batcher signs the data directly, and the Sequencer Inbox contract verifies the \nsignature accordingly. \nBatch Consistency Checks \nTo ensure that the batch has been finalized by the Espresso Network (“HotShot” is the name of \nthe consensus protocol), the following checks are required: \n1. Namespace Validation: Ensure that the set of transactions in an Arbitrum Nitro batch \ncorresponds to an Arbitrum Nitro/Orbit chain namespace. Namespacing allows \nmultiple chains to use Espresso’s fast confirmation layer simultaneously by associating \neach chain’s transactions with a unique namespace within the Espresso Network \nblocks. \n2. Espresso Block Merkle Proof Check: Confirm that the Arbitrum Nitro batch maps to a \nvalid HotShot block. Specifically, verify that the HotShot block associated with an \nArbitrum Nitro batch is a valid leaf in the Merkle tree maintained by the light client \ncontract, which stores the state of HotShot on Ethereum. \nIntegration Design \nThe integration flow is as follows: \n \nArbitrum Nitro integration flow with Espresso \nThe flow is as follows: \n1. The sequencer calls WriteSequencerMsg on the transaction streamer. \n2. The batcher fetches the message from the transaction streamer and submits the \ntransaction to HotShot via the transaction streamer. Code for this is already \nimplemented in the batcher here. \n3. The batcher then calls the query API to check if the transaction has been finalized by \nHotShot. This code is also written and can be found here. \n\n4. Once the transaction is finalized, the batcher performs batch consistency checks. \n5. The batcher computes the hashes of the blobs it plans to submit. (Note: Arbitrum uses \nblob transactions.) This allows the batcher to sign the combined hash of the blobs, \nalong with other calldata fields, before sending the transaction. The signature is then \nincluded in the transaction’s calldata. If blob transactions are not being used, the \nbatcher instead signs the l2MessageData along with the other calldata fields. \n6. The Sequencer Inbox contract must be modified to verify the batcher’s signature. If the \nsignature is invalid, it will revert the transaction. \nBatching Flow Changes \nRunning Arbitrum Nitro batch poster in TEE \nThe first set of changes require running the batch-poster in a Trusted Execution Environment \n(TEE), specifically Intel SGX. \n• Several LibOSes have been developed to run applications on SGX, including Gramine, \nOcclum and EGo. We selected Gramine due to its maturity, stability, and comprehensive \ndocumentation. \n• We implemented Remote Attestation support, enabling anyone to verify the SGX proof. \nThis is accomplished by generating a Remote Attestation TLS (RA-TLS) certificate during \nstartup, which embeds the SGX attestation report (more info here). \nSubmitting Transactions to the Espresso Network (HotShot) \nMost of the necessary code for this function (which allows the batcher to submit transactions to \nEspresso after receiving transactions from the sequencer) is already implemented in our \nbatcher. \nChecking for HotShot Finalization \nWe have existing code in place where the batcher prepares an Espresso justification once \nHotShot finalizes a transaction. \nVerifying Batch Consistency \nBatch consistency verification involves checking the block Merkle proof and the namespace \nproof. Jellyfish, Espresso’s cryptography library, is written in Rust. We created an espresso-\ncrypto-helper Rust package that allows us to verify the Merkle proof and namespace proof. \nTo enable the Go-based batcher to use these Rust functions, we implemented a Foreign \nFunction Interface (FFI) to expose the Rust verification functions to Go. \nSending Transactions to L1 \nWe add the batcher’s signature to the transaction calldata over the blob hashes, which can be \nconstructed using the ComputeCommitmentAndHashes method. We sign blob hashes instead \nof the blob data itself because, in a Blob Commitment transaction, the blob data is unavailable \non the execution layer, only blob hashes are accessible in Solidity via the blobhash opcode. \nIn a scenario when a chain is not using blob transactions, the batcher will sign the \nl2MessageData. \n\nSequencer Inbox Contract \n• The Sequencer Inbox’s methods will need to be modified to accept the batcher \nsignature as a parameter. \n• These methods verify that the signature is valid \nEscape Hatch \nWe also have an escape hatch where the batch poster calls IsHotshotLive on the \nLightClientContract before posting a batch. Each batch poster configuration allows the chain to \nchoose between two behaviors: (i) waiting for Hotshot to go live before posting the batch, or, (ii) \nif Hotshot is not live, proceeding without checking batch consistency with Hotshot. This \nflexibility enables chains to tailor their approach as needed. \nArbitrum Nitro Trust & Liveness Dependencies \nTL;DR - The Espresso Network is a confirmation layer that provides chains with information \nabout the state of their own chain and the states of other chains, which is important for cross-\nchain composability. Espresso confirmations can be used in addition to the soft confirmations \nfrom a centralized sequencer, are backed by the security of the Espresso Network, and are \nfaster than waiting for Ethereum finality (12-15 minutes). \nRecap: optimistic chain architecture - There are two components of the Arbitrum Nitro stack: \n1. The validator, the fraud proof mechanism, and everything that exists to secure the \nintegrity of the chain. \n2. The sequencer, which includes the batcher, and exists to drive progress and have some \ninfluence (in that they can re-order transactions) without the ability to corrupt the chain \nand steal user funds. \nWhat Espresso is changing and why - \n• What? Espresso is not changing how sequencing is done; instead we are ensuring that \nwhat gets sequenced is immediately posted to the Espresso Network and what is \nultimately published to the L1, is consistent with the Espresso Network. On a technical \nlevel, we do this by modifying the software module (the batcher), ensuring that the \nbatcher publishes batches to the Espresso Network. The batcher module is run inside a \nTEE so that even if hacked the batcher will never publish something to the L1 that is \ninconsistent with what was published and finalized on the Espresso Network. \n• Why? These additional checks confirm the order of transactions and prevent \nequivocation prior to finality. These confirmations provide stronger guarantees to chains \nwanting to verify their own state, or the state of chains they want to compose with. \nImportantly these changes are limited in power and do not provide the ability to directly \nsteal user funds, although there may still be chain reorganization risk for applications \nlike bridges that rely on soft-confirmations (instead of waiting for chain finality). \nResources:  \n• This document outlines key trust and liveness dependencies to help chains to \nunderstand the design. \n\n• The detailed technical design is outlined here for reference. \nLiveness Dependencies \nBatch Poster \nELI5: The chain’s sequencer batches individual transactions into blocks, and the chain’s batch \nposter is responsible for collecting these blocks and submitting them to the base layer (for \nexample, an Arbitrum Orbit L3 would typically post batches to Arbitrum One, an L2). If the batch \nposter were to crash, customers could still use the chain regularly and would only be limited in \ntheir ability to withdraw funds (as delays in batch posting impact the fraud proof window). \nTechnical explainer: the batch poster does play a role in the liveness of withdrawals. Any \ndowntime experienced by the batch poster adds to the latency of withdrawals initiated during \nits outage. This is because the challenge period cannot begin until the batch poster comes back \nonline and posts a batch containing the withdrawal. Given that the challenge period for an \noptimistic rollup is seven days, the delay caused by a batch poster outage would only become \nnoticeable to users if the batch poster remains down for several days, rather than just a few \nminutes. While this scenario is unlikely, it remains a potential risk. An extended delay in batch \nposting (>72 hours) may trigger Arbitrum Nitro’s force-inclusion mechanism and may result in a \nchain reorganization that may impact clients or applications that rely on soft confirmations. See \nbatch posting for details. \nIntegration considerations: The responsibility of running infrastructure (whether operated by \nyour Rollup-as-a-Service (”RaaS”) provider or your chain) is unchanged. In either case, your \nchain (or your RaaS provider) will be required to run a fork of the Nitro batch poster developed by \nEspresso Systems. The batch poster will need to be run in a Trusted Execution Environment \n(”TEE”) that is SGX supported, with support for TDX and AWS Nitro coming soon. Espresso can \nshare a getting-started guide, links to relevance code, and technical/integration support. \nEspresso Network \nELI5: The Espresso Network is designed to enhance the security of soft confirmations, which are \ngiven when a transaction has been included in a pending, recent block but has not yet been \nfinalized. In doing this, Espresso Network’s confirmations speed up the process of a transaction \nreaching finality (when a transaction is considered irreversible and the network has reached \nconsensus that the transaction is valid) by providing a quicker indication that a transaction is \nlikely to be valid. In the unlikely event that the Espresso Network goes offline, there may be an \nimpact on a users’ ability to withdraw funds as outlined above (see Batch Poster). Chains can \nchoose to (i) disable the escape hatch, or (ii) enable the escape hatch in the event that the \nEspresso Network is offline. Chains that enable the escape hatch may result in clients and \nbridges building an incorrect state due to their reliance on soft confirmations. \nTechnical explainer: Similarly, if the Espresso Network goes offline, there can be a similar \nimpact on withdrawals as outlined above (see Batch Poster). While the likelihood of such \ndowntime is low, it remains a possibility. \nTo mitigate these risks, the chain can configure the Escape Hatch feature in the batch poster \nconfiguration: \n• Enabling the Escape Hatch - When the Escape Hatch is configured, the batch poster \nbypasses Espresso checks and posts batches directly to parent chain (e.g. Arbitrum \n\nOne) if the Espresso Network is down. Once the Espresso Network is back online, it will \nresume verifying Espresso checks before posting batches. Note: The Escape Hatch is \nconfigured through the batch poster config, but the logic for switching between \nchecking Espresso checks based on Espresso Network’s liveness is automated through \ncode. However, configuring the Escape Hatch has implications for any clients or bridges \nrelying on the Espresso Network for finality. If the Escape Hatch is activated while the \nEspresso Network is offline, clients and bridges may build an incorrect state. \n• Disabling the Escape Hatch - We recommend that chains avoid enabling the Escape \nHatch by default. Although the likelihood of the Espresso Network being offline for an \nextended period is minimal, activating the Escape Hatch could compromise the integrity \nof client-side states, making bridge integrations using Espresso on the chain less \nsecure. Chains that enable the Escape Hatch may result in clients and bridges building \nan incorrect state due to their reliance on soft confirmations. \nIntegration considerations: Chains can choose between two options: \n• (i) enable the Escape Hatch to post batches directly to the base layer (e.g., Arbitrum \nOne) and default back to the Espresso Network once it is back online. \n• (ii) disable the Escape Hatch - wait until the Espresso Network comes back online \nIf your team feels strongly about enabling the Escape Hatch then Espresso can meet with the \nchain and/or RaaS provider to discuss how we plan to mitigate this risk. \nHotshot URL \nELI5: The Espresso Network provides updates on state via the HotShot URL, similar to an RPC. If \nthe HotShot URL were to go offline or provide an incorrect state, this may trigger a failure that \ncould delay batch posting. \nTechnical explainer: The batch poster configuration specifies a Hotshot URL, which is \nanalogous to an RPC URL. This URL is used to connect to the node running the Hotshot network \nto retrieve updates. \nThere is a liveness dependency on the Hotshot URL. If this URL provides incorrect state, it will \nlead to Espresso Network specific checks failing and thus preventing the batch poster from \nbeing able to post batches. The likelihood of this is low but still possible. \nIntegration considerations: None. Please note that chains can verify liveness by checking the \nEspresso Explorer and Query Service to verify that the Espresso Network is live and the HotShot \nURL is working as intended. If there have been no updates for 15-30 minutes, please assume \nthere is an issue that we will investigate via on-call procedures and resolve. \nTrust Dependencies \nL1 URL \nWe retrieve the Arbitrum state from an RPC node. This introduces a trust dependency on the \nRPC node to provide accurate state information. You may configure the batcher to use the same \nArbitrum RPC that it is currently using. \nLight Client Contract \n\nA light client contract has been deployed on the Arbitrum network. This contract maintains the \nstate of the Hotshot network. There is a trust dependency that this contract remains secure, \nunhacked, and consistently contains the correct state. \nSGX Intel Infrastructure \nWe have built our design using Intel SGX, so there is a trust dependency that Intel is behaving \nhonestly and their infrastructure is secure and trustworthy. \n ",
    "filename": "10. Arb Nitro Integration Overview.pdf"
  },
  {
    "id": 3,
    "content": "\n\nMigrating Arbitrum Orbit Chains to Espresso \nEspresso Nitro Integration \nYou may wish to familiarize yourself with the security audits for this integration. \nThe target audience for this document are developers who would like to migrate an existing \nArbitrum Orbit chain to use Espresso's Global Confirmation Layer (GCL). The document should \nalso be interesting for developers who are thinking of deploying a new Arbitrum Orbit chain that \nuses Espresso's GCL. \nThe goal of this document is to describe how to migrate an Arbitrum Orbit chain to using \nEspresso's global confirmation layer for fast confirmations. By leveraging Espresso's fast \nconfirmations, a rollup will only accept a batch after it has been finalized by Espresso. This \nintegration ensures that each batch processed by the rollup is consistent with HotShot-finalized \nblocks within its namespace. \nQuickstart \nThe quickest way to get familiar with what the migration entails is to run through our migration \ntest. \nThe script creates up an ephemeral Nitro chain on the local computer and migrates it to using \nEspresso's Global Confirmation Layer (also running locally on the computer). This test mocks \nout the TEE part by using a mocked TEE verifier contract and configured the batch poster to \nbypass the TEE interactions. Thereby it's possible to run through the migration locally, without \nrequiring a special CPU and extra software to support the TEE. \nFor a safe production deployment the TEE is required and the real TEE verifier contract must be \ndeployed. \nThe test requires Docker, Foundry, Yarn, openssl and jq to be installed. \nFor convenience the nix package manager can take care of installing all dependencies except \nfor Docker. Nix can be installed by running \nCopy \ncurl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install \nTo run the test first some setup: \nCopy \ngit clone --recursive https://github.com/EspressoSystems/nitro-testnode \ncd nitro-testnode \ngit checkout integration \nnix develop # omit if you have foundry, yarn, jq, openssl installed already \nNow you can run the test itself. \nCopy \nespresso-tests/migration-test.bash \n\nYou may need to run the following command to ensure the submodules are initialized: \nCopy \ngit submodule update --init --recursive \nYou should see substantial output. The first time you run this, it can take up to 15-20 minutes \ndue to initial setup steps like installing dependencies. Subsequent runs should take less than \n10 minutes. \nAfter some time, if successful, you should see Migration successfully completed!. \n    Warning: If you see \"Waiting for confirmed nodes\" logs for more than 60 seconds, it's \nrecommended to restart the process. \nIt's encouraged to read through the migration test to get an idea of all the steps involved and \nwhat information is required for each step. For the real migration using .env files instead of \nenvironment variables to provide the necessary inputs may be more convenient. \nOnce familiar, you might run some commands to further your understanding. Transfer some eth, \nfor example: \nCopy \ncast send $RECIPIENT_ADDRESS --value 1ether --rpc-url $CHILD_CHAIN_RPC_URL --private-\nkey $PRIVATE_KEY \nAssumptions \nBefore attempting a migration please verify that the assumptions listed here make sense for \nyour nitro deployment. If not, please get in touch with us via our contact form. \n1. The Orbit chain to be migrated is using vanilla Nitro stack, or Celestia DA via the Celestia \nfork of nitro. If your Orbit chain is using Celestia for DA you need to use the celestia-\nintegration branches of Espresso's nitro forks. \n2. You are able to run the batch poster in an SGX TEE environment. This is required to \nprovide TEE attestations that the L2 transactions have been finalized by Espresso. These \nattestations are verified in the TEE verifier smart contract on the parent chain as part of \nthe batch submission. \nProduction Setup Requirements \nFor production migrations, you will need the following system requirements (note: these are not \nneeded for running the test migration): \n• SGX \n• Gramine \n• 16GB RAM \nYou will also need these Espresso projects. Depending on whether you use celestia DA or not, \nyou will need either the integration branch or celestia-integration branch. Please take the time \nto review each project's respective README. \n• https://github.com/EspressoSystems/nitro-espresso-integration \n\n• https://github.com/EspressoSystems/nitro-contracts \n• https://github.com/EspressoSystems/nitro-testnode \n• https://github.com/EspressoSystems/orbit-actions \n• https://github.com/EspressoSystems/gsc \nAs well as Espresso's nitro-node docker image: \nCopy \n ghcr.io/espressosystems/nitro-espresso-integration/nitro-node:v3.3.2-fcd633f \nMigration Flow \nThis flow is more precisely defined in our migration test. Since that script is intended for testing \nmigrations, it does not not require SGX. Therefore it should not be use for production \ndeployments. \n1. Run the new batch poster inside SGX and get the MREnclave and MRSigner values \n(which is the hash of the code running inside the SGX). These values are needed to \ndeploy the EspressoTEEVerifier contract. \n2. Deploy the EspressoTEEVerifier contract. \n3. Stop Nitro node with the batch poster and copy the batch poster's databases files to the \nTEE. \n4. Perform the sequencer inbox migration. \n5. Run new Batch poster. You will notice it starts catching up messages and building up \nstate. \nPlease also be aware of steps to revert a migration should you need them. \nVerify SGX Setup \nSetup of SGX TEE is beyond the scope of this document. We have had success with Azure SGX \nVMs \nFirst verify you have linux version with in-kernel SGX. To ensure you have in-kernel SGX run the \nfollowing command to verify you are running a Linux kernel greater than version 5.11. \nCopy \nuname -r \nEnsure that your machine has an Intel SGX2 (Software Guard Extensions) enabled CPU to run \nour batch poster (See here and here). You can verify if your CPU supports SGX2 on Linux by \ninspecting CPU information: \nCopy \ngrep -i sgx /proc/cpuinfo \nIf your CPU supports SGX, the output should resemble the following. If it does not, your CPU \neither doesn't support SGX2, or it isn't enabled in the BIOS. \n\nCopy \n SGX2 supported                           = true \nInstall Gramine \nTo install gramine follow the following instructions: \n1. Install software packages \n2. Setup host configuration \n3. Install the SGX software stack \n4. If your machine is running on Microsoft Azure, you can refer to this document for \nconfiguring aesm. \nYou don't need to follow the build gramine steps as we will be using Gramine Shielded \nContainers (GSC) \nAt this point systemctl status aesmd should report a healthy service (Active: active (running)). \nIf you see any errors here, ensure your aesmd is configured properly. If your machine is running \non Microsoft Azure, you can refer to this document for configuring aesm. \nRunning the Batch Poster Inside SGX \nBuilding the Poster Image \nThese steps need to happen inside SGX \nUse our default Dockerfile.sgx-poster or the Celestia Dockerfile.sgx-poster to build the sgx-\nposter Docker image. \nFirst obtain the source: \nCopy \ngit clone https://github.com/EspressoSystems/nitro-espresso-integration \ncd nitro-espresso-integration \n• Alternative 1: using default Nitro stack \nCopy \n  git checkout integration \n• Alternative 2: using Celestia DA \nCopy \n  git checkout celestia-integration \nThen build the image. \nCopy \ndocker build -f Dockerfile.sgx-poster -t sgx-poster . \n\nBuilding the Gramine Image \nOnce the docker image is built, you need to build a Gramine Shielded Container (GSC) image. \nThe gsc python script requires a few python packages. For ubuntu 24.04 run \nCopy \nsudo apt-get install -y python3 python3-docker python3-jinja2 python3-tomli python3-tomli-w \npython3-yaml \nFor more information see the GSC docs. \nClone Espresso's gsc repo and build the Gramine image: \nCopy \ngit clone https://github.com/EspressoSystems/gsc.git \ncd gsc \ngit checkout master \ncp config.yaml.template config.yaml \n./gsc build sgx-poster ./nitro-espresso.manifest \nNow before building the gramine image, you need to edit your poster_config.json file to include \nthe following fields (given parent chain is arbitrum sepolia): \nCopy \n\"batch-poster\": { \n    \"hotshot-url\": \"https://query.decaf.testnet.espresso.network/v0\", \n    \"light-client-address\": \"0x08d16cb8243b3e172dddcdf1a1a5dacca1cd7098\", \n    \"resubmit-espresso-tx-deadline\": \"2m\" \n}, \n\"transaction-streamer\": { \n    \"user-data-attestation-file\": \"/dev/attestation/user_report_data\", \n    \"quote-file\": \"/dev/attestation/quote\" \n} \nYou need the sha256 hash of your poster_config.json file. You can get the hash using the \nfollowing command: \nCopy \nsha256sum poster_config.json \nReplace the nitro-espresso.manifest in the gsc with these contents and replace the \n<YOUR_SHA256_HERE> with the sha256 of your poster_config.json file. \n\nCopy \nsys.enable_extra_runtime_domain_names_conf = true \nsgx.edmm_enable = true \nsgx.remote_attestation = \"dcap\" \nsgx.use_exinfo = true \nsys.experimental__enable_flock = true \nfs.mounts = [ \n    { path = \"/home/user/.arbitrum/\", uri = \"file:/home/user/.arbitrum\"}, \n    { path = \"/config/\", uri = \"file:/config\"} \n] \nsgx.allowed_files = [\"file:/home/user/.arbitrum\"] \n[[sgx.trusted_files]] \nuri = \"file:/home/user/kzg10-aztec20-srs-1048584.bin\" \nsha256 = \"cded83e82e4b49fee4cb2e0f374f996954fe12548ad39100432ee493069ef09d\" \n[[sgx.trusted_files]] \nuri = \"file:/config/poster_config.json\" \nsha256 = \"<YOUR_SHA256_HERE>\" \nNext we will need to sign the image in order to run this container inside the SGX enclave. \nGenerate the signing key (if you don't already have one). \nCopy \nopenssl genrsa -3 -out enclave-key.pem 3072 \nPLEASE KEEP THIS KEY SAFE in some local private storage, but delete it from the server \nafter you have signed. \nSign the container \nCopy \n./gsc sign-image sgx-poster enclave-key.pem \nThe final step is to run the container inside the SGX enclave. This requires a config folder which \ncontains the poster_config.json file (available from the legacy batch poster). \nFinally we also need a .arbitrum folder which contains the state of the batch poster. At this point \nit can be an empty folder but once the legacy batch poster is shut down, we should fill this up \nwith the contents of the legacy batch poster and re-start the poster. \nRun the batch poster using the following command, replacing $CONFIG_PATH with the actual \npath to these folders on your host machine. \n\nThese folders are mounted in the docker container, so any changes to them on the host change \nthem in the container. \nCopy \ndocker run \\ \n    --device=/dev/sgx_enclave \\ \n    -v /var/run/aesmd/aesm.socket:/var/run/aesmd/aesm.socket \\ \n    -v $CONFIG_PATH/.arbitrum:/home/user/.arbitrum \\ \n    -v $CONFIG_PATH/config:/config \\ \n    gsc-sgx-poster \nAt this stage, you will see an attestation report similar to the following. The hex value, is the \nreport data which contains the MR_ENCLAVE (the hash of the code running inside SGX) and the \nMR_SIGNER. After you see the attestation report, you can shut down the batch poster. \nCopy \n0e0e100fffff010000000000000000000100000000000000000000000000 \n0000000000000000000000000000000000000500000000000000e7000000 \n000000001f43237dce5a5deecd51d834e6467af7cc9856c7455dcabab6bb \n2e7a2012c4c8000000000000000000000000000000000000000000000000 \n00000000000000000458a0e62674775ca9a048016f817f39b0bd40153000 \naceb44a5128ded30555e0000000000000000000000000000000000000000 \n000000000000000000000000000000000000000000000000000000000000 \n000000000000000000000000000000000000000000000000000000000000 \n000000000000000000000000000000000000000000000000000000000000 \n000000000000000000000000000000000000000000000000000000000000 \n000000000000000000000000000000000000000000000000000000000000 \n000000000000000000000000000000000000000000000000000000000000 \n000000000000000000000000000000000000000000000000888ae654e178 \nb8fa82cc685faa45a521000000000000000000000000000000009cc8bb48 \nd11c1dd7039a0ceadbdc1c77 \nSuccessfully read attestation report. \nYou can decode all this information using the following steps: \n1. Create a report.txt file with the hex value and create a bin file as follows: \nCopy \n\nxxd -r -p report.txt report.bin \n2. Use this bash script to decode the binary, it will print out the MR_ENCLAVE and \nMR_SIGNER. \nDeploying the EspressoTEEVerifier contract \nContract Deployment \nYou will need to use our version of nitro contracts. The instructions below assume you are using \ncelestia for DA. If not, simply checkout on integration branch instead. \nPlease clone this repo at the given branch to follow the next steps. \nCopy \ngit clone --recurse-submodules git@github.com:EspressoSystems/nitro-contracts.git \ngit checkout celestia-integration \nTo run this script to deploy a rollup on arbSepolia follow the given steps: \nCompile the contracts using yarn \nCopy \nyarn install && forge install \nyarn build:all \nCreate a .env file with variables from the previous steps \nCopy \nDEVNET_PRIVKEY=\"\" # Private key with funds on Arbitrum Sepolia. \nMR_ENCLAVE=\"MR_ENCLAVE_VALUE\" # MR_ENCLAVE value from the last step \nMR_SIGNER=\"MR_SIGNER_VALUE\" # MR_SIGNER value from the last step \nARBISCAN_API_KEY=\"\" # [Arbscan API Key](https://docs.arbiscan.io/getting-started/viewing-\napi-usage-statistics) \nDeploy the contract \nCopy \nnpx hardhat run scripts/deployEspressoTEEVerifier.ts --network arbSepolia \nSequencer Inbox Migration \nObtain the Contracts \nIn this section we will be working out of our orbit-actionsrepo. Again the choice of integration or \ncelestia-integration depends on DA. \nCopy \ngit clone git@github.com:EspressoSystems/nitro-contracts.git \n\nBe aware that the contract to deploy the sequencer inbox needs to locally deploy a mock \nArbitrumChecker to prevent foundry from declaring calls to precompiles as invalid opcodes. \nThis shouldn't affect on chain deployment from these scripts, nor should it affect the onchain \nexecution of the migration action. \nConfiguration \nCreate a .env file in the orbit-actions directory that contains the following values: \nCopy \n#The private key for use during the migration. This should be the rollup owner's private key for \nsteps involving performing the migration. \nPRIVATE_KEY=\"\" \n# Environment variables for chain name and rpc_url \n# These are essential for the upgrade \nPARENT_CHAIN_CHAIN_ID=\"\" \nPARENT_CHAIN_RPC_URL=\"\" \n# Addresses to the upgrade executors on both chains. \n# These are essential for the upgrade \nPARENT_CHAIN_UPGRADE_EXECUTOR=\"\" \n# Environment variables for the sequencer inbox migration action contract \nROLLUP_ADDRESS=\"\" \nPROXY_ADMIN_ADDRESS=\"\" \n \n# The reader address should be set to the zero address for orbit chains, for other chains, set this \nto the same address as your current reader for the sequencer inbox \nREADER_ADDRESS=\"0x0\" \nIS_USING_FEE_TOKEN=\"\" \n \nMAX_DATA_SIZE=\"104857\" #for orbit chains, use this value, for chains posting to ethereum, use \n117964 \n# The old batch poster address will be removed from the sequencer inbox proxy to ensure only \nthe batch poster running in the TEE will be allowed to post batches. \nOLD_BATCH_POSTER_ADDRESS=\"\" \nNEW_BATCH_POSTER_ADDRESS=\"\" \n# The new batch poster address and batch poster manager address will be provided to you to \nrun the migration. \n\nBATCH_POSTER_MANAGER_ADDRESS=\"\" \n# This should be the address of the contract you deployed in the last step. \nESPRESSO_TEE_VERIFIER_ADDRESS=\"\" \n# This will allow you to deploy a reverting sequencer inbox migration action if desired (there are \nadditional steps in the migration readme) \nIS_REVERT=\"\" \nSource it. \nCopy \nsource ./.env \nInstall all dependencies. \nCopy \nyarn \nyarn prepare \nyarn build \nRun the migration deployment scripts \nRun the migration deployment scripts for the parent chain. Including both \nDeployAndInitEspressoSequencerInbox.s.sol, and \nDeployEspressoSequencerInboxMigrationAction.s.sol. From the base directory of the orbit \nactions repo, you can use the following commands to run these scripts: \nDeployAndInitEspressoSequencerInbox.s.sol \nCopy \nforge script --chain $PARENT_CHAIN_CHAIN_ID contracts/parent-chain/espresso-\nmigration/DeployAndInitEspressoSequencerInbox.s.sol:DeployAndInitEspressoSequencerInbo\nx --rpc-url $PARENT_CHAIN_RPC_URL --broadcast -vvvv --skip-simulation \nBefore you proceed: make sure to store the address of the new SequencerInbox in the \nenvironment variable NEW_SEQUENCER_INBOX_IMPL_ADDRESS \nDeployEspressoSequencerInboxMigrationAction.s.sol \nCopy \nforge script --chain $PARENT_CHAIN_CHAIN_ID contracts/parent-chain/espresso-\nmigration/DeployEspressoSequencerMigrationAction.s.sol:DeployEspressoSequencerMigratio\nnAction --rpc-url $PARENT_CHAIN_RPC_URL --broadcast -vvvv --skip-simulation \nBefore you proceed: make sure to store the address of the new SequencerInbox in the \nenvironment variable SEQUENCER_MIGRATION_ACTION \nExecute the Upgrade \n\nThe final step for executing the migration involves using cast to call the perform() function of the \nsequencer inbox migration action via the upgrade executor. You can use the following command \nto accomplish this: \nCopy \ncast send $PARENT_CHAIN_UPGRADE_EXECUTOR \"execute(address, bytes)\" \n$SEQUENCER_MIGRATION_ACTION $(cast calldata \"perform()\") --rpc-url \n$PARENT_CHAIN_RPC_URL --private-key $PRIVATE_KEY \nAfter running this command, your rollup contracts should be set up to accept batches from your \nnew batch poster. \nRun the Batch Poster with Legacy State \nAs stated in upstream documentation, you need to copy the contents of the .arbitrum folder of \nthe legacy batch poster to the .arbitrum folder of the new \nThen re-start the batch poster. \nVerify the Migration \nYou should be able to see batch sent logs once the batcher starts posting batches. This would \nindicate that the batcher has started successfully. \nNitro Testnet \nTL;DR \nThe Espresso Network is a confirmation layer that provides chains with information about the \nstate of their own chain and the states of other chains, which is important for cross-chain \ncomposability. Espresso confirmations can be used in addition to the soft confirmations from a \ncentralized sequencer, are backed by the security of the Espresso Network, and are faster than \nwaiting for Ethereum finality (12-15 minutes). \nOverview \nPurpose \nThis document describes how the Espresso Network provides fast confirmations to Arbitrum \nOrbit chains. Espresso has developed a TEE based integration, which is ready for chain \noperators and rollup-as-a-service providers to implement. There is some assumed familiarity \nwith the Arbitrum Nitro stack. \nHow It Works \nIn a regular chain, the transaction lifecycle will look something like this:  \n1. A user transacts on an Arbitrum chain.  \n2. The transaction is processed by the chain’s sequencer, which provides a soft-\nconfirmation to the user, and the transactions are packaged into a block.  \n3. The sequencer, responsible for collecting these blocks, compressing, and submitting, \nsubmits the transactions to the base layer. \n\n1. If the base layer is Arbitrum One or Ethereum, then the transaction will take at \nleast 12-15 minutes to finalize, or longer depending on how frequently the \nsequencer posts to the base layer.  \n2. In this transaction lifecycle, the user must trust that the chain’s sequencer \nprovided an honest soft-confirmation and will not act maliciously. There are \nlimited ways to verify that the sequencer and batcher acted honestly or did not \ncensor transactions. \nThis is a strong assumption, and the key thing that the Espresso Network helps with. When the \nchain is integrated with the Espresso Network: The sequencer provides a soft-confirmation to \nthe user, while the transactions are also sent to the Espresso Network to provide a stronger \nconfirmation secured by BFT consensus. A software component of the sequencer called the \nbatch poster (henceforth referred to as “batcher”) is run inside a TEE and must honor the \nEspresso Network confirmation. It cannot change the ordering or equivocate. This gives a strong \nguarantee that the transaction will ultimately be included and finalized by the base layer. \nThe user must trust that the chain’s sequencer provided an honest soft-confirmation; however \nthe Espresso Network provides a stronger confirmation that keeps the sequencer \naccountable and prevents the sequencer from equivocating or acting maliciously. The initial \nimplementation of the batch poster is permissioned and the user must trust that it will not \nreorder blocks produced by the sequencer. \nIntegration \nIntegrating with the Espresso Network requires minimal changes to Arbitrum Nitro’s existing \nrollup design. The Espresso Team has already done that, and in the following sections we will \nprovide a comprehensive guide for running your own instance and building on Espresso! \nComponents \nWe model the rollup as a collection of three components: \n• The sequencer  \n• The batcher  \n• The TEE contract \nTransaction Flow \nChain Config \nLocal Deployment (`docker compose`) \nOverview \nFor those seeking to evaluate their infrastructure and to get a clearer picture of what a \"working\" \nimplementation looks like, we have made available a docker config that will allow for evaluation \nof the various components of the protocol needed to gain initial familiarity with the system. \nCopy \nservices: \n  nitro: \n\n    image: ghcr.io/espressosystems/nitro-espresso-integration/nitro-node:integration \n    container_name: nitro-node \n    ports: \n      - \"8547:8547\" \n      - \"8548:8548\" \n      - \"8549:8549\" \n    command: --conf.file /config/full_node.json \n    volumes: \n      - ./config:/config \n      - ./wasm:/home/user/wasm/ \n      - ./database:/home/user/.arbitrum \n    depends_on: \n      - validation_node \n \n  validation_node: \n    image: ghcr.io/espressosystems/nitro-espresso-integration/nitro-node:integration \n    container_name: validation_node \n    ports: \n      - \"8949:8549\" \n    volumes: \n      - ./config:/config \n    entrypoint: /usr/local/bin/nitro-val \n    command: --conf.file /config/validation_node_config.json \n ",
    "filename": "11. Migrating arbitrum orbit chains to espresso.pdf"
  },
  {
    "id": 4,
    "content": "\n\nUsing the Espresso Network as a Cartesi application \nCartesi integration with Espresso \nThe Cartesi team has built an integration that enables Cartesi applications to use Espresso for \nfast confirmations, low-cost DA, and decentralized sequencing. The integration is fully \nfunctional, but it is brand new and is still undergoing review, and should therefore be used with \ncaution. Developers interested in deploying their own Cartesi application using Espresso should \nget in touch with the Cartesi team via their Discord, where they can find a channel dedicated to \nthe Espresso integration. \nIntegration overview \nThe integration enables Cartesi applications to configure their source of transactions to be their \nspecific namespace in the Espresso Network. \n \nOverview of a Cartesi application using Espresso \nThe integration is based on the concept that inputs to Cartesi dApps are of two fundamentally \ndifferent natures: \n• L2 transactions: these refer to common interactions of users with the application, and \nrefer to application-specific actions such as “swap token”, “post message”, “attack \ngoblin”, etc.; these transactions do not require any direct information or logic from the \nbase layer (i.e., the L1); \n• L1 -> L2 messages: these refer to information that is relayed from the base layer to the \nrollup application, such as informing about deposits done via the Portals, relaying the \ndApp’s address, ensuring base layer validation for a given input, etc. \nThis integration proposes that L2 transactions are to be processed “immediately” (i.e., as soon \nas they are sequenced), whereas L1 -> L2 messages are only processed when they are finalized \non L1, meaning that they are processed “with a delay”. \nAside from that, from the application’s point of view, few things change: \n• Back-end: both L2 transactions and L1 -> L2 messages are received as regular inputs; \n• Front-end: L2 transactions are signed by the client and submitted to an L2 submission \nendpoint on the node, which will then forward them to Espresso; L1 -> L2 messages are \nsubmitted exactly in the same way as current regular Cartesi rollup inputs (i.e., as a \ntransaction that eventually calls the InputBox contract’s addInput method). \n ",
    "filename": "12. Espresso as Cartessi Application.pdf"
  },
  {
    "id": 5,
    "content": "\n\nRunning an Espresso Node \nInformation on the different ways to run an Espresso node. The Espresso node is referred to as a \n'sequencer node' from here on forth. \nNote: during Mainnet 0 only a fixed set of preregistered operators can run a node. The Espresso \nNetwork will upgrade to proof-of-stake in a later release. \nBasic usage \nVisit the espresso-sequencer repository for instructions on how to run an Espresso node \nnatively or with Docker. Find the latest Docker images here. \nUsage: \nCopy \n# Run a node natively \ntarget/release/sequencer [options] -- <module>  \n \n# Run a node with the sequencer Docker image \ndocker run -it \\ \n  --name sequencer1 \\ \n  -e <env_variable> \\ \n  ghcr.io/espressosystems/espresso-sequencer/sequencer:main \\ \n  sequencer [options] -- <module> \nFor brevity, we will omit the full sequencer path from here and simply refer to the executable as \nsequencer going forward. \nThe sequencer docker-compose file is a great reference point for configuring an entire local \nsequencer network, including a few Espresso nodes, an L1 client, and the HotShot data \navailability and orchestrator servers. \nRequired parameters \nEnvironment variable \nCLI flag \nDescription \nESPRESSO_SEQUENCER_L1_PROVIDER \n--l1-provider-url \nJSON-RPC URI of the L1 provider (e.g. http://localhost:8545. \nESPRESSO_SEQUENCER_ORCHESTRATOR_URL \n--orchestrator-url \n\nURL of the HotShot orchestrator. This service will be provided by Espresso. \nESPRESSO_SEQUENCER_CDN_ENDPOINT \n--cdn-endpoint \nThe CDN's entry point in host:port form. This service will be provided by Espresso. \nESPRESSO_STATE_RELAY_SERVER_URL \n--state-relay-server-url \nURL of the state relay web server. This service will be provided by Espresso. \nESPRESSO_SEQUENCER_STATE_PEERS \n--state-peers \nComma-separated list of peer URLs to use for catchup. This may include the archival query \nservice operated by Espresso as well as URLs of your own nodes (see catchup) \nESPRESSO_SEQUENCER_KEY_FILE \n--key-file \nPath to file containing private signing keys. See key management. \nESPRESSO_SEQUENCER_GENESIS_FILE \n--genesis-file \nPath to file containing genesis state. See genesis file. \nESPRESSO_SEQUENCER_LIBP2P_BIND_ADDRESS \n--libp2p-bind-address \nThe address to bind libp2p to in host:port form. Other nodes should be able to access this. This \nshould be a UDP port. \nESPRESSO_SEQUENCER_LIBP2P_ADVERTISE_ADDRESS \n--libp2p-advertise-address \nThe address we should advertise to other nodes as being our libp2p endpoint (in host:port \nform). It should resolve a connection to the above bind address. This should be a UDP port. \nOptional parameters \nEnvironment variable \nCLI flag \nDescription \nESPRESSO_SEQUENCER_PRIVATE_STAKING_KEY \n--private-staking-key \n\nThe private staking key to use for signing consensus messages. This parameter replaces the \nrequired --key-file, and must be provided alongside --private-state-key. Sometimes it is more \nconvenient to configure keys directly instead of via a file. \nESPRESSO_SEQUENCER_PRIVATE_STATE_KEY \n--private-state-key \nThe private key to use for signing finalized consensus states. See also --private-staking-key. \nESPRESSO_SEQUENCER_IS_DA \n--is-da \nWhether or not to register for and participate in the the DA committee. \nESPRESSO_SEQUENCER_L1_EVENTS_MAX_BLOCK_RANGE \n--l1-events-max-block-range \nMaximum number of blocks allowed in a single eth_getLogs call to the L1 RPC provider \nESPRESSO_SEQUENCER_LIBP2P_BOOTSTRAP_NODES \n--libp2p-bootstrap-nodes \nA comma separated list of well-known peers to use to bootstrap the initial Libp2p connection. If \nsupplied, it will override values from the orchestrator as well as those persistently saved to any \nconfig. Addresses need to be supplied in Multiaddress Format. \nESPRESSO_SEQUENCER_IDENTITY_NODE_NAME \n--node-name \nThis is for use in Espresso's node validator dashboard. This is a string that if supplied, this value \nwill be used to identify this particular node's display name on the Espresso's node validator \ndashboard. Example: espresso-sequencer-01 \nESPRESSO_SEQUENCER_IDENTITY_COMPANY_NAME \n--company-name \nThis is for use in Espresso's node validator dashboard. This is a string that if supplied, this value \nwill be used to populate the node's organization/company name on Espresso's node validator \ndashboard. Example: Espresso Systems \nESPRESSO_SEQUENCER_IDENTITY_COMPANY_WEBSITE \n--company-website \nThis is for use in Espresso's node validator dashboard.  \nThis is a valid URL that points to the target homepage for the organization that is operating the \nsequencer. \nExample: \nhttps://www.espressosys.com \n\nESPRESSO_SEQUENCER_IDENTITY_OPERATING_SYSTEM \n--operating-system \nThis is for use in Espresso's node validator dashboard. This is a string that is meant to represent \nthe operating system that the sequencer is running on. Ideally it is meant to be the primary \nname of the underlying operating system plus a version number. If not supplied, this has a \ndefault value that comes from supplied system constant values. By default the value will just be \nthe operating system name itself. The values supplied here will show as node distribution \nstatistics on Espresso's node validator dashboard. On 'nix-based systems that can be \npopulated with the uname command: uname -sr Example: Darwin 23.6.0 Linux 5.15.0-101-\ngeneric  \nESPRESSO_SEQUENCER_IDENTITY_NODE_TYPE \n--node-type \nThis is for use in Espresso's node validator dashboard. This is a string value that is meant to \nrepresent which binary that this node is being run with. This has a default value and doesn't \nneed to be supplied at all, unless you wish to ovewrite the value here. The values supplied here \nwill show as node distribution statistics on Espresso's node validator dashboard. Example: \nespresso-sequencer 1.0.0 \nESPRESSO_SEQUENCER_IDENTITY_NETWORK_TYPE \n--network-type \nThis is for use in Espresso's node validator dashboard. This is a string that is used to identify the \ntype of network that the sequencer is run on. The values supplied here will show as node \ndistribution statistics on Espresso's node validator dashboard. This value is expected to be a \nsimple representation of the values used for distribution purposes. Ideally it should be one of \nthe following values: - Residential - Hosted - AWS - GCP - Azure If a specific cloud provider \nwould rather not be specified then Cloud Provider can suffice. If it is a well known cloud provider \nlike AWS, GCP, or Azure and availability zone could be added to the end for further distinction. \nExample: AWS AWS us-west-2 \nESPRESSO_SEQUENCER_IDENTITY_COUNTRY_CODE \n--country-code \nThis is for use in Espresso's node validator dashboard. This is a string that is meant to contain \nthe two letter Alpha 2 ISO-3166 country code of the sequencer being run. The values supplied \nhere will show as node distribution statistics on Espresso's node validator dashboard. Example: \nUS DE CN \nESPRESSO_SEQUENCER_IDENTITY_LATITUDE \n--latitude \nThis is for use in Espresso's node validator dashboard. This is a value that is a decimal \nrepresentation of the approximate latitude of the node. This value should only be supplied when \nthe longitude value is also supplied, or should omitted. This value is meant to represent the \napproximate sequencer's latitude location. If concerned about being too specific about the \n\nexact location of the node, then a location that indicates a higher administrative level is \nrecommended. \nThis value is used to locate the node on the World Map display on Espresso's node validator \ndashboard. Example: 40.417300 \nESPRESSO_SEQUENCER_IDENTITY_LONGITUDE \n--longitude \nThis is for use in Espresso's node validator dashboard. \nThis is a value that is a decimal representation of the approximate longitude of the node. This \nvalue should only be supplied when the latitude value is also supplied, or should omitted. If \nconcerned about being too specific about the exact location of the node, then a location that \nindicates a higher administrative level is recommended. This value is used to locate the node on \nthe World Map display on Espresso's node validator dashboard. Example: -82.907100 \n[libp2p] Multiaddress Format \nMultiaddresses are a self-describing format that allow us to support a wide variety of protocols \nand address types. More information can be found here.  \nHere is an example of an address that represents a node on localhost on port 2000: \nCopy \n/ip4/127.0.0.1/udp/2000/quic-\nv1/p2p/12D3KooWDtGECieXrqKoVxfDhU7afYnS6toj1GqWXuEDfcaGPDxa \nYou can also represent a fully qualified domain name using this format: \nCopy \n/dns/mynode.example.com/udp/2000/quic-\nv1/p2p/12D3KooWDtGECieXrqKoVxfDhU7afYnS6toj1GqWXuEDfcaGPDxa \nNotice how for both we have appended the node's public key as part of the address scheme. \nYou can get your own libp2p node's public key by using the dockerized public key tool: \nCopy \ndocker run ghcr.io/espressosystems/espresso-sequencer/sequencer:main pub-key -l \n\"YOUR_PRIVATE_STAKING_KEY\" \nYour libp2p key will then be printed to the console. \nGenesis file \nThe genesis file is used to store settings that must be identical between all nodes, like the \ngenesis state and parameters that impact the (in)validity of proposed blocks. Genesis files for \nall officially supported networks are bundled with the official Docker image, making it easy to \ndistribute the configuration and ensure it is the same across all nodes: \nCopy \n% docker run ghcr.io/espressosystems/espresso-sequencer/sequencer:main ls /genesis \n\ncappuccino.toml \ndemo.toml \nstaging.toml \nThis Docker image can be configured to run a node for any supported network simply by setting \nthe genesis file path appropriately. For example, the local docker-compose demo uses \nESPRESSO_SEQUENCER_GENESIS_FILE=/genesis/demo.toml. It is also possible to run a node \nfor a custom network by defining your own genesis, mounting it via a volume, and pointing the \nnode at the mounted path. \nFormat \nThe genesis file is a TOML file with the following sections: \n• [chain_config] — Parameters of the state transition function of the Espresso chain. \no chain_id — Identifier for this instance of an Espresso chain. \no base_fee — Fee per byte of data sequenced. \no max_block_size — Maximum block size allowed to be sequenced. \no fee_recipient — Address used to track amount of fees paid. \no fee_contract — Address of L1 contract used to deposit fee tokens. \n• [header] — Inputs to the genesis header. \no timestamp — The timestamp to use for the genesis header. \n• [l1_finalized] — Description of the finalized L1 block from which to process L1 events. \no number — The L1 block number. \no timestamp — The L1 block timestamp. \no hash — The L1 block hash. \n• [stake_table] — Parameters for the Espresso stake table. \no capacity — The maximum number of distinct public keys. \n• [network] — Configuration for connecting to the peer-to-peer network. \no bootstrap_nodes — List of peers to bootstrap a P2P connection. \n• [accounts] — Prefunded fee accounts, for testing purposes only. \nThe [chain_config] section \nThis section defines parameters which affect how proposed blocks are validated and how they \naffect the Espresso state. \nCopy \n[chain_config] \nchain_id = 999999999 \n\nbase_fee = '1 wei' \nmax_block_size = '1mb' \nfee_recipient = '0x0000000000000000000000000000000000000000' \nfee_contract = '0xa15bb66138824a1c7167f5e85b957d04dd34e468' \nAll fields except fee_contract are required. \nThe chain_id field \nThe Espresso chain ID is a unique identifier for a given instance of the Espresso network. Since it \nis part of the chain config, and the chain config is hashed into each block header, an untrusting \nclient can check whether a block it is looking at belongs to the chain with a certain ID, which \nprevents attacks where a malicious server provides valid blocks from the wrong chain. \nThe chain ID is also used for certain protocol transactions to prevent cross-chain replay \ntransactions, as in other blockchains like EVM-based chains. Note, however, that the most \ncommon type of transaction—rollups submitting data to be sequenced—does not reference an \nEspresso chain ID, since rollups will have their own globally unique chain IDs to prevent cross-\nchain attacks. \nThe base_fee field \nThe base fee is the amount of Ether required to sequence a byte of data. These sequencing fees \nare paid by builders submitting blocks to be finalized by consensus, and are intended to offset \nthe cost of operating the network. Note that fees are not currently distributed to node operators, \nbut are collected in a burner address. \nThe base fee can be given as a number, decimal string, or hexadecimal string (with 0x prefix), \nindicating an amount in WEI. It may also be specified as a string containing a decimal number \nand a unit. Allowed units are wei, gwei, and eth. \nThe max_block_size field \nThe maximum size of a block. Honest nodes will reject a proposed block whose payload \nexceeds this size. This is intended as a failsafe in case the base_fee does not adequately \ndisincentivize DOS attacks from submitting very large blocks. This parameter may be removed \nin the future, when the network is upgraded to use a more sophisticated, dynamic pricing \nmodel. \nThe maximum block size can be given as an integer (indicating a number of bytes) or a string \nwith a number and a unit suffix. Allowed units consist of an SI prefix k, m, g, t, p, e (though you \nwill likely never have use of prefixes larger than mega), an optional i indicating a binary system \n(factors of 1024 instead of 1000) and an optional b suffix. All of these are valid units: KiB, kb, KB, \nk. \nThe fee_recipient field \nAn address that receives all sequencing fees. \nThe fee_contract field \n\nThe address of a contract on the L1 chain which allows users and builders to deposit ETH into \nthe Espresso state. The deposited tokens can then be used for paying sequencing fees. \nThis field may be omitted, in which case deposits are disabled. In this case, builders can still \npropose if \n• They are proposing an empty block, or \n• The base_fee is set to 0, or \n• They have been funded in the genesis block \nThe [header] section \nThis section provides some inputs that will be used to construct the genesis header. \nCopy \n[header] \ntimestamp = \"1970-01-01T00:00:00Z\" \nAll fields are required. \nThe timestamp field \nThe timestamp which will be included in the genesis header. For all blocks after genesis, the \ntimestamp is calculated dynamically, based on the system time of the proposing node. But the \ngenesis header, and thus its timestamp, must be known to all nodes to even start proposing and \nvalidating blocks, so it must be configured. \nThe timestamp is given as an RFC 3339 string. \nThe [l1_finalized] section \nThis section determines the finalized L1 block from which Espresso will start processing events \n(like deposit events). The first block proposed by Espresso will process all events from the \ngenesis l1_finalized block to the current finalized block, so this block should be recent enough \nthat all thoes events can be processed within the consensus view timeout. Subsequent blocks \nwill process events incrementally from the previous finalized L1 block to the current one. \nIf the L1 finalized block has not yet been finalized when a node is started, the node will wait for it \nto be finalized before starting consensus. \nCopy \n[l1_finalized] \nnumber = 6001199 \ntimestamp = \"0x66575298\" \nhash = \"0xcbc41e0637d18c879a9d6f09fb5046a37bae99d5637d12716b70fdd027a9135e\" \nIf all fields are specified, the node will only accept the exact block described. If only number is \nspecified, the node will fetch the details of that block using its L1 provider. \nThe number field \n\nThe block number of the finalized L1 block to use at Espresso genesis.  \nThis field is required. \nThe timestamp field \nThe timestamp of the finalized L1 block to use at Espresso genesis.  \nThis field may be omitted, in which case the timestamp will be fetched from the block indicated \nby number. Note that if timestamp is omitted, all other details must be omitted as well, and all \ndetails will be fetched based on number. \nThe hash field \nThe hash of the finalized L1 block to use at Espresso genesis.  \nThis field may be omitted, in which case the hash will be fetched from the block indicated by \nnumber. Note that if hash is omitted, all other details must be omitted as well, and all details \nwill be fetched based on number. \nThe [stake_table] section \nThis section configures the layout of the Espresso stake table. Note that this does not actually \ndefine the stake table. Currently the stake table is defined via the orchestrator service that \nnodes register with when they come online. Eventually, it will be defined by an L1 contract. This \nsection of the genesis file only configures how the stake table is represented as a Merkle tree for \nproving purposes. \nCopy \n[stake_table] \ncapacity = 200 \nThe capacity field \nThe maximum number of distinct nodes which can be represented by the stake table. After \ngenesis, this can only be expanded with a network upgrade. \nThe [accounts] section \nUse in testing environments only. \nThis section allows the definition of some prefunded accounts, for paying sequencing fees. This \nis useful for easily setting up a testing environment where a builder has enough funds to build \nblocks. However, this section must only be used in testing. Since the resulting funds are created \nby fiat instead of being bridged from L1, they do not actually correspond to tokens locked in the \nbridge contract on L1, and thus use of this section can cause the bridge contract to become \ninsolvent. \nThe body of this section is a list of key-value pairs, where the keys are addresses and the values \nindicate the balance of those accounts at Espresso genesis. The balances can be given as \nnumbers, decimal strings, or hexadecimal strings (representing an amount in WEI), or as strings \ncontaining a decimal number and a unit. Allowed units are wei, gwei, and eth. \nCopy \n\n\"0x23618e81E3f5cdF7f54C3d65f7FBc0aBf5B21E8f\" = 100000 \n\"0x184ba627DB853244c9f17f3Cb4378cB8B39bf147\" = \"0xabcdef\" \n\"0x184ba627DB853244c9f17f3Cb4378cB8B39bf147\" = \"1 eth\" \nThe [[upgrade]] section \nThis section details the parameters and settings for performing a consensus protocol upgrade. \nUpgrades ensure that nodes move to a new version, applying changes such as fee adjustments \nor applying new features \nThe [[upgrade]] section specifies the version for which the upgrade should be applied. It also \nincludes the hotshot configuration parameters. Hotshot provides two modes for upgrades: \ntime-based and view-based \nView: \nCopy \n[[upgrade]] \nversion = \"0.2\" \nstart_proposing_view = 5 \nstop_proposing_view = 400 \nTime: \nCopy \n[[upgrade]] \nversion = \"0.2\" \nstart_proposing_time = “2024-09-17T16:00:00Z” \nstop_proposing_time = “2024-09-18T16:00:00Z” \nView based: \n• start_proposing_view: the earliest view in which the node can propose an upgrade. This \nshould be set to when an upgrade is intended. \n• stop_proposing_view: view after which the node stops proposing an upgrade \nTime based: \n• start_proposing_time: the earliest UNIX timestamp in which the node can propose an \nupgrade. \n• stop_proposing_time: UNIX timestamp after which the node stops proposing an \nupgrade. \nThe window between start_proposing_view/time and stop_proposing_view/time should provide \nsufficient time e.g 300 views or 5-10 minutes for nodes to continue proposing the upgrade until \nsuccessful. \n\nFor each upgrade, the upgrade type along with its associated parameters needs to be defined. \nDifferent upgrade types can be defined by creating separate sections within the within the \n[[upgrade]] array of tables. Currently only two types are supported: Fee and marketplace \nNote: currently we only support one upgrade per run. To perform multiple upgrades, the \nsequencer binary needs to be restarted after each upgrade. \nFee upgrade example: \nCopy \n[upgrade.fee] \n \n[upgrade.fee.chain_config] \nchain_id = 999999999 \nbase_fee = '1 wei' \nmax_block_size = '1mb' \nfee_recipient = '0x0000000000000000000000000000000000000000' \nfee_contract = '0xa15bb66138824a1c7167f5e85b957d04dd34e468' \nclick here for more details on upgrades. \nOptional modules \nThe Espresso node supports a set of optional modules that extend the node with useful APIs \n(e.g. transaction submission or query functionality). Here we describe what these modules do \nand how to enable them. \nIn general, modules are enabled in the following way: \nCopy \n# Run an Espresso node with a couple of modules enabled \nsequencer -- <module1_name> --<arg1_name> <arg1_value> -- <module2_name>  \nHTTP \nThis module runs a basic HTTP server that comes with healthcheck and version endpoints. \nAdditional endpoints can be enabled with the modules listed below. \nUsage: \nCopy \nsequencer -- http --port 50000 \nParameters: \nEnv Variable \nCLI Flag \n\nDescription \nESPRESSO_SEQUENCER_API_PORT \n--port \nPort that the HTTP API will use. \nESPRESSO_SEQUENCER_MAX_CONNECTIONS \n--max-connections \nMaximum number of concurrent HTTP connections that the server will allow. E.g. 100. \nStatus \nThis module extends the HTTP API with telemetry and consensus metrics (e.g. an endpoint to \nretrieve the latest block height). \nUsage: \nCopy \nsequencer -- http -- status  \nThis will add a Prometheus endpoint GET /status/metrics containing useful metrics for \nmonitoring the performance of the node and network. See also monitoring. \nCatchup \nThis module extends the HTTP API with a module that serves queries for pending consensus \nstate. Other nodes can connect to this API to quickly sync with the latest state in the event that \nthey fall out of sync with consensus. \nUsage: \nCopy \nsequencer -- http -- catchup \nConfig \nThis module extends the HTTP API with a module that provides the node config information. This \nwill add two GET endpoints : /config/hotshot and /config/env.  \nUsage: \nCopy \nsequencer -- http -- config  \nThe /hotshot endpoint retrieves the Hotshot config for the current node, excluding the private \nkeys. \nThe /env endpoint outputs all Espresso environment variables set for the current node. The keys \nfor these variables are defined in the public-env-vars.toml file in the crates/sequencer/api \ndirectory. \nQuery \n\nThis module enables a HotShot query service API that connects to a persistent store containing \nthe history of the blockchain. This API provides endpoints that rollups can use to integrate with \nthe sequencer. \nThis module must be enabled alongside the http module. The query API can be accessed at the \nport specified by the http module. This option also requires a storage module, which defaults to \nstorage-fs (see below for more details on storage options). \nUsage: \nCopy \nsequencer -- http -- query  \nEnv Variable \nCLI Flag \nDescription \nESPRESSO_SEQUENCER_API_PEERS \n--peers \nA comma-separated list of peer query service URLs to fetch missing data from. \nFilesystem Storage \nThis module enables a local storage backend for the query service and consensus state. \nEventually, the backend will also store DA blocks and VID shares. This setting is useful for \ntesting and debugging, but is not recommended for production nodes because it is currently not \nvery stable or performant. Long term, we hope to improve this storage option. \nUsage: \nCopy \nsequencer -- http -- query -- storage-fs --path ./storage-path \nParameters: \nEnv Variable \nCLI Flag \nDescription \nESPRESSO_SEQUENCER_STORAGE_PATH \n--path \nStorage path for persistent data. \nSQL storage \nThis module enables a postgres storage backend for the query service and consensus state. \nEventually, the backend will also store DA blocks and VID shares. This setting is recommended \nfor production nodes. \n\nUsage: \nCopy \nsequencer -- http -- query -- storage-sql  \nParameters: \nEnvironment variable \nCLI flag \nDescription \nN/A \n--uri \nThis is a shorthand for setting a number of other options all at once in URI form. Components of \nthis URI can overridden by the parameters below. Example: \npostgres[ql]://[username[:password]@][host[:port],]/database[?parameter_list]. \nESPRESSO_SEQUENCER_POSTGRES_HOST \n--host \nHostname for the remote Postgres database server. \nESPRESSO_SEQUENCER_POSTGRES_PORT \n--port \nPort for the remote Postgres database server. \nESPRESSO_SEQUENCER_POSTGRES_DATABASE \n--database \nName of database to connect to. \nESPRESSO_SEQUENCER_POSTGRES_USER \n--user \nPostgres user to connect as. \nESPRESSO_SEQUENCER_POSTGRES_PASSWORD \n--password \nPassword for Postgres user. \nESPRESSO_SEQUENCER_POSTGRES_USE_TLS \n--use-tls \nUse TLS for an encrypted connection to the database. \nESPRESSO_SEQUENCER_POSTGRES_PRUNE \n--prune \n\nUse this flag or set variable to true to enable pruning of historical data \nESPRESSO_SEQUENCER_PRUNER_PRUNING_THRESHOLD \n--pruning-threshold \nIf storage usage exceeds this threshold (in bytes), data younger than the target retention will be \npruned (possibly up to the minimum retention) \nESPRESSO_SEQUENCER_PRUNER_MINIMUM_RETENTION \n--minimum-retention \nThe minimum time which data must be retained, regardless of storage usage \nESPRESSO_SEQUENCER_PRUNER_TARGET_RETENTION \n--target-retention \nThe desired amount of time to retain data, storage permitting \nESPRESSO_SEQUENCER_PRUNER_BATCH_SIZE \n--batch-size \nThe number of objects to delete at once when pruning \nESPRESSO_SEQUENCER_PRUNER_MAX_USAGE \n--max-usage \nThe maximum fraction of pruning-threshold to use. If storage usage exceeds pruning-threshold, \nit will be pruned back to this fraction of pruning-threshold. Expressed as an integer on a scale of \n1 to 10000. \nESPRESSO_SEQUENCER_PRUNER_INTERVAL \n--internval \nInterval for running the pruner. \nESPRESSO_SEQUENCER_FETCH_RATE_LIMIT \n--fetch-rate-limit \nMaximum number of simultaneous requests allowed when fetching missing data from peers. \nSetting this can limit the load placed on peers by catchup and in turn make catchup more \nefficient. E.g. 25. \nESPRESSO_SEQUENCER_ACTIVE_FETCH_DELAY \n--active-fetch-delay \nThe minimum delay between requests to fetch data from another node, when syncing a DA or \narchival node. This can be used to ensure this node complies with the upstream node's rate \nlimit. E.g. 50ms. \nESPRESSO_SEQUENCER_CHUNK_FETCH_DELAY \n\n--chunk-fetch-delay \nThe minimum delay between loading chunks of data in a sequential stream. This can limit the \nload a node places on its own database, especially for streaming from old blocks, where lots of \ndata is loaded from the database eagerly. E.g. 100ms. \nSubmit \nThis module extends the HTTP API with a POST endpoint to submit a transaction for sequencing. \nUsage: \nCopy \nsequencer -- http -- submit \nKey management \nEach Espresso node needs two signing key pairs to run: \n• The staking key is a BLS key used to sign consensus messages (votes, proposals), and it \nsupports efficient signature aggregation, important for consensus performance. \n• The state key is a Schnorr key used to sign finalized consensus states, which in turn \ndrives the onchain Espresso light client on L1. \nThese keys are typically stored in a .env file within the sequencer container, and the sequencer \nis configured via ESPRESSO_SEQUENCER_KEY_FILE to load private keys from this file. While \nyou are welcome to generate these keys however you like, as long as they have the right \nperformance, Espresso provides a utility program keygen which is distributed with the \nsequencer image. The simplest way to generate keys is to run the following command: \nCopy \ndocker run $IMAGE keygen -o /keys \n$IMAGE is the ID or name of the sequencer node Docker image, such as \nghcr.io/espressosystems/espresso-sequencer/sequencer:main. You can use Espresso's pre-\nbuilt images or build the image yourself from the source code. This command will generate a file \ncalled /keys/0.env in the Docker container containing the private keys. It will also print the \ngenerated public keys in the terminal. You can then pass this file to the sequencer by setting \nESPRESSO_SEQUENCER_KEY_FILE=/keys/0.env. \nThis method of generating keys is nice because the keys never leave the Docker container where \nthey will be used. If, however, you want to store the keys on the host machine as well, or if you \nwant to use a different container to generate the keys than you will use to run the sequencer \n(such as a one-off container from docker run) you need only create a Docker volume to store the \nkeys in a host directory, such as  \nCopy \ndocker run -v ./keys:/keys $IMAGE keygen -o /keys \nThis will store the generated keys at ./keys on the host. You can then pass them into the \nsequencer by mounting the same volume in the sequencer container. \n\nThe keygen utility has some additional options which you can view by running with --help. One \nof the most useful is --seed <SEED>, to use a seed for generating the keys deterministically, \ninstead of using entropy from the OS. This is particularly useful if you want to use your own \nentropy instead of the default entropy source: you can generate a randomized seed however you \nlike and then pass it to the keygen program. The seed is a 32-byte integer encoded as hex (with \nno 0x prefix). \nMonitoring \nWhen running the status API, the performance of an Espresso node can be monitored using \nPrometheus tools, by monitoring the endpoint /status/metrics. Some of the most important \nmetrics to monitor include: \n• consensus_current_view: should be incrementing once every 1-2 seconds. In rare cases \nit is acceptable for this metric to remain static for up to 1 minute. If not increasing, it \nmay mean the network has lost liveness, or your node has fallen out of sync with the \nnetwork. \n• consensus_last_decided_view: should be increasing mostly in tandem with \nconsensus_current_view. If current_view is increasing but last_decided_view is not, it \nindicates a network-wide problem with consensus state, or a recurring problem with \nbuilders or proposers. \n• consensus_number_of_timeouts_as_leader - The number of slots (views) this leader \nhas failed to propose a block for. If this is increasing frequently, there is potentially an \nissue with your node. \n• consensus_libp2p_num_connected_peers - The number of connected peers on the \nLibp2p network. This should be nonzero. \n• consensus_outstanding_transactions: smaller is better, and this metric should not \nshow any trend over time. If it is especially large (relative to volume) or increasing over a \nlong period of time, it may indicate that your node is not garbage collecting the public \nmempool properly.  \nHere are some important logs to look out for as a secondary measure: \n• Vote sending timed out in ViewSyncTimeout - We entered a view synchronization phase \nand did not immediately leave. We expect to see this in some cases (e.g. some leaders \nare failing in a row), but it's indicative of an issue if it does not eventually resolve. \n• Failed to publish proposal - As the consensus leader, this prints if we fail to propose a \nblock for some reason. It may be fine to happen occasionally (e.g. we just restarted and \nare still catching up) but lots of these are a problem. \n• Progress: entered view - Every 100 views, we print this log with the view number. With \nthe current block builder we expect to see this every ~13 minutes or less. \nModes \nIt is possible to run an Espresso node in three modes, differentiated by how long the nodes store \nhistorical data. \nLightweight node \n\nA lightweight node stores only the data needed to run consensus. It does not keep any historical \ndata, and it is not eligible to be on the consensus DA committee. It has negligible storage \nrequirements on the order of kilobytes. A lightweight node is any node running without the \noptional query module. \nArchival node \nAn archival node stores all historical data in perpetuity, and is thus able to serve queries for \narbitrary historical state. It is eligible to be on the DA committee. Its storage requirements \ndepend on how much data the network is processing, which in turn depends on how much the \nnetwork is being used. In testnets, this has been on the order of tens of gigabytes per month, but \nin mainnet this may be more. \nTo run an archival node, simply enable the optional query module without any of the pruning \noptions. \nDA node \nPruning of old data is not yet supported with filesystem storage, and thus it is only possible to \nrun a DA node with Postgres storage at this time. \nA DA node provides data availability for recently finalized data. It is eligible to be one the DA \ncommittee, because it will make data available for long enough for an archival node to fetch it \nand persist it, before the data is pruned from the DA node. Unlike the archival node, the DA node \nhas bounded storage requirements. \nThe storage requirements for a DA node are determined by how long we want it to retain data in \nthe worst case. Typically, we want DA nodes to retain data for 1 week under average load, and a \nminimum of 1 day under worst case load, which give archival nodes time to ensure the data is \npersisted long term. Detailed hardware requirements for DA nodes are given below. \nTo run a DA node, enable the optional query module as you would for an archival node, but \nadditionally set pruning parameters: \n• ESPRESSO_SEQUENCER_POSTGRES_PRUNE=true \n• ESPRESSO_SEQUENCER_PRUNER_MINIMUM_RETENTION=1d \n• ESPRESSO_SEQUENCER_PRUNER_TARGET_RETENTION=7d \n• ESPRESSO_SEQUENCER_PRUNER_PRUNING_THRESHOLD set to the worst case \nstorage usage, in bytes, based on the hardware requirements \n• ESPRESSO_SEQUENCER_IS_DA=\"true\" \nHardware requirements \nHardware requirements are still in flux as we refine our testnets and add new features, but for \nnow we recommend the following: \nRAM: 16-32 GB. \nCPU: 2-4 Cores. \nStorage (DA node): 20 GB minimum, ability to scale to 1 TB on demand. \n\nStorage (non-DA Node): Negligible, kilobytes \n ",
    "filename": "13. running espresso node.pdf"
  },
  {
    "id": 6,
    "content": "\n\nRunning a Builder \nInformation about running a builder for the Espresso Network \nOverview \nParticipants creating blocks for the Espresso Network must run a builder, a piece of software \nwhich tracks the state of HotShot consensus so that it is able to and propose blocks at the \ncorrect time. The builder functions to create a block filled with transactions, drawing from \ntransactions accessible in Espresso's public mempool as well as its own private mempool.  \nEspresso provides a simple builder implementation, which participants can run out-of-the-box \nor build their own software on top of. This document describes how to run the basic builder \nsoftware.  \nFor comprehensive guidance on the design of an Espresso builder, it is recommended to refer to \nthe hotshot-builder-core repository. Furthermore, to gain insight into the process of enabling \nbuilder services for a sequencer, pertinent information can be found at espresso-sequencer-\nbuilder. For access to the most recent builder Docker images, please visit here. \nUsage \nCopy \n# Clone the espresso-sequencer repository \ngit clone https://github.com/EspressoSystems/espresso-sequencer \n \n# Build the executable in release mode \ncargo build --release \n \n# Run a builder natively \ntarget/release/permissionless-builder [options] \n \n# To understand more about the available builder options \ntarget/release/permissionless-builder -h \n \n# Run a node with the builder docker image \ndocker run -it \\ \n  [-e ENV=VALUE...] \\ \n  ghcr.io/espressosystems/espresso-sequencer/builder:main \nFor a quick start, we recommend referring to the espresso-sequencer docker-compose file, and \nlooking particularly at permissionless-builder. \n\nParameters & options \nEnvironment variable \nCLI flag \nDescription \nESPRESSO_SEQUENCER_HOTSHOT_EVENT_STREAMING_API_URL \n--hotshot-event-streaming-url \nURL of hotshot events API running on Espresso Sequencer DA committee node. A builder will \nsubscribe to this server to receive hotshot events. (e.g. http://localhost:8081) \nESPRESSO_BUILDER_ETH_MNEMONIC \n--eth-mnemonic \nMnemonic phrase for builder account (e.g. \"test test test test test test test test test test test \njunk\") \nESPRESSO_BUILDER_ETH_ACCOUNT_INDEX \n--eth-account-index \nIndex of a funded account. Note: This account must be funded in Espresso, meaning ETH must \nbe bridged from the L1 \nESPRESSO_BUILDER_L1_PROVIDER \n--l1-provider-url \nA Url builder will use for RPC communication with L1 (e.g. http://demo-l1-network:8545) \nESPRESSO_SEQUENCER_STATE_PEERS \n--state-peers \nPeer nodes use to fetch missing state \nESPRESSO_SEQUENCER_CHAIN_ID \n--chain-id \nUnique identifier for an instance of the sequencer network \nESPRESSO_SEQUENCER_MAX_BLOCK_SIZE \n--max-block-size \nMaximum allowed size (in bytes) for a block \nESPRESSO_SEQUENCER_BASE_FEE \n--base-fee \nMinimum fee in WEI per byte of payload \nESPRESSO_BUILDER_SERVER_PORT \n\n--port \nPort to run builder server on through which sequencer node can query builder provided APIs (e.g \n41003) \nESPRESSO_BUILDER_BOOTSTRAPPED_VIEW \n--view-number \nBootstrapping View number (e.g. 0) \nESPRESSO_BUILDER_CHANNEL_CAPACITY \n--channel-capacity \nThe most outstanding hotshot events a builder wants at a point in time (e.g. 1024) \nESPRESSO_BUILDER_WEBSERVER_RESPONSE_TIMEOUT_DURATION \n--max-api-timeout-duratio \nThe amount of time a builder can wait before timing out a request to the API (e.g 1s) \nESPRESSO_BUILDER_BUFFER_VIEW_NUM_COUNT \n--buffer-view-num-count \nThe number of views to buffer before a builder garbage collects its state (e.g. 15) \nHardware requirements \nHardware requirements are subject to change as new features are added, but for now we \nrecommend the following: \n• Memory: 4-8 GB \n• CPU: 2-4 Cores \n ",
    "filename": "14. running a builder.pdf"
  },
  {
    "id": 7,
    "content": "\n\nBridging with the Espresso Network \nUsing Espresso confirmations for bridging \nChains that use the Espresso Network to secure their sequencer confirmations benefit from \nenhanced security when interfacing with external systems, such as bridges, exchanges, and \nother rollups. This page describes how to use a caffeinated node to quickly and securely derive \nrollup state from data confirmed by Espresso. A caffeinated node can in turn be used: \n• by solvers, to quickly release liquidity on behalf of users when bridging out of this \nchange, with minimized risk \n• by message passing bridges, to deliver a message on a destination chain quickly without \nrequiring the destination chain to trust the source chain's sequencer \n• by exchanges, to quickly and safely fill deposits from the caffeinated chain \nSince a caffeinated node is just a modified version of the normal rollup full node, interacting \nwith one once you have it running is easy: just use the usual RPC interface. Thus it should be \npossible to plug a caffeinated node into your existing solver, bridging protocol, or exchange just \nby updating your RPC URL. \nUsing a Caffeinated Node on a Supported Stack \nEspresso Systems is developing caffeinated versions of the node software for each stack which \nwe are developing an Espresso integration for. These caffeinated nodes are provided as Docker \nimages, which can be run in the exact same way as a normal full node for each stack, with a few \nextra parameters for connecting to Espresso. Detailed documentation on running each node \nwill be provided with each release. \nThe latest list of supported stacks is: \n• Arbitrum Nitro (expected early April 2025) \n• OP stack (expected Q2 2025) \nWriting a Caffeinated Node for a New Rollup Stack \nIf Espresso Systems has not yet developed an integration for your rollup stack, it is entirely \npossible to develop your own. All the tech and endpoints are open source. This section gives a \nbrief outline for approaching caffeinated node development. \nThe basic idea is to modify your full node to read inputs from Espresso, rather than reading \ninputs from an inbox contract on the L1 or from a sequencer feed. As such, the integration will \nprimarily change how data is ingested. It should not require any changes to the core state \ntransition function of the rollup. \nYou will need a connection to a server running the Espresso API. Espresso Systems maintains \npublicly accessible endpoints for mainnet and our Decaf testnet. \nThe basic flow for ingesting data from Espresso is this, replacing whatever your rollup currently \ndoes to read data from a sequencer or inbox contract, and resulting in a stream of blocks which \ncan be executed via the normal state transition function: \n• Read the next header from Espresso. This can be done in streaming or polling fashion. \n\n• Verify that this header is finalized. This is done in two parts: \no by verifying a light client proof for HotShot consensus, to verify that the current \nheader or a later one is finalized. The consensus light client is currently in \ndevelopment, so existing integrations read a finalized header from the light client \ncontract on the base layer. Espresso Systems currently operates light client \ncontracts on Ethereum and Arbitrum One as well as testnets. \no by verifying a Merkle proof relative to the known finalized header from the \nprevious step proving the inclusion of the desired header in the Espresso history. \nThis is only necessary if the previous step verified a later header. The Merkle \nproof can be fetched from an untrusted query service via the block-state API. \n• Retrieve namespace data from Espresso. This gives you the data which was confirmed \nfor your specific rollup in the finalized Espresso block, along with a proof of Espresso \nconfirmation. \n• Verify the namespace proof. This is done by checking a VID proof. The open source \nEspresso codebase includes Rust and Go libraries for verifying these proofs. \n• Namespace verification terminates in a list of Espresso transactions for your rollup's \nnamespace. The caffeinated node must verify that each of these transactions is from an \nauthorized sequencer, or else ignore them, since posting data to Espresso for a given \nnamespace is permissionless. In most rollups with a centralized sequencer, this just \nrequires checking a signature, which should be included with the transaction in the data \nsent to Espresso. \n• Each Espresso transaction is a byte string. In a typical rollup integration, these byte \nstrings encode batches or blocks. From here, you will decode these bytes into your \nrollup-specific data structure, and execute them exactly as you would had you read data \nfrom a sequencer feed or inbox contract. \n ",
    "filename": "15. briding with the espresso network.pdf"
  },
  {
    "id": 8,
    "content": "\n\nEspresso API \nReference for REST APIs served by Espresso nodes and query services \nModularity \nThe Espresso API comprises several independent modules serving different purposes and \nrequiring different resources. A given node may serve one, or all, or any combination of these \nmodules, depending on its role in the system and the resources available to it. To see a list of \nthe modules available from a particular node, navigate to the root URL of that node's API. \nIn brief, the available API modules are: \n• Status API: node-specific state and consensus metrics \n• Catchup API: serves recent consensus state to allow peers to catch up with the network \n• Availability API: serves data recorded by the Tiramisu DA layer, such as committed \nblocks \n• Node API: complements the availability API by serving eventually consistent data that is \nnot (yet) necessarily agreed upon by all nodes \n• State API: serves consensus state derived from finalized blocks \n• Events API: streams events from HotShot \n• Submit API: allows users to submit transactions to the public mempool \nContent Types \nAll APIs support JSON and binary formats in both request and response bodies. \n• The JSON format is a straightforward serialization of the data types used internally by \nconsensus nodes. In the future, a formal specification will be published, and the API will \nconform to that specification. \n• The binary format is the bincode serialization of consensus data types, prefixed with an \n8-byte version header. In the future, this will be replaced with a binary format with better \ncross-language support, and the data types will be defined by a published schema, \nrather than generated from code. \nIn requests and responses, the JSON format is denoted by the MIME type application/json, and \nthe binary format by application/octet-stream. For requests with a body, the content type of the \nbody must be set via the Content-Type header. The desired content type of the response can be \ncontrolled via the Accept header of the request. If the Accept header does not preference either \nformat, JSON will be used by default. \nServer-Hosted Documentation \nAll Espresso API servers provide self-hosted API documentation, which makes it easy to see \nexactly what APIs the server supports, and can be easier to browse than these docs. The root \nURL of an application, e.g. my-server.xyz, lists the supported API modules and versions. \nClicking on any API module, or navigating to the root of that API, e.g. my-server.xyz/status, \ndocuments the endpoints available in that module. \n\nVersioning and Stability \nA node may serve multiple major versions of a given API at the same time. The desired version \ncan be selected via a URL prefix. For example, my-server.xyz/v0/status/metrics and my-\nserver.xyz/v1/status/metrics both hit the same endpoint, but in different API versions. A URL \nwith no version segment will get a permanent redirect response to the latest supported version. \nIn this case, /status/metrics would redirect to /v1/status/metrics. \nWhenever a breaking change is made to an API, a new major version will be created, and the old \nversion will continue to be served for some time, giving clients time to upgrade to the new \nversion whenever it is convenient for them. Note that non-breaking changes, such as adding \nnew endpoints, may be made in place to existing versions. \nTo see a list of versions of an API supported by a server, visit the root URL of that server. \nTypes \nThese types are used in requests and responses across many of the API modules. \nPrimitives \nInteger \nWe use integer to represent any JSON integer, with a maximum size of at least 2^63 - 1. Of \nspecial note, byte arrays are sometimes represented as arrays of integers ([integer]). When the \ntype [integer] is used as a byte array, each integer therein is restricted to the range [0, 255]. \nHex \nIn the following, we use the type hex to indicate a hex-encoded binary string, with 0x prefix. \nBase 64 \nIn the following, we use the type base64 to indicate a base64-encoded binary string, using the \nstandard base 64 alphabet with padding. \nTagged Base 64 \nSome types use an enhanced tagged base 64 encoding, which consists of a prefix identifying \nthe type of the encoded object, a ~, and then a base 64 string using the URL-safe base 64 \nalphabet without padding. The base 64 string encodes the binary representation of a typed \nobject, plus a checksum. Because the encoding is URL-safe, these strings can be used not only \nin request and response bodies, but also in URL paths. The checksum allows the server to \nprovide useful errors if a tagged base 64 string is mistyped or corrupted. The tag makes it easy \nfor a human to tell different types of objects apart. \nFor example, a transaction hash might be encoded like \nTX~QDuwVkmexu1fWgJbjxshXcGqXku838Pa9cTn0d-v3hZ-, while a block hash could look like \nBLOCK~00ISpu2jHbXD6z-BwMkwR4ijGdgUSoXLp_2jIStmqBrD. \nWe use the type tagged<TAG> to indicate a tagged base 64 object with the given tag, as in \ntagged<TX> or tagged<BLOCK>. \nNamespaceTable \nCopy \n\n{ \n    \"bytes\": base64 \n} \nChainConfig \nA chain config determines properties of consensus, such as the base fee for sequencing and \nthe chain ID. To save space, it can be represented either as the full config object (Left variant \nbelow) or as a commitment to the chain config (Right variant). The genesis header will always \ncontain the full config, so clients can fetch the full config from genesis and then compare its \ncommitment against any other header. \nCopy \n{ \n    \"chain_config\":  \n        { \"Left\": { \"chain_id\": hex, \"max_block_size\": integer, \"base_fee\": hex } } \n        | { \"Right\": tagged<CHAIN_CONFIG> } \n} \nHeader \nCopy \n{ \n    \"height\": integer, \n    \"timestamp\": integer, \n    \"l1_head\": integer, \n    \"l1_finalized\": null | { \n        \"number\": integer, \n        \"timestamp\": hex, \n        \"hash\": hex \n    }, \n    \"payload_commitment\": tagged<HASH>, \n    \"builder_commitment\": tagged<BUILDER_COMMITMENT>, \n    \"ns_table\": NamespaceTable, \n    \"block_merkle_tree_root\": tagged<MERKLE_COMM>, \n    \"fee_merkle_tree_root\": tagged<MERKLE_COMM>, \n    \"fee_info\": { \"account\": hex, \"amount\": hex }, \n\n    \"chain_config\": ChainConfig \n} \nPayload \nCopy \n{ \n    \"raw_payload\": base64, \n    \"ns_table\": NamespaceTable \n} \nVidCommon \nCopy \n{ \n    \"poly_commits\": tagged<FIELD>, \n    \"all_evals_digest\": tagged<FIELD>, \n    \"payload_byte_len\": integer, \n    \"num_storage_nodes\": integer, \n    \"multiplicity\": integer \n} \nLeaf \nCopy \n{ \n    \"view_number\": integer, \n    \"justify_qc\": QC, \n    \"parent_commitment\": string, \n    \"block_header\": Header, \n    \"proposer_id\": string, \n} \nTransaction \nCopy \n{ \n    \"namespace\": integer, \n    \"payload\": base64 \n\n} \nIf using the Rust API, you may notice that namespace is represented by a u64. However, some \ninternal sub-protocols represent the namespace as a u32, and thus the maximum allowable \nnamespace ID is 4294967295 (2^32 - 1). Larger namespace IDs will be rejected on transaction \nsubmission. \nMerkleProof \nThe low-level proof type for a Merklized data structure. The specific format of this type is not \ncurrently specified, but it can be deserialized and interpreted in Rust using the MerkleProof \ntype. \nNsIndex \nThe 0-based position of a namespace in a NamespaceTable. The index is a little-endian byte-\nencoded 4-byte integer, as in [3, 2, 1, 255] (0xff010203). \nNsProof \nA proof that a certain list of transactions corresponds to a certain namespace in a block. \nCopy \n{ \n    \"ns_index\": NsIndex, \n    \"ns_payload\": base64, // binary encoding of the namespace data \n    \"ns_proof\": { \n        \"prefix_elems\": tagged<FIELD>, \n        \"suffix_elems\": tagged<FIELD>, \n        \"prefix_bytes\": [integer], \n        \"suffix_bytes\": [integer] \n    } \n} \nns_proof is a low-level range proof in the Espresso ADVZ VID scheme. The details of this proof \nare out of scope of this document, but this JSON object corresponds to the \nLargeRangeProofType, and can be manipulated in Rust using that type. \nStatus API \nNode-specific state and consensus metrics \nThis API provides insight into the inner workings of consensus. It is primarily useful to the \noperator of the node, as many of the metrics provided here make for useful alerts. \nEndpoints \nGET /status/block-height \n\nThe last known block height of the chain. \nReturns integer \nGET /status/success-rate \nThe view success rate since genesis. This equals the number of views completed divided by the \nnumber of successful views, i.e. the block height. \nReturns float \nGET /status/time-since-last-decide \nThe elapsed time, in seconds, since consensus last decided on a block. Useful to alert when \nconsensus is stalled or this node has been disconnected from consensus. \nReturns integer \nGET /status/metrics \nExposes all metrics recorded by consensus in Prometheus format. \nCatchup API \nServes recent consensus state to allow peers to catch up with the network \nThe primary customer of this API is peer consensus nodes who may have recently joined the \nnetwork or were temporarily disconnected. These nodes need the very latest state, one which \nhasn't even been finalized yet, in order to start voting and proposing in consensus. \nIn HotShot, all state required to participate is represented in the form of Merkle trees or Merkle \ntries, so this API is able to provide select segments of the state with a proof that will convince \nthe client that the returned segment is accurate, as long as they know the corresponding state \ncommitment. \nTypes \nAccount \nCopy \n{ \n    // Account balance in WEI. Serialized as a hex string so as not to exceed the \n    // range of JSON integers. \n    \"balance\": hex, \n    // Merkle proof justifying \"balance\" relative to the state commitment. \n    \"proof\": { \n        \"account\": hex, \n        \"proof\": \n            { \"Presence\": MerkleProof }, \n\n          | { \"Absence\": MerkleProof } \n        } \n    }        \n} \nEndpoints \nGET /catchup/account/:address \nGet the balance of the requested fee account in the latest finalized state. \nParameters \nName \nType \nDescription \naddress \nhex \nThe account (Ethereum address) to look up \nReturns Account \nGET /catchup/:view/account/:address \nGet the balance of the requested fee account in the state from the requested consensus view. \nThis is used to fetch the state for unfinalized views, to facilitate rapid catchup. If :view has \nalready been finalized, this endpoint may fail with error 404. \nParameters \nName \nType \nDescription \nview \ninteger \nThe view from which to look up the account balance \naddress \nhex \nThe account (Ethereum address) to look up \nReturns Account \nGET /catchup/blocks \n\nGet the Merkle frontier (path to most recently inserted element) of the accumulator of blocks, \nfrom the most recently finalized state. This frontier is sufficient to append new blocks, so it is all \nthat is needed for state catchup. \nReturns MerkleProof \nGET /catchup/:view/blocks \nGet the Merkle frontier (path to most recently inserted element) of the accumulator of blocks, \nfrom the requested consensus view. This is used to fetch the state for unfinalized views, to \nfacilitate rapid catchup. If :view has already been finalized, this endpoint may fail with error 404. \nParameters \nName \nType \nDescription \nview \ninteger \nThe view from which to look up the frontier \nReturns MerkleProof \nAvailability API \nServes data recorded by the Tiramisu DA layer, such as committed blocks \nThe availability API is the place to get onchain data, like blocks and transactions. It is the \nprimary interface for downstream components like rollups and end users. \nThe API is designed to be robust and pure. Robust means that if the node hosting the API misses \nsome data, for example from being offline when a certain block was finalized, it will \nautomatically fetch the missing data from a peer, and will eventually fetch and store all finalized \ndata. Pure means that each endpoint is a pure function -- with the exception of occasionally \nreturning 404 for missing data, each endpoint will always give the same response given the \nsame parameters. \nDue to purity, this API provides no aggregate queries, like block or transaction counts, which \nmight change as missing data is fetched. Likewise, every endpoint takes some specification of \nthe exact point in the chain the client is looking for, like a block height or hash. There is no \n\"latest block\" query. Thus, most real-world use cases will need to complement the availability \nAPI with use of the node API. \nOrganization \nWhile this API has many endpoints, don't be intimidated -- there is a method to the madness. \nThe API is organized around collections of different resources, each of which corresponds to \nblocks and can be indexed by block height or hash. \nResources \n• Leaves \n\n• Headers \n• Blocks \n• Block summaries \n• Payloads \n• VID common \nIndices \nEach of these resources can be addressed in the following ways: \n• <resource>/:height \n• <resource>/hash/:hash \n• <resource>/payload-hash/:payload-hash \nNot all of the indices are implemented for all resources, although in principle they can be. The \nsupported indices are documented below for each endpoint. Future releases will fill in the \nmissing functionality. \nLeaves are currently indexed slightly differently from other resources. See documentation on \nleaf endpoints. Future versions of this API will merge the concept of a leaf and a header, \nresolving this discrepancy. \nIn addition, there are endpoints to fetch a range of each resource (<resource>/:form/:until) and \nto subscribe to a WebSockets stream (/stream/<resource>/:from). \nTypes \nBlockSummary \nCopy \n{ \n    \"header\": Header, \n    \"hash\": tagged<BLOCK>, \n    \"size\": integer, \n    \"num_transactions\": integer \n} \nBlockResponse \nCopy \n{ \n    \"header\": Header, \n    \"payload\": Payload, \n    \"hash\": tagged<BLOCK>, \n\n    \"size\": integer, \n    \"num_transactions\": integer \n} \nLeafResponse \nCopy \n{ \n    \"leaf\": Leaf, \n    \"qc\": QC, \n} \nPayloadResponse \nCopy \n{ \n    \"data\": Payload, \n    \"height\": integer, \n    \"size\": integer, \n    \"block_hash\": tagged<BLOCK>, \n    \"hash\": tagged<HASH> \n} \nVidCommonResponse \nCopy \n{ \n    \"common\": VidCommon, \n    \"block_hash\": tagged<BLOCK>, \n    \"payload_hash\": tagged<HASH> \n} \nEndpoints \nGET /availability/leaf \nPaths \n• /availability/leaf/:height \n• /availability/leaf/hash/:hash \nParameters \n\nName \nType \nDescription \nheight \ninteger \nHeight of the leaf to fetch \nhash \ntagged<COMMIT> \nHash of the leaf to fetch \nReturns LeafResponse \nGET /availability/leaf/:from/:until \nRetrieve a range of consecutive leaves. \nParameters \nName \nType \nDescription \nfrom \ninteger \nHeight of the first leaf to fetch \nuntil \ninteger \nHeight just after the last leaf to fetch \nReturns [LeafResponse] \nGET /availability/stream/leaves/:height \nThis is a WebSockets endpoint. The client must be prepared to upgrade the connection to a \nWebSockets connection, including the proper headers. \nSubscribe to a stream of leaves, in order, starting from the given height. \nParameters \nName \nType \nDescription \n\nheight \ninteger \nHeight of the first leaf to yield \nYields LeafResponse \nGET /availability/header \nPaths \n• /availability/header/:height \n• /availability/header/hash/:hash \n• /availability/header/payload-hash/:payload-hash \nParameters \nName \nType \nDescription \nheight \ninteger \nHeight of the header to fetch \nhash \ntagged<BLOCK> \nHash of the header to fetch \npayload-hash \ntagged<HASH> \nHash of the payload of the header to fetch. Note that block payloads are not necessarily unique. \nIf there are multiple blocks whose payload matches this hash, it is unspecified which one is \nreturned. \nReturns Header \nGET /availability/header/:from/:until \nRetrieve a range of consecutive headers. \nParameters \nName \nType \nDescription \nfrom \n\ninteger \nHeight of the first header to fetch \nuntil \ninteger \nHeight just after the last header to fetch \nReturns [Header] \nGET /availability/stream/headers/:height \nThis is a WebSockets endpoint. The client must be prepared to upgrade the connection to a \nWebSockets connection, including the proper headers. \nSubscribe to a stream of headers, in order, starting from the given height. \nParameters \nName \nType \nDescription \nheight \ninteger \nHeight of the first header to yield \nYields Header \nGET /availability/block \nPaths \n• /availability/block/:height \n• /availability/block/hash/:hash \n• /availability/block/payload-hash/:payload-hash \nParameters \nName \nType \nDescription \nheight \ninteger \nHeight of the block to fetch \nhash \n\ntagged<BLOCK> \nHash of the block to fetch \npayload-hash \ntagged<HASH> \nHash of the payload of the block to fetch. Note that block payloads are not necessarily unique. \nIf there are multiple blocks whose payload matches this hash, it is unspecified which one is \nreturned. \nReturns BlockResponse \nGET /availability/block/:from/:until \nRetrieve a range of consecutive blocks. \nParameters \nName \nType \nDescription \nfrom \ninteger \nHeight of the first block to fetch \nuntil \ninteger \nHeight just after the last block to fetch \nReturns [BlockResponse] \nGET /availability/stream/blocks/:height \nThis is a WebSockets endpoint. The client must be prepared to upgrade the connection to a \nWebSockets connection, including the proper headers. \nSubscribe to a stream of blocks, in order, starting from the given height. \nParameters \nName \nType \nDescription \nheight \ninteger \nHeight of the first block to yield \n\nYields BlockResponse \nGET /availability/block/summary \nPaths \n• /availability/block/summary/:height \nParameters \nName \nType \nDescription \nheight \ninteger \nHeight of the block to fetch \nReturns BlockSummary \nGET /availability/block/summaries/:from/:until \nRetrieve a range of consecutive block summaries. \nParameters \nName \nType \nDescription \nfrom \ninteger \nHeight of the first block summary to fetch \nuntil \ninteger \nHeight just after the last block summary to fetch \nReturns [BlockSummary] \nGET /availability/payload \nPaths \n• /availability/payload/:height \n• /availability/payload/block-hash/:block-hash \n• /availability/payload/hash/:hash \nParameters \n\nName \nType \nDescription \nheight \ninteger \nHeight of the block whose payload should be fetched \nblock-hash \ntagged<BLOCK> \nHash of the block whose payload should be fetched \nhash \ntagged<HASH> \nHash of the payload to fetch. Note that block payloads are not necessarily unique. If there are \nmultiple payloads matching this hash, it is unspecified which one is returned. \nReturns PayloadResponse \nGET /availability/payload/:from/:until \nRetrieve a range of consecutive payloads. \nParameters \nName \nType \nDescription \nfrom \ninteger \nHeight of the first payload to fetch \nuntil \ninteger \nHeight just after the last payload to fetch \nReturns [PayloadResponse] \nGET /availability/stream/payloads/:height \nThis is a WebSockets endpoint. The client must be prepared to upgrade the connection to a \nWebSockets connection, including the proper headers. \nSubscribe to a stream of payloads, in order, starting from the given height. \nParameters \n\nName \nType \nDescription \nheight \ninteger \nHeight of the first payload to yield \nYields PayloadResponse \nGET /availability/vid/common \nPaths \n• /availability/vid/common/:height \n• /availability/vid/common/hash/:hash \n• /availability/vid/common/payload-hash/:payload-hash \nParameters \nName \nType \nDescription \nheight \ninteger \nHeight of the block whose VID common data should be fetched \nhash \ntagged<BLOCK> \nHash of the block whose VID common data should be fetched \npayload-hash \ntagged<HASH> \nHash of the payload of the block whose VID common data should be fetched. Note that block \npayloads are not necessarily unique. If there are multiple blocks whose payload matches this \nhash, it is unspecified which one is returned. \nReturns VidCommonResponse \nGET /availability/stream/vid/common/:height \nThis is a WebSockets endpoint. The client must be prepared to upgrade the connection to a \nWebSockets connection, including the proper headers. \nSubscribe to a stream of VID common objects, in order, starting from the given height. \n\nParameters \nName \nType \nDescription \nheight \ninteger \nHeight of the first VID common to yield \nYields VidCommonResponse \nGET /availability/block/:height/namespace/:namespace \nGet the list of transactions in a block from a given namespace, along with a proof that these are \nonly and all such transactions from that block. Note that the proof may be null if transactions is \nempty, in which case the caller should check the namespace table for the specified block to \nconfirm that :namespace is not present. \nParameters \nName \nType \nDescription \nheight \ninteger \nHeight of the block containing the desired namespace \nnamespace \ninteger \nID of the desired namespace \nReturns \nCopy \n{ \n    \"transactions\": [Transaction], \n    \"proof\": NsProof | null \n} \nGET /availability/transaction \nPaths \n• /availability/transaction/:height/:index \n\n• /availability/transaction/hash/:hash \nParameters \nName \nType \nDescription \nheight \ninteger \nHeight of the block containing the desired transaction \nindex \ninteger \n0-based position of the desired transaction in its block \nhash \ntagged<TX> \nHash of the desired transaction. Note that transactions are not necessarily unique. If there are \nmultiple transactions matching this hash, it is unspecified which one is returned. \nReturns \nCopy \n{ \n    \"transaction\": Transaction, \n    \"hash\": tagged<TX>, \n    \"index\": integer, \n    \"proof\": TransactionInclusionProof, \n    \"block_hash\": tagged<BLOCK>, \n    \"block_height\": integer \n} \nThe response contains the hash of the transaction, the hash and height of the block that \ncontains it, and its index within that block. It also contains a TransactionInclusionProof, which \nproves inclusion of this transaction in the block with block_hash. The specific format of this \ntype is not currently specified, but it can be deserialized and interpreted in Rust using the \nTxInclusionProof type. \nNode API \nComplements the availability API by serving eventually consistent data that is not necessarily \nagreed upon by all nodes \n\nThe availability API provides a pure view of snapshots of the Espresso blockchain at various \npoints in time. Because it strives for robustness and purity, it does not include aggregate \nstatistics like block or transaction counts, which may briefly return incorrect results and will \ngradually correct themselves as missing data is fetched. The node API does provide this data, \nmaking it a useful complement to the availability API. \nIn other words, while the availability API is a view of the blockchain abstractly, the node API \nprovides information about this node's view of the chain, at the present moment in time. \nEndpoints \nGET /node/block-height \nGet the height of the chain as known to this node. This is equal to one more than the height of \nthe latest known block. It is not a count of the blocks in this node's database, as blocks earlier \nthan the latest known block could be missing. \nReturns integer \nGET /node/transactions/count \nGet the number of finalized transactions. This count may be too low if blocks are missing from \nthe database. \nReturns integer \nGET /node/payloads/total-size \nGet the total size, in bytes, of all finalized block payloads. This count may be too low if blocks \nare missing from the database. \nReturns integer \nGET /node/vid/share \nGet the VID share belonging to this node for a given block. \nPaths \n• /node/vid/share/:height \n• /node/vid/share/hash/:hash \n• /node/vid/share/payload-hash/:payload-hash \nParameters \nName \nType \nDescription \nheight \ninteger \nHeight of the block whose VID share should be fetched \n\nhash \ntagged<BLOCK> \nHash of the block whose VID share should be fetched \npayload-hash \ntagged<HASH> \nHash of the payload of the block whose VID share should be fetched. Note that block payloads \nare not necessarily unique. If there are multiple blocks whose payload matches this hash, it is \nunspecified which one is returned. \nReturns VidShare \nThe specific format of this type is not currently specified, but it can be deserialized and \ninterpreted in Rust using the VidShare type. \nGET /node/sync-status \nGet the node's progress in syncing with the latest state of blockchain. \nIf the node is fully synced (that is, all the missing counts are 0 and pruned_height is null or 0) \nother endpoints in this API should give accurate results. \nReturns \nCopy \n{ \n    \"missing_blocks\": integer, \n    \"missing_leaves\": integer, \n    \"missing_vid_common\": integer, \n    \"missing_vid_shares\": integer, \n    \"pruned_height\": null | integer, \n} \nGET /node/header/window \nGet a range of consecutive headers by timestamp window. \nReturns all available headers, in order, whose timestamps fall between :start (inclusive) and \n:end (exclusive), or between the block indicated by :height or :hash (inclusive) and :end \n(exclusive). The response also includes one block before the desired window (unless the \nwindow includes the genesis block) and one block after the window. This proves to the client \nthat the server has not omitted any blocks whose timestamps fall within the desired window. \nIt is possible that not all blocks in the desired window are available when this endpoint is called. \nIn that case, whichever blocks are available are included in the response, and next is null to \nindicate that the response is not complete. The client can then use one of the /from/ forms of \n\nthis endpoint to fetch the remaining blocks from where the first response left off, once they \nbecome available. If no blocks are available, not even prev, this endpoint will return an error. \nPaths \n• /node/header/window/:start/:end \n• /node/header/window/from/:height/:end \n• /node/header/window/from/hash/:hash/:end \nParameters \nName \nType \nDescription \nstart \ninteger \nTimestamp in seconds where the window should start \nend \ninteger \nTimestamp in seconds where the window should end \nheight \ninteger \nBlock height where the window should start \nhash \ntagged<BLOCK> \nBlock hash where the window should start \nReturns \nCopy \n{ \n    \"window\": Header, \n    \"prev\": null | Header, \n    \"next\": null | Header \n} \nState API \nServes consensus state derived from finalized blocks \n\nAll state derived from block data is represented in the form of Merkle trees or Merkle tries, so \nthis API is able to provide select segments of the state with a proof that will convince the client \nthat the returned segment is accurate, as long as they know the corresponding state \ncommitment (part of each block header). \nEndpoints \nGET /fee-state \nGet a Merkle proof proving the balance of a certain fee account in a given snapshot of the state. \nThe element in the returned Merkle proof contains the balance of the requested account, or null \nif the account has no balance. \nPaths \n• /fee-state/:height/:account \n• /fee-state/commit/:commit/:account \nParameters \nName \nType \nDescription \nheight \ninteger \nBlock height of the state snapshot to read from \ncommit \ntagged<MERKLE_COMM> \nCommitment of the state snapshot to read from \naccount \nhex \nFee account to look up \nReturns MerkleProof \nGET /fee-state/block-height \nThe latest block height for which fee state is available. \nNote that this may be less than the block height indicated by other APIs, such as status or node, \nsince the fee state storage is updated asynchronously. \nReturns integer \nGET /block-state \n\nGet a Merkle proof proving the inclusion of a certain block at a position in the history. The \nelement in the returned Merkle proof contains the commitment of the block at the requested \nposition in history. \nPaths \n• /block-state/:height/:index \n• /block-state/commit/:commit/:index \nParameters \nName \nType \nDescription \nheight \ninteger \nBlock height of the state snapshot to read from \ncommit \ntagged<MERKLE_COMM> \nCommitment of the state snapshot to read from \nindex \ninteger \nHeight of the block to look up \nReturns MerkleProof \nGET /block-state/block-height \nThe latest block height for which block state is available. \nNote that this may be less than the block height indicated by other APIs, such as status or node, \nsince the block state storage is updated asynchronously. \nReturns integer \nState API \nServes consensus state derived from finalized blocks \nAll state derived from block data is represented in the form of Merkle trees or Merkle tries, so \nthis API is able to provide select segments of the state with a proof that will convince the client \nthat the returned segment is accurate, as long as they know the corresponding state \ncommitment (part of each block header). \nEndpoints \nGET /fee-state \n\nGet a Merkle proof proving the balance of a certain fee account in a given snapshot of the state. \nThe element in the returned Merkle proof contains the balance of the requested account, or null \nif the account has no balance. \nPaths \n• /fee-state/:height/:account \n• /fee-state/commit/:commit/:account \nParameters \nName \nType \nDescription \nheight \ninteger \nBlock height of the state snapshot to read from \ncommit \ntagged<MERKLE_COMM> \nCommitment of the state snapshot to read from \naccount \nhex \nFee account to look up \nReturns MerkleProof \nGET /fee-state/block-height \nThe latest block height for which fee state is available. \nNote that this may be less than the block height indicated by other APIs, such as status or node, \nsince the fee state storage is updated asynchronously. \nReturns integer \nGET /block-state \nGet a Merkle proof proving the inclusion of a certain block at a position in the history. The \nelement in the returned Merkle proof contains the commitment of the block at the requested \nposition in history. \nPaths \n• /block-state/:height/:index \n• /block-state/commit/:commit/:index \nParameters \n\nName \nType \nDescription \nheight \ninteger \nBlock height of the state snapshot to read from \ncommit \ntagged<MERKLE_COMM> \nCommitment of the state snapshot to read from \nindex \ninteger \nHeight of the block to look up \nReturns MerkleProof \nGET /block-state/block-height \nThe latest block height for which block state is available. \nNote that this may be less than the block height indicated by other APIs, such as status or node, \nsince the block state storage is updated asynchronously. \nReturns integer \nSubmit API \nSubmit transactions to the public mempool \nEndpoints \nPOST /submit/submit \nReturns the hash of the transaction if it was successfully submitted. This does not mean the \ntransaction has yet been sequenced. The user can check for inclusion of the transaction using \n/availability/transaction/hash/:hash from the availability API. \nThis endpoint will fail with a 400 status code if the submitted transaction has a namespace ID \nlarger than 4294967295 (2^32 - 1). \nRequest Body Transaction \nReturns tagged<TX> \n ",
    "filename": "16. Espresso API.pdf"
  },
  {
    "id": 9,
    "content": "\n\nBuilder API \nThe following describes the API endpoints a builder needs to support in order to build blocks in \nEspresso. \nThese API endpoints are actively maintained and might change going forward. \nGET block_info/availableblocks/:parent_hash/:view_number/:sender/:signature: \nThis endpoint allows a sequencer node to query a builder for a list of available blocks that the \nbuilder can provide, ensuring consistency with the ongoing consensus defined by the specified \nparent_hash. \nParameters \nName \nType \nDescription \nparent_hash \nTaggedBase64 \nThe hash of the parent block in the current consensus chain. This parameter ensures that the \nreturned blocks are valid continuations of the existing chain. \nview_number \nInteger \nView number in Hotshot. \nsender \nTaggedBase64 \nThe address or identifier of the entity making the request (i.e., the sequencer node). \nsignature \nTaggedBase64 \nA cryptographic signature generated by the sequencer node, authenticating the request and \nproving its authenticity \nReturns \nCopy \n[ \n    {  \n        \"block_hash\": TaggedBase64, \n        \"block_size\": integer, \n        \"offered_fee\": integer, \n\n        \"signature\": TaggedBase64, \n        \"sender\": TaggedBase64 \n    } \n] \nGET block_info/claimblock/:block_hash/:view_number/:sender/:signature: \nUpon receiving the metadata about available blocks through the previously mentioned \nendpoint, the sequencer node will utilize this dedicated endpoint to retrieve the complete and \ncomprehensive content of chosen block. This endpoint will provide the sequencer node with \nthe entirety of the block data, including all transactions, consensus data, and any other \npertinent information encapsulated within the block. By leveraging this endpoint, the sequencer \nnode can access and process the complete block content, enabling it to perform its designated \nfunctions effectively inside the Hotshot consensus. \nParameters \nName \nType \nDescription \nblock_hash \nTaggedBase64 \nThe hash of the block provided as a response in previous API. \nview_number \nInteger \nView number in Hotshot. \nsender \nTaggedBase64 \nThe address or identifier of the entity making the request (i.e., the sequencer node). \nsignature \nTaggedBase64 \nA cryptographic signature generated by the sequencer node, authenticating the request and \nproving its authenticity \nReturns \nCopy \n{  \n    \"block_payload\": Payload, \n\n    \"metadata\": MetaData, \n    \"signature\": TaggedBase64, \n    \"sender\": TaggedBase64 \n} \nGET block_info/claimheaderinput/:block_hash/:view_number/:sender/:signature \nThis endpoint enables a sequencer node to retrieve the relevant fee information associated with \na specific block, facilitating a transparent payout mechanism for the builder responsible for \nconstructing that block. \nNote: The block_hash should match the one requested in previous API i.e claim_block for \nconsistent fee payoff. \nParameters \nName \nType \nDescription \nblock_hash \nTaggedBase64 \nThe hash of the block provided as a response in first API. \nview_number \nInteger \nView number in Hotshot. \nsender \nTaggedBase64 \nThe address or identifier of the entity making the request (i.e., the sequencer node). \nsignature \nTaggedBase64 \nA cryptographic signature generated by the sequencer node, authenticating the request and \nproving its authenticity \nReturns \nCopy \n{  \n    \"vid_commitment\": VidCommitment,  \n    \"vid_precompute_data\": VidPrecomputeData, \n\n    \"fee_signature\": TaggedBase64, \n    \"signature\": \"TaggedBase64, \n    \"sender\" = TaggedBase64  \n} \nPOST /txn_submit/submit \nIt enables external user to submit directly to builder's private transactions mempool. \n• Returns the hash of the transaction if it was successfully submitted. This does not mean \nthe transaction has yet been sequenced. The user can check for inclusion of the \ntransaction using /availability/transaction/hash/:hash from the availability API. \no Request Body Transaction \no Returns tagged<TX> \nPOST /txn_submit/batch \nIt enables external user to submit a list of transactions to builder's private transactions \nmempool. \n• Returns the corresponding list of transaction hashes if it were successfully submitted. \nThis does not mean the transaction has yet been sequenced. The user can check for \ninclusion of the transaction using /availability/transaction/hash/:hash from the \navailability API. \no Request Body Vec<Transaction> \no Returns Vec<tagged<TX>> \nGET /txn_submit/status/:transaction_hash \nIt enables external user to track the status of submitted transactions in short period, old \ntransaction will be pruned if the map doesn't have enough space. \n• Returns the status of the queried transaction commitment if successfully retrieved. The \nstatus will be one of the following: \"Pending\", \"Unknown\" or \"Rejected\" (with a reason \nmessage). The status \"Sequenced\" is not yet supported. While this information can be \nobtained from the query service or block explorer, transactions with a \"Sequenced\" \nstatus will appear as \"Pending\" when retrieved. An error will be returned if the request \nfails due to issues like malformed input, incorrect deserialization, or missing data. \no Request Body tagged<TX> \nReturns \nCopy \n  { \"Pending\" } \nor \nCopy \n\n  { \"Unknown\" } \nor \nCopy \n  { \"Rejected\": { \"reason\": TaggedBase64 } } \nor \nCopy \n  {  \"error\": TaggedBase64 } \n ",
    "filename": "17. builder api.pdf"
  },
  {
    "id": 10,
    "content": "\n\nMainnet 0 \nEspresso Mainnet 0 release — October 2024 \nMainnet 0 marks the production release of the Espresso Network. This release is an important \nstep towards enabling Espresso's vision of an ecosystem of open, composable and \npermissionless applications. \nEspresso’s Global Confirmation Layer will support applications such as rollups with faster \nbridging, and lays the groundwork for chains to improve coordination amongst each other, \nenhancing cross-chain interoperability. \nUnderpinning the Espresso Network is HotShot, which will be run in a production setting for the \nfirst time. During the initial stages of Mainnet 0, HotShot will be run by a set of 20 node \noperators, running 100 nodes in total. An important upcoming milestone will be to open up \nparticipation to more node operators and increase economic security through proof-of-stake.  \nDuring the initial release, block size has conservatively been set to 1 MB, with plans to scale up \nto 5 MB in the short term if there is sufficient demand. We expect it to take around ~8 seconds \nfor a transaction to be confirmed in the initial release. In the short term, an update to HotShot \nwill implement ideas from HotStuff-2 to reduce this time to ~5-6 seconds. In the medium term, \na VID update is in the works that will enable confirmation latency of ~3 seconds, even as we \nscale to thousands of consensus nodes. This will also enable us to increase the blocksize \nfurther without greatly impacting latency.  \nYou can track activity on Mainnet in our block explorer, and you can interact with it via the public \nAPI endpoint. \nRunning a Mainnet 0 Node \nThis page provides the specific configuration used to run different types of nodes in Mainnet 0. \nNote: during Mainnet 0 only a fixed set of preregistered operators can run a node. The Espresso \nNetwork will upgrade to proof-of-stake in a later release. \n   TL;DR: For operator running Decaf nodes, the critical changes from that configuration are as \nfollows: \n• All URLs supplied by Espresso have changed \n• The new container image version is 20241120-patch5 \n• The JSON-RPC endpoint specified by ESPRESSO_SEQUENCER_L1_PROVIDER should \nbe an Ethereum mainnet endpoint instead of Sepolia \nThe container image to use for this deployment is  \n• ghcr.io/espressosystems/espresso-sequencer/sequencer:20241120-patch5 (if using \nEspresso's images) \n• built off of the branch `20241120-patch5` (if building from source) \nThe configuration for all node types includes \nESPRESSO_SEQUENCER_GENESIS_FILE=/genesis/mainnet.toml. This file is built into the \n\nofficial Docker images. Operators building their own images will need to ensure this file is \nincluded and their nodes are pointed at it. \n1. Regular Node \nCommand \nCopy \nsequencer -- http -- catchup -- status \nEnvironment \nSame for all nodes \nCopy \nESPRESSO_SEQUENCER_ORCHESTRATOR_URL=https://orchestrator-\nkdrhoi6lwz.main.net.espresso.network/ \nESPRESSO_SEQUENCER_CDN_ENDPOINT=cdn.main.net.espresso.network:1737 \nESPRESSO_STATE_RELAY_SERVER_URL=https://state-relay.main.net.espresso.network \nESPRESSO_SEQUENCER_GENESIS_FILE=/genesis/mainnet.toml \nRUST_LOG=\"warn,libp2p=off\" \nRUST_LOG_FORMAT=\"json\" \n# At least one state peer is required. The following URL provided by Espresso works. \n# Optionally, add endpoints for additional peers, separated by commas. \nESPRESSO_SEQUENCER_STATE_PEERS=https://query.main.net.espresso.network \nChosen by operators \nCopy \n# JSON-RPC endpoint for Ethereum Mainnet \nESPRESSO_SEQUENCER_L1_PROVIDER # e.g. https://mainnet.infura.io/v3/<API-KEY> \n# Port on which to host metrics and healthchecks \nESPRESSO_SEQUENCER_API_PORT # e.g. 80 \n# Path in container to store consensus state \nESPRESSO_SEQUENCER_STORAGE_PATH # e.g. /mount/sequencer/store/ \n# Path in container to keystore \nESPRESSO_SEQUENCER_KEY_FILE # e.g. /mount/sequencer/keys/0.env \n# The address to bind Libp2p to in host:port form. Other nodes should be able to \n# access this; i.e. port must be open for UDP. \nESPRESSO_SEQUENCER_LIBP2P_BIND_ADDRESS \n\n# The address we should advertise to other nodes as being our Libp2p endpoint \n# (in host:port form). It should resolve a connection to the above bind address; i.e. \n# should use public IP address or hostname, and forward to the port given in the bind \n# address. \nESPRESSO_SEQUENCER_LIBP2P_ADVERTISE_ADDRESS \nVolumes \n• $ESPRESSO_SEQUENCER_STORAGE_PATH \n• $ESPRESSO_SEQUENCER_KEY_FILE \n2. DA Node \nRequires operator to additionally run a Postgres server \nCommand \nsequencer -- storage-sql -- http -- catchup -- status -- query \nEnvironment \nSame for all nodes \nCopy \nESPRESSO_SEQUENCER_ORCHESTRATOR_URL=https://orchestrator-\nkdrhoi6lwz.main.net.espresso.network/ \nESPRESSO_SEQUENCER_CDN_ENDPOINT=cdn.main.net.espresso.network:1737 \nESPRESSO_STATE_RELAY_SERVER_URL=https://state-relay.main.net.espresso.network \nESPRESSO_SEQUENCER_GENESIS_FILE=/genesis/mainnet.toml \nESPRESSO_SEQUENCER_POSTGRES_PRUNE=\"true\" \nESPRESSO_SEQUENCER_IS_DA=\"true\" \nRUST_LOG=\"warn,libp2p=off\" \nRUST_LOG_FORMAT=\"json\" \n# At least one state peer is required. The following URL provided by Espresso works. \n# Optionally, add endpoints for additional peers, separated by commas. \nESPRESSO_SEQUENCER_STATE_PEERS=https://query.main.net.espresso.network \nESPRESSO_SEQUENCER_API_PEERS=https://query.main.net.espresso.network \nChosen by operators \nCopy \n# JSON-RPC endpoint for Ethereum Mainnet \n\nESPRESSO_SEQUENCER_L1_PROVIDER # e.g. https://mainnet.infura.io/v3/<API-KEY> \n# Port on which to host metrics, healthchecks, and DA API \nESPRESSO_SEQUENCER_API_PORT # e.g. 80 \n# Path in container to keystore \nESPRESSO_SEQUENCER_KEY_FILE # e.g. /mount/sequencer/keys/0.env \n# Connection to Postgres \nESPRESSO_SEQUENCER_POSTGRES_HOST \nESPRESSO_SEQUENCER_POSTGRES_USER \nESPRESSO_SEQUENCER_POSTGRES_PASSWORD \n# The address to bind Libp2p to in host:port form. Other nodes should be able to \n# access this; i.e. port must be open for UDP. \nESPRESSO_SEQUENCER_LIBP2P_BIND_ADDRESS \n# The address we should advertise to other nodes as being our Libp2p endpoint \n# (in host:port form). It should resolve a connection to the above bind address; i.e. \n# should use public IP address or hostname, and forward to the port given in the bind \n# address. \nESPRESSO_SEQUENCER_LIBP2P_ADVERTISE_ADDRESS \nVolumes \n• $ESPRESSO_SEQUENCER_KEY_FILE \nRequires operator to additionally run a Postgres server \nCommand \nsequencer -- storage-sql -- http -- catchup -- status -- query -- state \nEnvironment \nSame for all nodes \nCopy \nESPRESSO_SEQUENCER_ORCHESTRATOR_URL=https://orchestrator-\nkdrhoi6lwz.main.net.espresso.network/ \nESPRESSO_SEQUENCER_CDN_ENDPOINT=cdn.main.net.espresso.network:1737 \nESPRESSO_STATE_RELAY_SERVER_URL=https://state-relay.main.net.espresso.network \nESPRESSO_SEQUENCER_GENESIS_FILE=/genesis/mainnet.toml \nESPRESSO_SEQUENCER_IS_DA=true \n\nESPRESSO_SEQUENCER_ARCHIVE=true \nRUST_LOG=\"warn,libp2p=off\" \nRUST_LOG_FORMAT=\"json\" \n# At least one state peer is required. The following URL provided by Espresso works. \n# Optionally, add endpoints for additional peers, separated by commas. \nESPRESSO_SEQUENCER_STATE_PEERS=https://query.main.net.espresso.network \nESPRESSO_SEQUENCER_API_PEERS=https://query.main.net.espresso.network \nChosen by operators \nCopy \n# JSON-RPC endpoint for Ethereum Mainnet \nESPRESSO_SEQUENCER_L1_PROVIDER # e.g. https://mainnet.infura.io/v3/<API-KEY> \n# Port on which to host metrics, healthchecks, and query API \nESPRESSO_SEQUENCER_API_PORT # e.g. 80 \n# Path in container to keystore \nESPRESSO_SEQUENCER_KEY_FILE # e.g. /mount/sequencer/keys/0.env \n# Connection to Postgres \nESPRESSO_SEQUENCER_POSTGRES_HOST \nESPRESSO_SEQUENCER_POSTGRES_USER \nESPRESSO_SEQUENCER_POSTGRES_PASSWORD \n# The address to bind Libp2p to in host:port form. Other nodes should be able to \n# access this; i.e. port must be open for UDP. \nESPRESSO_SEQUENCER_LIBP2P_BIND_ADDRESS \n# The address we should advertise to other nodes as being our Libp2p endpoint \n# (in host:port form). It should resolve a connection to the above bind address; i.e. \n# should use public IP address or hostname, and forward to the port given in the bind \n# address. \nESPRESSO_SEQUENCER_LIBP2P_ADVERTISE_ADDRESS \nContracts \nEspresso Mainnet maintains light client contracts that rollups and other applications integrating \nwith Espresso can use to read the state of the Espresso network in a trust-minimized way. \n• Light client on Ethereum (for L2s integrating Espresso): \n0x95ca91cea73239b15e5d2e5a74d02d6b5e0ae458 \n\n• Light client on Arbitrum (for Orbit L3s integrating Espresso): \n0x47495bb99cccbb1bda9f15b32b69093137f886db \n ",
    "filename": "18. mainnet 0.pdf"
  },
  {
    "id": 11,
    "content": "\n\nTestnets \nOverview of current and past testnets \nEspresso Systems has released and deployed several testnets since 2022:  \n• Decaf, our ongoing persistent testnet \n• Cappuccino (May 2024) \n• Gibraltar (January 2024) \n• Cortado (September 2023) \n• Doppio (July 2023) \n• Americano (November 2022) \nDecaf Testnet Release \nEspresso Persistent Testnet (Decaf) — September 2024 \nIn September of 2024, we launched Decaf, which will run as a persistent \ntestnet of the Espresso system alongside the upcoming Mainnet release. \nDecaf continues the process of decentralizing Espresso, by onboarding an \nadditional 13 node operators, bringing the total operator set to 24. These \n24 operators will run 100 geographically distributed nodes, all participating \nin HotShot consensus together. You can track activity on the Decaf testnet \nin our block explorer, and you can interact with Decaf via the public API \nendpoint. \nRunning a Node \nConfiguration for Decaf nodes \nDecaf node operators are limited to a select group. If you are interested in running a node in a \nfuture release of Espresso, contact us. \nThis page give the configuration used to run different types of nodes in the Decaf testnet. For \ngeneral information on running an Espresso node, see Running an Espresso Node. \nAll nodes in Decaf use the ghcr.io/espressosystems/espresso-sequencer/sequencer:20250228 \nDocker image, or an equivalent image built from source. Depending on the type of node, the \nconfiguration varies. \nThe configuration for all node types includes \nESPRESSO_SEQUENCER_GENESIS_FILE=/genesis/decaf.toml. This file is built into the official \nDocker images. Operators building their own images will need to ensure this file is included and \ntheir nodes are pointed at it. \nRegular Node \n\nCommand \nsequencer -- http -- catchup -- status \nEnvironment \nSame for all nodes \nCopy \nESPRESSO_SEQUENCER_ORCHESTRATOR_URL=https://orchestrator-\nUZAFTUIMZOT.decaf.testnet.espresso.network/ \nESPRESSO_SEQUENCER_CDN_ENDPOINT=cdn.decaf.testnet.espresso.network:1737 \nESPRESSO_STATE_RELAY_SERVER_URL=https://state-relay.decaf.testnet.espresso.network \nESPRESSO_SEQUENCER_GENESIS_FILE=/genesis/decaf.toml \nRUST_LOG=\"warn,hotshot_libp2p_networking=off\" \nRUST_LOG_FORMAT=\"json\" \n \n# At least one state peer is required. The following URL provided by Espresso works. \n# Optionally, add endpoints for additional peers, separated by commas. \nESPRESSO_SEQUENCER_STATE_PEERS=https://query.decaf.testnet.espresso.network \nChosen by operators \nCopy \n# An HTTP JSON-RPC endpoint for Sepolia testnet. This is required \nESPRESSO_SEQUENCER_L1_PROVIDER # e.g. https://sepolia.infura.io/v3/<API-KEY> \n \n# A `ws://` or `wss://` endpoint for Sepolia testnet. This is optional but \n# recommended since it decreases the load on your provider. \nESPRESSO_SEQUENCER_L1_WS_PROVIDER # e.g. wss://sepolia.infura.io/v3/<API-KEY> \n \n# Port on which to host metrics and healthchecks \nESPRESSO_SEQUENCER_API_PORT # e.g. 80 \n \n# Path in container to store consensus state \nESPRESSO_SEQUENCER_STORAGE_PATH # e.g. /mount/sequencer/store/ \n \n\n# Path in container to keystore \nESPRESSO_SEQUENCER_KEY_FILE # e.g. /mount/sequencer/keys/0.env \n \n# The address to bind Libp2p to in host:port form. Other nodes should be able to \n# access this; i.e. port must be open for UDP. \nESPRESSO_SEQUENCER_LIBP2P_BIND_ADDRESS \n \n# The address we should advertise to other nodes as being our Libp2p endpoint \n# (in host:port form). It should resolve a connection to the above bind address; i.e. \n# should use public IP address or hostname, and forward to the port given in the bind \n# address. \nESPRESSO_SEQUENCER_LIBP2P_ADVERTISE_ADDRESS \nVolumes \n• $ESPRESSO_SEQUENCER_STORAGE_PATH \n• $ESPRESSO_SEQUENCER_KEY_FILE \nDA Node \nRequires operator to additionally run a Postgres server \nCommand \nsequencer -- storage-sql -- http -- catchup -- status -- query \nEnvironment \nSame for all nodes \nCopy \nESPRESSO_SEQUENCER_ORCHESTRATOR_URL=https://orchestrator-\nUZAFTUIMZOT.decaf.testnet.espresso.network/ \nESPRESSO_SEQUENCER_CDN_ENDPOINT=cdn.decaf.testnet.espresso.network:1737 \nESPRESSO_STATE_RELAY_SERVER_URL=https://state-relay.decaf.testnet.espresso.network \nESPRESSO_SEQUENCER_GENESIS_FILE=/genesis/decaf.toml \nESPRESSO_SEQUENCER_POSTGRES_PRUNE=\"true\" \nESPRESSO_SEQUENCER_IS_DA=\"true\" \n \nRUST_LOG=\"warn,hotshot_libp2p_networking=off\" \n\nRUST_LOG_FORMAT=\"json\" \n \n# At least one state peer is required. The following URL provided by Espresso works. \n# Optionally, add endpoints for additional peers, separated by commas. \nESPRESSO_SEQUENCER_STATE_PEERS=https://query.decaf.testnet.espresso.network \nESPRESSO_SEQUENCER_API_PEERS=https://query.decaf.testnet.espresso.network \nChosen by operators \nCopy \n# An HTTP JSON-RPC endpoint for Sepolia testnet. This is required \nESPRESSO_SEQUENCER_L1_PROVIDER # e.g. https://sepolia.infura.io/v3/<API-KEY> \n \n# A `ws://` or `wss://` endpoint for Sepolia testnet. This is optional but \n# recommended since it decreases the load on your provider. \nESPRESSO_SEQUENCER_L1_WS_PROVIDER # e.g. wss://sepolia.infura.io/v3/<API-KEY> \n \n# Port on which to host metrics, healthchecks, and DA API \nESPRESSO_SEQUENCER_API_PORT # e.g. 80 \n \n# Path in container to keystore \nESPRESSO_SEQUENCER_KEY_FILE # e.g. /mount/sequencer/keys/0.env \n \n# Connection to Postgres \nESPRESSO_SEQUENCER_POSTGRES_HOST \nESPRESSO_SEQUENCER_POSTGRES_USER \nESPRESSO_SEQUENCER_POSTGRES_PASSWORD \n \n# The address to bind Libp2p to in host:port form. Other nodes should be able to \n# access this; i.e. port must be open for UDP. \nESPRESSO_SEQUENCER_LIBP2P_BIND_ADDRESS \n \n# The address we should advertise to other nodes as being our Libp2p endpoint \n\n# (in host:port form). It should resolve a connection to the above bind address; i.e. \n# should use public IP address or hostname, and forward to the port given in the bind \n# address. \nESPRESSO_SEQUENCER_LIBP2P_ADVERTISE_ADDRESS \nVolumes \n• $ESPRESSO_SEQUENCER_KEY_FILE \nArchival Node \nRequires operator to additionally run a Postgres server \nCommand \nsequencer -- storage-sql -- http -- catchup -- status -- query \nEnvironment \nSame for all nodes \nCopy \nESPRESSO_SEQUENCER_ORCHESTRATOR_URL=https://orchestrator-\nUZAFTUIMZOT.decaf.testnet.espresso.network/ \nESPRESSO_SEQUENCER_CDN_ENDPOINT=cdn.decaf.testnet.espresso.network:1737 \nESPRESSO_STATE_RELAY_SERVER_URL=https://state-relay.decaf.testnet.espresso.network \nESPRESSO_SEQUENCER_GENESIS_FILE=/genesis/decaf.toml \nESPRESSO_SEQUENCER_IS_DA=true \nESPRESSO_SEQUENCER_ARCHIVE=true \n \nRUST_LOG=\"warn,hotshot_libp2p_networking=off\" \nRUST_LOG_FORMAT=\"json\" \n \n# At least one state peer is required. The following URL provided by Espresso works. \n# Optionally, add endpoints for additional peers, separated by commas. \nESPRESSO_SEQUENCER_STATE_PEERS=https://query.decaf.testnet.espresso.network \nESPRESSO_SEQUENCER_API_PEERS=https://query.decaf.testnet.espresso.network \nChosen by operators \nCopy \n# An HTTP JSON-RPC endpoint for Sepolia testnet. This is required \n\nESPRESSO_SEQUENCER_L1_PROVIDER # e.g. https://sepolia.infura.io/v3/<API-KEY> \n \n# A `ws://` or `wss://` endpoint for Sepolia testnet. This is optional but \n# recommended since it decreases the load on your provider. \nESPRESSO_SEQUENCER_L1_WS_PROVIDER # e.g. wss://sepolia.infura.io/v3/<API-KEY> \n \n# Port on which to host metrics, healthchecks, and query API \nESPRESSO_SEQUENCER_API_PORT # e.g. 80 \n \n# Path in container to keystore \nESPRESSO_SEQUENCER_KEY_FILE # e.g. /mount/sequencer/keys/0.env \n \n# Connection to Postgres \nESPRESSO_SEQUENCER_POSTGRES_HOST \nESPRESSO_SEQUENCER_POSTGRES_USER \nESPRESSO_SEQUENCER_POSTGRES_PASSWORD \n \n# The address to bind Libp2p to in host:port form. Other nodes should be able to \n# access this; i.e. port must be open for UDP. \nESPRESSO_SEQUENCER_LIBP2P_BIND_ADDRESS \n \n# The address we should advertise to other nodes as being our Libp2p endpoint \n# (in host:port form). It should resolve a connection to the above bind address; i.e. \n# should use public IP address or hostname, and forward to the port given in the bind \n# address. \nESPRESSO_SEQUENCER_LIBP2P_ADVERTISE_ADDRESS \nVolumes \n• $ESPRESSO_SEQUENCER_KEY_FILE \nContracts \nDecaf testnet maintains light client contracts that rollups and other applications integrating \nwith the testnet can use to read the state of the Decaf network in a trust-minimized way. \n\n• Light client on Sepolia testnet (for L2 testnets integrating Espresso): \n0x303872bb82a191771321d4828888920100d0b3e4 \n• Light client on Arbitrum Sepolia (for Orbit L3 testnets integrating Espresso): \n0x08d16cb8243b3e172dddcdf1a1a5dacca1cd7098 \n ",
    "filename": "19. decaf tesnet.pdf"
  },
  {
    "id": 12,
    "content": "\n\nThe Espresso Network \nThe global confirmation layer \nEspresso's Global Confirmation Network is a shared source of truth that provides secure \nconfirmations for transaction ordering and data across chains. It leverages a BFT consensus \nprotocol called HotShot. Rollup transactions are processed by HotShot consensus and \nconfirmed within a few seconds. By sharing the same Global Confirmation Network, chains \nhave certainty within seconds about what state will eventually be finalized on the L1. This is \nachieved by preventing sequencers from changing a transaction sequence after it has been \nconfirmed by HotShot, which can happen within only a few seconds (long before settlement on \nEthereum). \nCompared to relying on a disparate set of centralized sequencers for pre-confirmations, sharing \na decentralized consensus protocol as a Global Confirmation Network bolsters the L2 \necosystem as a whole against equivocation of pre-confirmations that may occur as a result of \nhacks or bad actors. Applications that interact between multiple chains, such as bridges, are \nespecially sensitive to the quality of confirmations of individual chains they are involved with.  \nThis is reflected by the fact that for many of the largest bridge providers today, the waiting time \nfor moving funds away from a rollup is around 15 minutes; the time it takes for the bridging \ntransaction to be finalized inside a blob or as calldata on Ethereum. With HotShot, bridging \nbetween rollups can be done in a manner of seconds, as opposed to waiting 15 minutes for \ntransactions to finalize on the Ethereum L1, with similar security guarantees.  \nThe confirmation guarantees of HotShot mirror that of Ethereum L1. Once HotShot confirms a \ntransaction reached, an adversary would need to control at least 1/3rd of the overall stake to \nrevert said transaction. Combined with restaking, this means that HotShot's safety guarantee \ncan approach that of Ethereum L1 over time. In the event that safety is broken, the adversary's \nstake will be slashed, making it very expensive to undo confirmations in HotShot.  \nSystem Overview \nChains (e.g. L2 rollups) interact with Espresso and with the layer 1 blockchain to facilitate \ntrustless state checkpoints. Specifically, when we refer to L1 and L2s we mean the following:  \n• The L1 (layer 1) blockchain: This is the blockchain that chains using Espresso post \nstate checkpoints to. HotShot must also checkpoint its state and history on the layer-1, \nwhich will serve as an interface to the rollups. Initially Espresso will run a smart contract \non Ethereum that tracks HotShot's state commitments. Other projects can recycle \nthese state commitments to reuse on other chains if they wish.  \n• The L2 (layer 2) chains: These are the chains that use Espresso for selling sequencing \nrights and confirmations. They could be anything from app-specific chains to fully-\nfeatured smart contract systems in their own right (like EVM rollups). Each chain is \nassumed here to be structured as a rollup: after receiving an ordered sequence of \ntransactions, the transactions are executed off-chain in some deterministic VM, and \nperiodically state updates are posted to the layer-1, along with a proof of validity (for ZK-\nrollups) or a potential fraud proof (for optimistic rollups). \nThis diagram shows the flow of information through the entire system, starting with transaction \nsubmission through clients to various integrated rollups and finally to being certified and \ncheckpointed on the layer-1.  \n\nThe next sections dive into these components and their interactions in more detail. \nRollup Components \nThe internal architecture of each rollup will vary greatly depending on the type of rollup (e.g., \nZK/optimistic; EVM/app-specific) but all rollups will have a few similar basic components. The \ninternal structure of Rollup 1 is shown in detail in the diagram (for simplicity, the other rollups \nare abstracted). \nA rollup must provide an interface for clients to interact with it. This can be any kind of API, \nalthough Ethereum-compatible JSON-RPC will be common. The API responds to queries about \nthe rollup state by reading from a state database, which is populated by the executor, a \ncomponent which executes every block provided by the sequencer. Finally, a prover (which may \nbe part of a decentralized prover network) is triggered by updates to the state and is responsible \nfor justifying those state updates. For ZK-rollups, the prover will be triggered by every block and \nproduce a validity proof for the state update. For optimistic rollups, the prover will only be \ntriggered if another node publishes an invalid state update, in which case the prover will \ngenerate a fraud proof. \nIn addition to answering state queries, the rollup API may also serve as an endpoint for clients to \nsubmit transactions. While clients can submit transactions directly to the Espresso Network's \nmempool, doing so may be inconvenient for a few reasons: \n• The Espresso Network's transaction submission interface is not specific to any one \nrollup. Clients will have to wrap their rollup-specific transactions into a more generic \nkind of transaction before submitting. \n• It requires clients to interact with two different services: the rollup API for state queries \nand the Espresso Network (HotShot) for transaction submissions. Depending on the \nclient software, this may not even be possible. MetaMask, for example, requires a single \nURL for each chain that it can use for queries and transaction submission. \nIt is therefore recommended that rollup servers provide a transaction submission interface as \npart of their API, for those clients who are already using the rest of the API. Such an interface is \nactually required for conforming JSON-RPC implementations, since the \neth_sendRawTransaction RPC method allows clients of the RPC to submit a transaction. \nWhatever its interface, the implementation of the rollup’s submission API can be as simple as \nwrapping rollup transactions into generic transactions and forwarding them to the HotShot \nmempool. \nTransaction Flow \nOnce a transaction is forwarded or submitted, it will be included in a block, and confirmed and \nmade available by HotShot, after which blocks propagate back through the rollups’ executors \nand provers, which in turn forward their blocks to the L1. Espresso also sends a block \ncommitment to the layer 1 sequencer contract, along with a quorum certificate that the \ncontract uses to authenticate the block. This allows layer 1 rollup contracts to compare the \nrollup state update proof against a block commitment which is certified as having been finalized \nby HotShot consensus. \nThe diagram below shows in more detail the flow of a single transaction from a client through \nthe system. \n\nTransaction Lifecycle \n1. User sends a transaction to a chain's server (e.g. an RPC service). \n2. The chain forwards the transaction, along with an identifier for that rollup, to the chain's \nsequencer. \n3. The chain's sequencer includes the transaction in a block, which is broadcast to \nsubscribers. One of these subscribers, the rollup node, executes the transaction (along \nwith any other transactions in the block belonging to that rollup). In the case of a ZK \nrollup, the node may produce a proof of correct execution, which can be broadcast to \nclients to quickly convince them of the new state. \n4. A commitment to the block containing the transaction is persisted in the L1 sequencer \ncontract (along with a proof that the block has been finalized by consensus). \n5. A rollup node which has executed the block sends the new rollup state to the L1. It may \ninclude a validity proof (for ZK rollups) or open a window for fraud proofs (optimistic \nrollups). \n6. The L1 rollup contract verifies any proofs related to the state update, using the certified \nblock commitment from the sequencer contract to check that the state update \ncorresponds to the correct block. \nConfirmations \nCentralized sequencers may give rollup users a pre-confirmation that their transaction will \neventually be included in the finalized rollup state. These pre-confirmations can be trusted via a \ncombination of reputation, security bonds, and/or fraud proofs. \nClients can also opt to wait for a stronger confirmation provided by HotShot (step 3). As long as \nno adversary controls more than one third of the HotShot stake, the client's transaction can \nnever be rolled back. This is especially usefull for bridging, as they are sensitive to reorgs in the \nsource chain of a bridge transaction. \nIn the case that a rollup posts their tx data to the L1, clients can also wait for their tx to be \nfinalized on the L1, though in the case of Ethereum, this guarantee can take 15 minutes to \nattain, as opposed to a few seconds in the case of HotShot.  \nOnce a transaction is finalized, clients may want to read the updated rollup state, either to \ncheck the results of their transaction's execution or to prepare another transaction. They have \nseveral options (listed below) for doing this, depending on who they trust and how much work \nthey are willing to do themselves. Depending on which option they choose, each client can get \ntheir preferred tradeoff between latency, trust, and the amount of work they are required to \nperform. \n• They can rely on the pre-confirmation provided by the sequencer to compute the next \nrollup state. \n• They can leverage Espresso's Global Confirmation Layer and immediately execute it \nthemselves to compute the new state. \n\n• They can get a state update, at the cost of additional trust assumptions, by trusting a \nrollup server who has executed the transaction to give them the new state, even before a \nstate update proof is generated. \n• In the case of a ZK rollup, they can wait for a state update proof to be generated (step 3) \nand check that proof. This requires less computation than executing the block, and it is \nstill trustless. \n• Finally, if a client does not want to do any computation on their own (or in the case of an \noptimistic rollup, where there is no validity proof for the client to check) and does not \nwant to trust a rollup server, they can wait until a state update is certified by the L1 (step \n6) to fetch the updated state with no trust or computation. \nMulti-Chain Blocks \nIn the case that a sequencer has won the right to sequence for multiple chains simultaneously, \nblocks produced by that sequencer (step 3) may include transactions from many different \nrollups. Chains need some way to prove they have executed all of the transactions belonging to \nthem, and only those transactions, preferably without searching exhaustively through the entire \nblock. Without this, chain nodes could censor users by simply ignoring some sequenced \ntransactions, or they could create confusing state updates by executing transactions that were \nintended for another chain. \nTo support this efficiently, each transaction submitted to the sequencer must be associated \nwith a chain-specific identifier (step 2). Each block produced by the sequencer contains \ntransactions organized by namespace. The block also has a short, unique commitment, which \ndepends on the transactions in the block and other metadata, like its height and timestamp. It is \npossible to create proofs relative to this commitment which convince a verifier that a given \nnamespace in the block contains a certain list of transactions. This allows someone who only \ncares about a particular namespace, like a rollup node, to download just the data they care \nabout (plus a short proof) from an untrusted server, and verify it. \nNamespace proofs are small and computationally cheap to check, which makes them suitable \nfor verification by an L1 smart contract or in a ZK circuit. This functionality is essential for ZK \nrollups that need to prove they have executed all the relevant transactions in a certain block in \norder to compute a new state, and for optimistic rollups that need to arbitrate a fraud challenge \nabout the execution of relevant transactions in a block. \nThis namespaced block commitment is the commitment that is certified and stored in the L1 \nsequencer contract (step 4). When rollup contracts need to verify an inclusion proof for a \nsequence of rollup-specific transactions in a particular block, they can read the corresponding \nblock commitment directly from the sequencer contract and use it to verify the proof. \nNote that the sequencer does not authenticate the mapping from chain IDs to transaction \nsequences. Nothing stops a bad actor from submitting a transaction for one chain with the ID of \nanother, or from submitting a transaction which is invalid in a particular chain with the ID of that \nchain. As such, chains will still need the ability to detect and exclude invalid transactions in \ntheir execution layers. This is fundamental to chains with permissionless sequencing. \nA well-behaved chain should choose an ID for itself which is distinct from any IDs which are in \nuse by other well-behaved chains. This is similar to how, in the Ethereum ecosystem, different \nEVM blockchains choose different chain IDs, and the responsibility ultimately falls on users to \n\navoid using a malicious chain which copies the chain ID of some other system. In fact, EVM \nrollups could in theory use their EVM chain IDs as their Espresso chain identifiers. \n \n ",
    "filename": "2. system overview.pdf"
  },
  {
    "id": 13,
    "content": "\n\nCappuccino Testnet Release \nEspresso testnet 5 (Cappuccino)—May 2024 \nWith the release of Decaf, Espresso's persistent testnet, the Cortado testnet is currently \npaused.  \nIn May 2024, we announced the Cappuccino release of Espresso. \nCappuccino continues the process of decentralizing Espresso, by onboarding a total of 10 \noperators to participate in Cappuccino. The 10 operators will be running a total of 100 \ngeographically distributed nodes, all participating in HotShot together.  \nThe Tiramisu DA layer is now also further upgraded and supports VID, which ensures that data is \nrecoverable, even if the CDN and DA committee fail to be responsive.  \nAs part of the Arbitrum tech stack integration, Espresso now supports Arbitrum Nitro fraud \nproofs, which enables Arbitrum chains to be fully productized on top of Espresso. We will be \nreleasing test versions of Arbitrum Orbit chains in collaboration with Caldera and AltLayer.  \nWe have also implemented a simple block builder for Cappuccino. This lays the foundation for \nshared sequencers elected through the Espresso marketplace to build valuable multi-rollup \nblocks and give out atomic execution guarantees for cross-chain transactions. You can track \nactivity on the Cappuccino testnet in our new block explorer here: \nhttps://explorer.cappuccino.testnet.espresso.network/ And you can interact with Cappuccino \nvia the public endpoint here: https://query.cappuccino.testnet.espresso.network/v0/ \nRunning a Node \nConfiguration for Cappuccino nodes \nCappuccino node operators are limited to a select group. If you are interested in running a node \nin a future release of Espresso, contact us. \nThis page give the configuration used to run different types of nodes in the Cappuccino testnet. \nFor general information on running an Espresso node, see Running an Espresso Node. \nAll nodes in Cappuccino use the ghcr.io/espressosystems/espresso-\nsequencer/sequencer:cappuccino Docker image. Depending on the type of node, the \nconfiguration varies. \nRegular Node \nCommand \nsequencer -- http -- catchup -- status \nEnvironment \nSame for all nodes \nCopy \nESPRESSO_SEQUENCER_ORCHESTRATOR_URL=https://orchestrator.cappuccino.testnet.espre\nsso.network \nESPRESSO_SEQUENCER_CDN_ENDPOINT=cdn.cappuccino.testnet.espresso.network:1737 \n\nESPRESSO_STATE_RELAY_SERVER_URL=https://state-\nrelay.cappuccino.testnet.espresso.network \nESPRESSO_SEQUENCER_GENESIS_FILE=/genesis/cappuccino.toml \nRUST_LOG=\"warn,libp2p=off\" \nRUST_LOG_FORMAT=\"json\" \n \n# At least one state peer is required. The following URL provided by Espresso works. \n# Optionally, add endpoints for additional peers, separated by commas. \nESPRESSO_SEQUENCER_STATE_PEERS=https://query.cappuccino.testnet.espresso.network \nChosen by operators \nCopy \n# JSON-RPC endpoint for Sepolia testnet \nESPRESSO_SEQUENCER_L1_PROVIDER # e.g. https://sepolia.infura.io/v3/<API-KEY> \n \n# Port on which to host metrics and healthchecks \nESPRESSO_SEQUENCER_API_PORT # e.g. 80 \n \n# Path in container to store consensus state \nESPRESSO_SEQUENCER_STORAGE_PATH # e.g. /mount/sequencer/store/ \n \n# Path in container to keystore \nESPRESSO_SEQUENCER_KEY_FILE # e.g. /mount/sequencer/keys/0.env \nVolumes \n• $ESPRESSO_SEQUENCER_STORAGE_PATH \n• $ESPRESSO_SEQUENCER_KEY_FILE \nDA Node \nRequires operator to additionally run a Postgres server \nCommand \nsequencer -- storage-sql -- http -- catchup -- status -- query \nEnvironment \nSame for all nodes \n\nCopy \nESPRESSO_SEQUENCER_ORCHESTRATOR_URL=https://orchestrator.cappuccino.testnet.espre\nsso.network \nESPRESSO_SEQUENCER_CDN_ENDPOINT=cdn.cappuccino.testnet.espresso.network:1737 \nESPRESSO_STATE_RELAY_SERVER_URL=https://state-\nrelay.cappuccino.testnet.espresso.network \nESPRESSO_SEQUENCER_GENESIS_FILE=/genesis/cappuccino.toml \nESPRESSO_SEQUENCER_POSTGRES_PRUNE=\"true\" \nESPRESSO_SEQUENCER_PRUNER_PRUNING_THRESHOLD=\"549755813888\" # 0.5 TB \nESPRESSO_SEQUENCER_PRUNER_MINIMUM_RETENTION=\"1d\" \nESPRESSO_SEQUENCER_PRUNER_TARGET_RETENTION=\"7d\" \nESPRESSO_SEQUENCER_PRUNER_BATCH_SIZE=5000 \nESPRESSO_SEQUENCER_IS_DA=\"true\" \nESPRESSO_SEQUENCER_FETCH_RATE_LIMIT=25 \n \nRUST_LOG=\"warn,libp2p=off\" \nRUST_LOG_FORMAT=\"json\" \n \n# At least one state peer is required. The following URL provided by Espresso works. \n# Optionally, add endpoints for additional peers, separated by commas. \nESPRESSO_SEQUENCER_STATE_PEERS=https://query.cappuccino.testnet.espresso.network \nESPRESSO_SEQUENCER_API_PEERS=https://query.cappuccino.testnet.espresso.network \nChosen by operators \nCopy \n# JSON-RPC endpoint for Sepolia testnet \nESPRESSO_SEQUENCER_L1_PROVIDER # e.g. https://sepolia.infura.io/v3/<API-KEY> \n \n# Port on which to host metrics, healthchecks, and DA API \nESPRESSO_SEQUENCER_API_PORT # e.g. 80 \n \n# Path in container to keystore \nESPRESSO_SEQUENCER_KEY_FILE # e.g. /mount/sequencer/keys/0.env \n\n \n# Connection to Postgres \nESPRESSO_SEQUENCER_POSTGRES_HOST \nESPRESSO_SEQUENCER_POSTGRES_USER \nESPRESSO_SEQUENCER_POSTGRES_PASSWORD \nVolumes \n• $ESPRESSO_SEQUENCER_KEY_FILE \nArchival Node \nRequires operator to additionally run a Postgres server \nCommand \nsequencer -- storage-sql -- http -- catchup -- status -- query -- state \nEnvironment \nSame for all nodes \nCopy \nESPRESSO_SEQUENCER_ORCHESTRATOR_URL=https://orchestrator.cappuccino.testnet.espre\nsso.network \nESPRESSO_SEQUENCER_CDN_ENDPOINT=cdn.cappuccino.testnet.espresso.network:1737 \nESPRESSO_STATE_RELAY_SERVER_URL=https://state-\nrelay.cappuccino.testnet.espresso.network \nESPRESSO_SEQUENCER_GENESIS_FILE=/genesis/cappuccino.toml \nESPRESSO_SEQUENCER_FETCH_RATE_LIMIT=25 \nRUST_LOG=\"warn,libp2p=off\" \nRUST_LOG_FORMAT=\"json\" \n \n# At least one state peer is required. The following URL provided by Espresso works. \n# Optionally, add endpoints for additional peers, separated by commas. \nESPRESSO_SEQUENCER_STATE_PEERS=https://query.cappuccino.testnet.espresso.network \nESPRESSO_SEQUENCER_API_PEERS=https://query.cappuccino.testnet.espresso.network \nChosen by operators \nCopy \n# JSON-RPC endpoint for Sepolia testnet \nESPRESSO_SEQUENCER_L1_PROVIDER # e.g. https://sepolia.infura.io/v3/<API-KEY> \n\n \n# Port on which to host metrics, healthchecks, and query API \nESPRESSO_SEQUENCER_API_PORT # e.g. 80 \n \n# Path in container to keystore \nESPRESSO_SEQUENCER_KEY_FILE # e.g. /mount/sequencer/keys/0.env \n \n# Connection to Postgres \nESPRESSO_SEQUENCER_POSTGRES_HOST \nESPRESSO_SEQUENCER_POSTGRES_USER \nESPRESSO_SEQUENCER_POSTGRES_PASSWORD \nVolumes \n• $ESPRESSO_SEQUENCER_KEY_FILE \nDeploying a Rollup on Cappuccino \nUsers interested in deploying their own rollup on Cappuccino can make use of the following \n• Query Service: https://query.cappuccino.testnet.espresso.network \n• Light client contract address: 0xfdbf8b5ed2c16650aa835315a67d83eda5c98872 \nFor more information on deploying an Arbitrum Nitro chain to Cappuccino, please see the \nfollowing guide. This makes use of our Arbitrum Nitro integration with Espresso. \nBenchmarks \nPerformance metrics for HotShot consensus and Tiramisu data availability in Espresso's \nCappuccino testnet release \nAs a part of launching the Cappuccino testnet and releasing our implementation of HotShot \nunder the MIT license, we are publishing benchmarks related to performance of this release. \nCompared to earlier benchmarks, these results benchmark the addition of Tiramisu DA's \nSavoiardi layer to the HotShot protocol. \nIn our evaluations, we progressively increased the block size from 50KB to 20MB and tested on \nnetwork sizes ranging from 10 to 1000 nodes. In all settings, a subset of 10 nodes serves both as \nvalidators and the committee for Tiramisu DA's Mascarpone layer. As shown in the below figure, \nthroughput rises with the increasing load without a corresponding increase in latency, up to a \ncertain point of saturation. Beyond this point, latency begins to increase while throughput either \nremains steady or shows a slight increase. In the below table, we show the benchmark data for \nblock sizes of 5MB block size, which is approximately the turning point. \n \nHotShot throughput vs. end-to-end latency for varying network sizes and increasing block sizes \n\nNetwork Size \nMascarpone Committee Size \nBlock Size (MB) \nAverage Latency (s) \nAverage View Time (s) \nThroughput (MB/s) \n10 \n10 \n5 \n3 \n1.08 \n4.58 \n100 \n10 \n5 \n2 \n0.85 \n5.76 \n200 \n10 \n5 \n4 \n1.21 \n4.04 \n500 \n10 \n5 \n9 \n1.97 \n2.48 \n1000 \n\n10 \n5 \n21 \n5.56 \n0.88 \nExperimental Setup \nThese benchmarks were run on HotShot version 0.5.63. \nWe conducted our experiments on two types of machines: \n• CDN Instances: Our CDN (repository located here) is a distributed and fault-tolerant \nsystem responsible for routing messages between validators. The CDN was run across 3 \nAmazon EC2 m6a.xlarge instances located in the us-east-2 region. Each instance ran a \nbroker, which is the component responsible for forwarding messages to their intended \nrecipients. One instance also ran the marshal service, which is the service that \nfacilitates the authentication and marshaling of validators to a specific broker. Each \ninstance had 4 vCPUs and 16.0 GiB memory. \n• Validator Instances: HotShot nodes were run on Amazon ECS tasks with 2 vCPUs and 4 \nGiB memory. Nodes were equally distributed among the us-east-2a, us-east-2b and us-\neast-2c availability zones. \nData Calculation \nEach benchmark was run until 100 blocks were committed. After each benchmark run, nodes \nreported:  \n• the total time elapsed for the run \n• the throughput per second \n• the total latency \n• the total number of blocks committed \n• the total number of views it took to reach 100 commits \n• the number of failed views (views that failed to make progress) \nThese values were collected and averaged in the final results. Note that throughput is measured \nin megabytes per second, not mebibytes per second. \nAnalysis of Results \n• Our implementation of the Tiramisu data availability protocol achieves better maximum \nthroughput in large networks than standard consensus protocols where data is sent to \nall nodes. That being said, this particular implementation’s latency is worse in large \nnetworks. However, we’ve identified several implementation-specific bottlenecks to fix \nthis issue. \n\n• During benchmarks where the network is unsaturated with data, small network sizes (10 \nand 100 nodes) achieve finality in ~1s, and large network sizes (500, and 1000 nodes) \nachieve finality between 2-5s. \n• The primary bottlenecks of this particular implementation are twofold: \no Our current implementation of Tiramisu DA's Savoiardi layer is compute-\nintensive. This causes builders, leaders, and Mascarpone DA committee \nmembers to spend additional time computing Savoiardi shares during each \nview. This bottleneck can be addressed by more optimally parallelizing intensive \ncompute, dynamically tuning Savoiardi parameters such as multiplicity to \noptimally encode block data, experimenting with different hardware such as \nGPUs, and having the Cocoa layer optimistically calculate Savoiardi shares. \no The builder used in these benchmarks is a simple, naive builder. Unlike a \nsophisticated builder, this builder does no optimistic execution or optimistic \nSavoiardi calculations. The simple builder does not begin building blocks until \nthe HotShot leader requests it to do so. This causes the builder to be slow in \nreturning block data to the HotShot leader, thus adding unneeded latency each \nview. This bottleneck can be addressed by using a sophisticated builder that \noptimistically builds blocks. \n• This implementation of HotShot uses the HotStuff-1 protocol. We plan to upgrade to the \nHotStuff-2 protocol in the future, which will reduce commit latency significantly. \nNotes \n• These benchmarks did not use a public transaction mempool. Instead, block builders \nwere configured to build predetermined-sized blocks each view. This configuration is \nequivalent to block builders only building blocks with privately-sent transactions. A \npublic mempool is part of the current HotShot implementation, however, and will be \nincluded in future benchmarks. Note that throughput and latency results will differ with \nthe inclusion of the public mempool. \n• Multiplicity: Tiramisu DA's Savoiardi VID scheme is inspired by Ethereum's \ndanksharding proposal, where the block payload is viewed as a list of polynomial \ncoefficients. Ordinarily, these coefficients are partitioned into multiple polynomials, and \neach storage node gets one evaluation from each of those polynomials. At the other \nextreme, one could instead gather these coefficients into a single high-degree \npolynomial, and give each storage node multiple evaluations from this polynomial. We \nuse the word “multiplicity” to denote the number of evaluations per polynomial sent to \neach storage node. Multiplicity is a parameter that can be tuned between two extremes \nto optimize performance.  \n ",
    "filename": "20. cappucino tesnet.pdf"
  },
  {
    "id": 14,
    "content": "\n\nAppendix \nInteracting with L1 \nEspresso is intended primarily for use with rollups, and an essential feature of rollups is that \nthey periodically send verified checkpoints of their state to some other blockchain, the layer 1. \nIn the case of Espresso we have two types of checkpoints: \n• HotShot checkpoints record the history and state of HotShot. This includes \ncommitments to all blocks that have been sequenced and the state that is needed to \nverify new blocks (such as the stake table) starting from the latest checkpoint. These \ncheckpoints are recorded in the sequencer contract and shared by all applications \nusing Espresso. \n• Rollup checkpoints record the state of individual rollups, such as the state of an EVM \ninstance. Each rollup records its own checkpoints in its rollup contract. When recording \na new checkpoint, the rollup contract will validate the checkpoint against the \ncommitted sequence of blocks by reading the historical sequence of block \ncommitments from a corresponding HotShot checkpoint. \nMany rollups will not only write their state to the L1, but will also allow information from the L1 \nto flow back into the rollup. In this case, the interaction with the rollup contract becomes a \nbridge between the L1 and the rollup, allowing tokens and other information to be exchanged \nbidirectionally. \nEspresso is interesting in that HotShot itself also allows this bidirectional flow of information to \nand from the L1, specifically regarding the stake table. This allows operations on the L1 to \ninteract with the Espresso staking token, a crucial requirement for implementing restaking, \nwhich allows Espresso to share the security budget of the L1 and pass value generated by the \nsequencer back to L1 validators. \nSince state updates are not only written to the L1 but also read back into the HotShot and rollup \nstates, we will hereafter eschew the \"checkpoint\" terminology and instead refer to HotShot \nstate updates and rollup state updates. \nThe following sections dive into the use cases for these state updates and explore some of the \nutility and security properties that L2s are able to derive from the L1 that they use for state \nupdates. \nTrustless Sync \nOne of the main reasons to use blockchains is to decentralize trust. However, the current \nEthereum ecosystem often compromises on this principle by using trusted query services like \nInfura for clients to access the blockchain. This trades off trustlessness for convenience and \nscalability, since a client that trusts a query service does not need to verify any Ethereum block \ndata. \n \nHowever, Ethereum's switch to proof-of-stake and its rollup-centric roadmap offer the potential \nto break out of this tradeoff, enabling efficient and user-friendly clients that retain decentralized \ntrust. When rollups post their state updates to Ethereum or a similar L1, each L1 validator \nindependently validates the state transition for the rollup by executing the rollup's smart \n\ncontract. This means that any user who trusts the collective L1 validator set can quickly sync \nwith the latest state of the rollup simply by reading a recent, verified state update from the L1 \nstate. \n \nOf course, this only pushes the problem of fast, trustless sync to the L1. This is nonetheless a \nsubstantial improvement. In the case of Ethereum, the proof-of-stake consensus protocol and \nthe fixed block time enable the creation of L1 light clients which sync far faster than real time. \nFor instance, Helios is an Ethereum light client which manages trustless sync in only 2 seconds. \nBy verifying HotShot and rollup states on Ethereum, any such Ethereum client can be turned \ninto a client for any rollup merely by syncing with Ethereum and then reading rollup state from \nthe appropriate smart contract. \nFork Recovery \nWhen we think of rollups checkpointing to an L1, one of the first things that comes to mind is \nallowing the rollups to inherit the security of the L1. This subject is subtle, and discussions of it \noften lack a clear accounting of the security properties we hope for the rollup to obtain and how \nexactly they are inherited. This section will detail how rollups that checkpoint to an L1 inherit an \nextra level of finality from the L1. \nFirst, it is important to note that Espresso, on its own, even without any form of checkpointing, \nalready offers extremely strong finality guarantees. In order to break finality, and adversary must \ncause a safety violation in the HotShot finality gadget, but HotShot is a secure BFT protocol—\nviolating its security requires an adversary to control over 1/3 of the total stake, a massive \ninvestment once HotShot reaches a critical mass of adoption. \nNevertheless, we can consider what happens in the extremely unlikely event that HotShot does \nsuffer a safety violation, and some block which was considered final according to the HotShot \nprotocol is removed from the history of the ledger. In this case, we can maintain finality by using \nthe HotShot checkpoints on L1 to decide on an immutable, canonical chain. \nLet's look at how such a scenario could happen. There are two ways an adversary could \ncompromise HotShot's finality. \nFirst, if they control more than 1/3 of the total stake at any given time, they can cause or exploit \na failure in the network connecting honest nodes, partitioning honest nodes controlling 1/3 of \nthe stake each into two disjoint sub-networks. The adversary, being Byzantine, can then use \ntheir 1/3 of the stake to vote for conflicting blocks in each partition, committing both conflicting \nblocks with a 2/3 quorum each, and thus creating a fork. \nSecond, an adversary can create a fork without even controlling 1/3 of the current stake via a \nlong range attack. In a long range attack, an adversary compromises some old private keys \nwhich were used to sign a quorum certificate many blocks ago. This may be substantially \ncheaper than compromising current private keys, as the owners of the old private keys may \nalready have withdrawn their stake and thus may not be very invested in maintaining the \nsecurity of their keys. \nWhile compromising old keys does not directly enable the adversary to append new invalid \nblocks, they can build a new fork including signed QCs starting from the block at which they \ncompromised the keys, and unilaterally grow this fork until it is similar in length to the canonical \nchain. \n\nOnce an adversary has created a fork, they can compromise finality by convincing honest nodes \nto switch from the canonical branch of the fork to the malicious branch. Blocks which were \nconsidered finalized on the canonical branch may not have been committed at all on the \nmalicious branch. \n \nFork choice between two conflicting QCs without L1 checkpoints \nThe adversary does this by taking advantage of an ambiguity which honest nodes must \nsomehow resolve when they are joining the network for the first time or catching up after being \noffline for some time. In order to catch up, such nodes will look for signed QCs attesting to \nHotShot state changes, so they can figure out which state to sync with. Since they cannot tell \nthe difference between honest peers and adversaries, and adversary who can provide QCs from \nthe malicious fork faster than honest peers can provide QCs from the canonical fork can \nconvince honest nodes in catchup to sync to the malicious fork. Over time, the adversary may \nconvince enough honest nodes to switch that the malicious fork becomes the \"canonical\" one, \nand blocks which were committed in the old canonical fork are permanently lost. \nBy leveraging the L1 checkpoints, we can prevent an adversary from tricking honest nodes onto \nthe wrong branch of a fork, even if the adversary can present signed QCs. We simply use the \ncheckpointed state on L1 as a deterministic fork choice rule: when presented with conflicting \nQCs during catchup, an honest node will choose the one which is compatible with the state \ncheckpointed on L1. In this way, the L1 checkpoint plays for the Espresso blockchain the same \nrole that weak subjectivity checkpoints play for Ethereum. \n\nChoosing a fork consistent with the latest L1 checkpoint only requires a node to check the \nhistory between the latest L1 checkpoint and the QCs justifying each branch of the fork, which \nshould be a fairly small, bounded amount of work, assuming checkpoints are posted at a fixed \nrate and fairly frequently. This is far more practical than storing and validating the entire history, \nwhich a node would have to do in order to detect a fork arbitrarily far in the past without using a \ntrusted checkpoint. \n \n\nFork choice between two conflicting QCs, facilitated by L1 checkpoints \nIn order to ensure that this fork choice rule uniquely specifies a branch, the L1 sequencer \ncontract enforces a linear history. This is easy enough to do by storing the latest block \ncommitment and checking that each new block specifies the previous block commitment as its \nparent (and has the correct height). This is difficult to do in a node because it requires the node \nto receive every single block in order, but in practice nodes are not always online, and when they \nrecover from an outage they catch up immediately to the head of the chain without validating \nconsensus for every intermediate block. The contract, on the other hand, does behave as if it is \nalways online and directly validates the QC for every block consecutively, because anyone can \ndrive the contract forward by providing a block and a QC to append, so we only require that \nsome honest node is online for every block in order for the contract to remain live. \nBy forcing the L1 checkpoints to be linear and ensuring that honest nodes always work on a fork \nwhich is compatible with those checkpoints, we gain L1-level finality for any blocks which have \nbeen included in an L1 checkpoint. Note that this checkpointing scheme does not prevent an \nadversary from causing a fork—the L1 HotShot contract will not distinguish between \n\"malicious\" and \"canonical\" forks when appending a new block, so long as that block has a valid \nQC and follows from its parent. It will take whichever block it sees first. However, it will ensure \nthat once a block is added to the contract, its fork becomes canonical, and the block will \nforever remain in the canonical chain. \nBridging \nRollup state updates facilitate interoperability between the layer 1 and the rollup. If the state of \nthe rollup is verified and stored by the layer 1, then the layer 1 can also validate claims against \nthat state, such as a claim that some tokens have been deposited into a bridge contract on the \nrollup. The L1 can also write to the state which is maintained by the L1, and the rollup can thus \nreceive messages and tokens from the L1. \nThis is the idea used by the LX-to-LY bridge, which Polygon zkEVM uses to bridge ETH between \nthe layer 1 and layer 2. In this design, part of the L1 state, a Merkle tree of messages to be sent \nto the L2, is represented directly in the L2 VM semantics. Since the canonical execution of L2 \ntransactions happens in a smart contract on the L1, this executor is able to read from the \nappropriate L1 state when executing operations in the L2 VM. \nGlossary of Key Terms \nData Availability Layer - Storage nodes that guarantee the availability and retrievability of \ntransaction raw data and in charge of the eventual dissemination of the full data across the \nnetwork. \nData Availability Proposal - The DA proposal is a proposal on a block for data availability. The \nconsensus on a data availability proposal makes sure that enough parties have the block data. \nThe DA proposal consists of the block and the corresponding view number. \nData Availability Votes - A DA vote is a vote indicating the receipt of either a DA proposal or a \nvalid VID share. The vote contains the justify QC commitment, view number, block \ncommitment, signature, and vote token. Data Availability Certificate (DAC) - DAC is a \ncertificate that the proposed data is available to a quorum of distinct parties in the random \nsmall data availability (DA) committee. A DA leader, after receiving a sufficient number of votes \non its DA proposal, assembles the votes into an optimistic DAC. Data Retrievability Certificate \n\n- A retrievability certificate is a certificate that valid VID shares are available to a quorum of \nreplicas. This ensures the liveness/data availability of the HotShot protocol even if the random \nsmall DA committee is bribed by an adversary. A DA leader assembles the retrievability \ncertificate after receiving enough votes from the entire node/replica set. \nOptimistic Data Availability Certificate - A optimistic DA certificate is a certificate that the \nproposed data is available to a quorum of distinct parties in the random small data availability \n(DA) committee. A DA leader, after receiving a sufficient number of votes on its DA proposal, \nassembles the votes into an optimistic DAC. \nCommitment Proposal - A commitment proposal is a proposal on a block commitment. A \nblock can only be applied to the chain if the consensus on the commitment proposal is \nreached. A commitment proposal is used as follows: \n• A (consensus) leader, upon receiving a DAC for this view or sufficient quorum votes for \nthe previous view, will construct a commitment proposal and multicast it to replicas. \nThe commitment proposal will contain the block commitment, view number, height, \njustify QC, and the proposer ID. \n• A node, after receiving the commitment proposal, if also gets either the DAC or the DA \nproposal, will validate the set and send the quorum vote back to the leader.\\ \nQuorum Vote - quorum vote is a vote on the commitment proposal representing the status of \nthe node/replica's decision on the proposal. \nYes Vote: is sent if the replica successfully verifies the commitment proposal. It consists of the \njustify QC commitment, view number, leaf commitment, signature, and vote token. No Vote: on \nthe contrary, indicates the replica's rejection. A no vote has the same contents as a yes vote. \nTimeout Vote: means the replica cannot decide due to timeout. Therefore, it does not include \nthe leaf commitment as the yes or no vote does, but has all the other fields. \nThe node is not only signing on the data that is voted on but also the vote type, preventing a \ndishonest node from altering the vote. E.g., it is impossible to use the signature associated with \na no vote to create a yes vote. DA Vote - A node vote on the data commitment, indicating the \nreceipt of either the corresponding data proposal or a valid VID share. \n ",
    "filename": "21. appendix - end.pdf"
  },
  {
    "id": 15,
    "content": "\n\nProperties of HotShot \nHotShot’s design intentionally aims to give chains fast confirmations to transactions, while \nbeing able to scale to a large number of participating nodes. However, the participating nodes \ndo not execute transactions; hence, individual nodes only need assurance of data availability to \nvote in consensus, not to have full access to the data. This alleviates high hardware \nrequirements for participation, without sacrificing throughput. \nHotShot is based on the consensus techniques used within HotStuff and HotStuff-2. \nFor more details on HotShot and EspressoDA, see our post, Designing the Espresso Network: \nCombining HotShot Consensus with Espresso's Data Availability Layer \nKey Properties of HotShot \nSeparating data availability (DA) and execution from consensus \nThe HotShot implementation is purpose-built for providing fast confirmations to a large number \nof generic chains. In particular, it does not perform execution, and the data availability \nrequirement (i.e., ensuring the system has access to data) is handled by a separate DA solution \n(integrating chains can use the DA solution of their choice, but have default access to use our \ncustom, low-cost DA layer, EspressoDA). This enables HotShot to process more data than \ntypical state machine replication protocols. Such modularity also allows the use of various \nappropriate sub-protocols as needed.  \nScalability \nHotShot relies on all-to-leader and leader-to-all communication, thus reducing the consensus \ncommunication complexity to linear in the number of nodes. Since HotShot does not require \nevery node to get a full copy of transaction data, low consensus communication is especially \nimportant. HotShot combines this optimistically with a content delivery network (CDN) to \nefficiently route data and perform computation. This reduces the leader bottleneck and \nsupports a system with a heterogeneous set of nodes, without sacrificing safety and liveness \nguarantees. These improvements will help HotShot to scale to thousands of nodes, such that it \ncan be run by a large number of Ethereum validators through restaking.  \nResponsiveness \nHotShot is optimistically responsive and thus, under favorable conditions, commits new blocks \nas fast as the network allows. This ensures that the protocol’s performance is directly related to \nthe state of the network—under optimistic conditions, the protocol can have low latency and \nconsequently high throughput, too. In HotShot, using a CDN at the network layer synergizes with \nthe optimistic responsiveness property to provide even better performance. \n \nEspressoDA \nFor a detailed technical overview of EspressoDA, see EspressoDA: Our Three-Layered DA \nSolution. \nData availability is the requirement that all transaction data included in blocks is available to \nevery node participating in consensus before a decision can be reached. Because the amount \nof data can be quite large, requiring each node to download and verify the data before reaching \n\nconsensus presents a fundamental bottleneck in the throughput of consensus protocols (i.e., \nthe data availability problem).  \nEspressoDA resolves this bottleneck while ensuring data availability via a three-layer system \nwe've designed to balance performance and security: \n• VID Layer: Stores erasure-coded data chunks across all nodes. \n• DA Committee Layer: A small committee stores the full data and guarantees efficient \nrecovery of data. \n• CDN Layer: Uploads full data for retrieval efficiency. \nVID Layer \nEspressoDA eliminates the need for each storage node to download all block data by using \nVerifiable Information Dispersal (VID), a technique that encodes block data into erasure-coded \nchunks, which are disseminated among nodes in a way that recoverability is ensured. Nodes \nonly need to store their chunk rather than the entire block. This method is more efficient than \ndata availability sampling (DAS) as it limits unnecessary redundancies. \nBy using VID, EspressoDA guarantees a block will only be finalized if data is verified to be \navailable. \nDA Committee Layer \nA small DA committee, selected from the network's nodes, receives the entire data blob and \nallows for very fast data retrievability, with the VID protocol acting as a fallback in case the DA \ncommittee fails to make data available. \nEspressoDA ensures data is made available for rollups (optimistically by the DA committee, and \nguaranteed through VID) without incurring the high costs of posting transactions to the \nEthereum L1 (though rollups may still choose to do so). It also avoids centralized DA solutions, \nwhich allow the DA operators to freeze the rollup and censor its users. \nCDN Layer \nWe provide EspressoDA with web2-level performance by using a content delivery network \n(CDN) to quickly share a block’s data to many different nodes. It can massively accelerate data \ndissemination. Benchmarks from our Cappuccino testnet show a data dissemination of around \n5.7 MB/s with 100 nodes. The CDN can also help with efficient recovery of subsets of the data, \nsuch as single transactions. \nImportantly, the CDN is not trusted for security and thus doesn’t present a single point of failure. \nEspressoDA works perfectly fine without the CDN, which is only helpful for accelerating the DA \nand can easily be replaced or removed. \n \nHow It Works \nA step-by-step guide on the data availability process \nOverview \n\nThe data availability process initiates with sequencers submitting blocks to HotShot. Each view, \na leader is selected within HotShot who bundles these blocks into a single block within \nHotShot. Rather than sending the full block data to other HotShot nodes, the leader only sends \na commitment to the block for other nodes to vote on. All HotShot nodes also participate as \nstorage nodes in the VID protocol and receive a small chunk representing their VID share of the \nrespective block. The DA leader sends a DA proposal to the randomly sampled DA committee \nand all HotShot nodes. After receiving enough votes from the DA committee and the nodes in \nthe network, the DA leader constructs a data availability certificate (DAC). \nThe DAC is composed of an optimistic DAC obtained from the DA committee and a retrievability \ncertificate from the VID protocol. The optimistic DAC certifies that the proposed data is \navailable to a quorum of the DA committee. The retrievability certificate in turn certifies that VID \nchunks are available to a quorum of nodes. The DAC design thus enables the best of both \nworlds, fast DA through DA committees, and robustness through VID. By combining the block \ncommitment with the DAC, Tiramisu ensures that HotShot blocks will only finalized if data is \nguaranteed to be available. \nExhibit A: DA and Rollup Architecture \n \nProcess Steps \nStep 1: The DA leader begins a broadcast to ensure data is available for all nodes in the network \nthat consists of: \n• Sending the DA proposal to the DA committee. \n• Sending the VID chunks to all replicas/nodes. \n• Gradually sending the DA proposal to all replicas/nodes. Broadcasting proceeds \nconcurrently, prioritized by order of initialization. \nA DA proposal will be rejected in either of the following cases: \n• The view number is earlier than the view corresponding to the latest valid quorum \ncertificate (QC). \n• The proposal is not from the correct leader. \nStep 2: Nodes receive the DA proposal (and/or VID share) and submit the data availability vote. \nAnyone who receives and approves the DA proposal sends a strong DA vote to the DA leader, \nwhile anyone who only receives the VID share sends a normal DA vote. \nStep 3: The DAC is formed. \nThe DAC is formed with the creation of the retrievability certificate and optimistic DAC \ncertificate. \nThe retrievability certificate can be formed by: \n• The DA leader receives f + 1 strong votes, which may come from nodes on the \ncommittee or just regular nodes who happened to receive the DA proposal quickly \n\nenough. f = number of faulty nodes (nodes not performing correct function) in the entire \nnetwork. \n• f + m normal votes, where m is the number of VID shares the block was split into. \nThe optimistic DAC can be formed in the following way: \n1. The DA leader receives 2f + 1 strong votes from the DA committee (where, unlike above, f is \nthe number faulty nodes in the committee, not in the entire network) \nThe DA leader stops broadcasting to the nodes after the DAC is formed, or when the quorum \ncertificate from the next leader is received. \nStep 4: The block commitment proposal is sent to replicas/nodes. The block commitment is a a \ncryptographic proof that a block is valid and has guaranteed DA. \nThe leader, once getting sufficient quorum votes for the previous view and the DAC is obtained, \nsends the commitment proposal. A block can only be applied to a chain if consensus on the \ncommitment proposal is reached by HotShot consensus nodes. The leader also sends the QC \nto the next leader if one can be constructed. \nStep 5: The HotShot nodes validate the commitment proposal. \nThe replicas/nodes validate the commitment proposal if either of the following set is received: \n• Commitment proposal and the DAC. \n• Commitment proposal and DA proposal, which is simply a block and the view number \nproposed \nThe node will send a quorum vote to the next consensus leader once the commitment proposal \nis validated. As long as over 2/3 of HotShot stake is honest, it is impossible for an adversary, \neven if bribing the DA committee, to forge a DAC. Utilizing the randomly elected DA committee \nalongside VID thus enables fast and secure DA, and disincentivizes bribery attacks. \nExhibit B: DA Process Overview \n \nImplementation Options \nBy integrating with the Espresso Network, chains can utilize EspressoDA without any additional \nintegration work. However, chains retain the flexibility to use the DA solution of their choice (i.e., \nchains can use the Espresso Network for fast confirmations, but use Ethereum or another third-\nparty provider for DA). \n ",
    "filename": "3. hotshot.pdf"
  },
  {
    "id": 16,
    "content": "\n\nInterfaces \nThis section defines in detail the interfaces between each major component in the system \narchitecture. \nEspresso ↔ Rollup \nIn order to keep HotShot nodes themselves as generic and simple as possible, there is no \nrollup-specific logic in Espresso itself, and thus Espresso never actively communicates with any \nrollup. Instead, HotShot query service nodes present a public interface which rollups are \nexpected to query in order to integrate with Espresso. This interface takes the form of a REST \nAPI. See the API reference for details. \nUsage \nRollup nodes may use these APIs differently depending on the role they are playing in the \nsystem (e.g., prover, full node, etc.). A prover can use the API to stream block data from a node, \nso that it can execute blocks as they are finalized and generate proofs. The prover also interacts \nwith the L1, since it can only verify a rollup proof on the L1 if the L1 has already verified the \nsequencing of the corresponding block. \n \nProver / Sequencer Integration \nA rollup may also include full nodes which store and provide access to rollup-related state, but \ndo not run a prover. Such a full node can stream blocks and verify consensus proofs (QCs) \ndirectly from the HotShot APIs, without interacting with the L1. Avoiding interaction with the L1 \nallows state updates to be computed faster. \n \nFull Node / Sequencer Integration \n\nThe rollup also interacts with HotShot via the submit API. This interaction is completely \nindependent of the streaming interaction illustrated above. It is simply used to add transactions \nto HotShot’s mempool so that they may eventually be included in the sequence. Any rollup \nnode which serves a rollup API (e.g. JSON-RPC) should be able to handle transaction \nsubmissions through HotShot's submit API. \nThe body of submit requests includes the transaction to submit as well as a rollup-specific \nnumeric identifier. This identifier is associated with the transaction in the final sequence, so \nrollup proofs can use the ID to easily exclude transactions intended for other rollups. Each \nrollup should have its own protections against cross-rollup replay attacks, such as an EVM \nchain ID, in addition to this rollup ID. \n \nEspresso ↔ L1 \nEspresso interacts with the L1 via the sequencer contract, which validates HotShot consensus \nand provides a certified, trustless interface for other participants to check the sequence of \nblocks. Note that the contract only deals with short block commitments, not full blocks, in \norder to minimize the cost of sending data to the L1. Anyone who has verified a commitment \nagainst the contract can get the corresponding block—and authenticate it against the \ncommitment—from the HotShot availability API. \nEspresso interacts with the sequencer contract via an interface like: \n// HotShot.sol \nCopy \nstruct QC { /* Fields omitted */ }  \n \n// Root of a Merkle tree accumulating the verified sequence of block commitments. \nbytes32 public commitmentsRoot; \n \n// Event emitted when new blocks are sequenced. \nevent NewBlocks(uint firstBlockNumber, uint256[] commitments, bytes32[] frontier); \n  \n// Called to append a chain of new blocks, given proof that consensus has finalized them. \nfunction newBlocks(QC[] calldata qcs, bytes calldata proof, bytes32[] calldata frontier) external; \nThe newBlocks method allows a sequencer node to append a list of newly sequenced block \ncommitments to the log stored in the contract. It takes a list of quorum certificates, a validity \nproof, and a Merkle frontier corresponding to commitmentsRoot, and it validates that \n• Each QC extends from the previous QC in the chain (starting with the previously \nsequenced QC) \n• Each QC is properly signed (the contract will need to store and keep up-to-date the \nstake table) \n\n• There are enough QCs to prove finality for one or more block commitments. HotShot \nconsensus currently requires a chain of at least 3 QCs before the first QC in the chain is \nconsidered finalized (an upcoming version of HotShot will only require a 2-chain of QCs) \nIf validation succeeds, it updates commitmentsRoot, which can then be used by other \ncontracts to validate proofs of inclusion of block commitments in the sequence. On success, \nnewBlocks emits a NewBlocks event informing clients (e.g. rollup provers) that new blocks have \nbeen appended. Those clients can read the new block commitments from the event logs using \nan Ethereum client. The event logs also include a snapshot of the Merkle frontier just before the \nnew blocks were appended. A client can construct a Merkle path for any given commitment by \nappending commitments to the snapshotted frontier. \nnewBlocks will fail if the given batch has already been sequenced, since qcs will fail the check \nthat it must extend from the last sequenced QC. This ensures that each batch of blocks will only \nbe sequenced once — whoever calls this method first will be the one to sequence it. It is an \nopen question whether the contract will explicitly incentivize sequencer nodes to call \nnewBlocks. \nTo learn more about the sequencer contract, how it stores data, and how it validates QCs, read \nthe section on its internal functionality. \nRollup ↔ L1 \nEach rollup communicates with the L1 via its own rollup contract, which can have a unique \npublic interface. In order to verify state updates sent from the rollup (either proactively with \nvalidity proofs in the case of ZK-rollups, or when presented with a fraud proof in the case of \noptimistic rollups), each rollup contract must have access to the certified sequence of blocks \nwhich led to the claimed state update. \nWhen using Espresso, the authoritative sequence of blocks is the output of HotShot, which is \nreplicated to the L1 and certified by the HotShot contract. Therefore, rollup contracts will \ninterface with the HotShot contract. The interface which allows rollup contracts to query the \ncertified sequence of block commitments is very simple: the sequencer contract provides a \npublic sequencedCommitments field, which is an array of block commitments which have \nbeen verified by the contract. \n \n \n ",
    "filename": "4. interfaces.pdf"
  },
  {
    "id": 17,
    "content": "\n\nInternal Functionality \nThis section describes in detail the internal workings of each component in Espresso. \nContracts Overview \nCore Contracts: \n• LightClient \n• FeeContract \n• StakeTable (not used for Mainnet 0) \nEspresso Node \nAn Espresso node is a participant in the HotShot consensus protocol which also makes \navailable some services to support L2 clients. The Espresso node is L2 agnostic: it does not \nprovide services specifically tailored to any particular L2. It merely exposes all available \ninformation about HotShot and the log of blocks which have been sequenced. Any L2 may query \nthis information and interpret the blocks according to its own execution rules in order to \nimplement a prover, executor, RPC service, etc. \n \nInternal components of an Espresso node and their possible usage by various participants in an \nL2 \nThe main internal components of the Espresso node and their respective functions are: \n• HotShot node: The component which actually runs consensus and communicates with \nother nodes. \n• HotShot query service: The query service maintains a database containing the history \nand current state of HotShot, including all committed blocks and QCs, consensus-\nspecific data like view numbers and stake tables, and status information like validator \nuptime. It populates this data using events provided by the HotShot validator and \nprovides a REST API for querying the data. There is also a WebSockets-based API which \nallows clients to subscribe to notifications when new blocks and QCs are produced. The \nquery service does not provide any L2-specific information. The contents of the blocks it \nprovides are the generic transactions that HotShot itself understands. \n• Submit API: The Espresso node also provides an interface allowing clients to submit \ntransactions to the HotShot mempool. It takes as input a transaction serialized into \nbytes and an L2 identifier, and it wraps these into HotShot's generic transaction type. \nLight Client Contract \nThe source code for the light client contract can be found on GitHub. \nThe light client contract is responsible for maintaining state about HotShot consensus on the \nlayer 1 blockchain that Espresso (and L2 rollups) checkpoint to.  \nLightClient validates the HotShot state through cryptographic proofs (SNARK proofs). Rollups \ncan use the new finalizedState from the contract to confirm the validity and finality of \ntransactions that have been bundled and processed. \n\nCopy \nLightClientState public finalizedState; \nLight Client State \nThis LightClientState includes information such as: \n• the latest view number, viewNum, and block height, blockHeight, of the finalized \nHotShot chain \n• the Merkle root of finalized block commitments, blockCommRoot \nCopy \nstruct LightClientState { \n    uint64 viewNum; \n    uint64 blockHeight; \n    BN254.ScalarField blockCommRoot; \n} \nThe most important part of the LightClientState is the blockCommRoot, which is the root of an \nappend-only Merkle tree of all blocks sequenced by Espresso. This root is public, allowing other \ncontracts to use succinct Merkle proofs to verify the inclusion of a certain block commitment at \na certain height and to update the root when new blocks are appended. \nAt any given time, the LightClientState contains Espresso block commitments from height 0 up \nto (but not including) the current block height, blockHeight. All commitments for heights greater \nor equal to blockHeight are not present — the corresponding leaf is empty in the tree. \nThere's another struct, StakeTableState, which is used to store the initial stake table \ncommitments. The stake table is currently fixed, meaning it is initialized once and stored in the \ngenesisStakeTableState variable when the Light Client contract is first deployed. The initial stake \ntable commitments include the BLS and Schnorr verification keys and the amounts the \nvalidators staked. \nCopy \n    struct StakeTableState { \n        uint256 threshold; \n        BN254.ScalarField blsKeyComm; \n        BN254.ScalarField schnorrKeyComm; \n        BN254.ScalarField amountComm; \n    } \nHowever, in future versions, the stake table is will become dynamic. This will allow validators' \nstake commitments and key information to be updated in real-time, allowing the system to \nadjust to changes in stake amounts, the registration of new validators, or the withdrawal of \nexisting ones. \n\nRollups and the Light Client Contract \nRollup contracts on L1 must use the blockCommRoot when validating a state transition to \nensure that the rollup block claimed to have been executed is indeed the next block in the \ncanonical sequence. \nThe proposer of a rollup state transition must provide a proof, relative to blockCommRoot, \nshowing that the Espresso state commitment at a specified height is consistent with the rollup \nblock commitment. \nFor rollups to integrate with Espresso, they need to modify their contract on L1 to prove that \ntheir state is derived from the Espresso state. For instance, consider a scenario where the \nEspresso prover pushes a new Espresso state commitment to the light client contract every 1 \nminute, and the rollup prover submits a new rollup block commitment every 10 minutes to their \nrollup contract. The rollup prover needs to prove that the block commitment it publishes to the \nrollup contract corresponds to all its rollup transactions contained in the Espresso blocks \nduring the 10-minute period. \nEach Espresso block commitment also commits to a list of rollup transactions (among other \nmetadata) which facilitates lightweight proofs with transaction granularity for arbitrarily old \nrollup blocks. This approach allows for the verification of these rollups blocks on Espresso's \nchain and enables the light client contract to operate with a constant amount of storage, \nirrespective of the HotShot chain's length. \nData Availability \nSince the actual block commitments (let alone the full blocks) are not stored on-chain, it is \nimportant to understand the data availability properties ensuring that clients can always \nretrieve an old block, block commitment, or a block's Merkle proof. \nClients can fetch a Merkle proof for any block from an archival query service and authenticate it \nagainst the block root in the light client state. Failing that, they can fetch the individual blocks \nfrom HotShot DA and extract the proof themselves. \nZK Proofs, SNARK Proofs, and ZK Circuits \nA circuit defines the computation to be proven. Zero-knowledge proof (ZKP) systems can \ngenerate cryptographic proofs attesting to the validity of statements described by the circuit \nwithout revealing underlying witness. In our context, we rely on SNARK proofs (a special kind of \nZKP), whose succinct nature is especially valuable for verifying computation in smart contracts \nwhere gas costs are a critical concern. Irrespective of the number of signatures/consensus \nvotes, the size and verification cost of the proof remain constant. \nIn a zero-knowledge protocol there are two main roles: (i) the prover and (ii) the verifier. \nThe prover uses a combination of secret inputs (HotShot nodes' Schnorr signatures), also called \nwitnesses, public inputs (the LightClientState), and a circuit description in order to generate a \nSNARK proof. \nThe verifier uses the public inputs and the SNARK proof to verify that the rules defined by the ZK \ncircuit are satisfied. In this case, the LightClient contract acts as the verifier for this ZK proof via \nthe verifyProof method which is invoked within the newFinalizedState function. \n\nFinally, both the prover and verifier use some public parameters. These public parameters are \nderived from the circuit and a structured reference string (SRS) that requires a trusted setup to \nbe generated but can be reused for other circuits. \nThe Light Client Circuit \nLet the following circuit CqcCqc over prime field FpFp, the corresponding Jubjub curve group \nG=⟨g⟩G=⟨g⟩ whose scalar field is FrFr and base field is exactly FpFp, so that each group element \n∈Fp×Fp∈Fp×Fp. \n• Public input: \no stake table commitment: cmT∈FpcmT∈Fp \n▪ A stake table entry now composes of a triple \n(bls_ver_key,schnorr_ver_key,stake_amount)(bls_ver_key,schnorr_ver_k\ney,stake_amount). BLS verification key is under ark_bn254::Fq, Schnorr \nverification key is under ark_bn254::Fr, and the stake amount is within \nthe range of ark_bn254::Fr. The commitment should be computed in the \nfollowing way: first serialize all BLS keys into elements of ark_bn254::Fr, \nfollows by a list of Schnorr keys and then the stake amount. The \ncommitment is the rescue hash of this list. \no quorum Threshold: T∈FpT∈Fp \no attested new finalized hotshot state: \nm:=(v,h,rootcm,cmledger,cmstake_table)∈Fp5m:=(v,h,rootcm,cmledger\n,cmstake_table)∈Fp5 \n▪ the merkle tree for block commitments can use any hash function (e.g. \nSHA2) and best if there is an injective mapping between the root value to \na FpFp element. \n• Secret witness: \no signers indicator vector: v⃗SvS \no stake table vector (consists of public key, weight pair): \nT=[(pki∈G,wi∈Fp)]i∈[n]T=[(pki∈G,wi∈Fp)]i∈[n] \no list of schnorr signatures: {σi=(si,Ri)∈Fr×G}i∈[n]{σi=(si,Ri)∈Fr×G}i∈[n] \n• Relation: \no the input signers indicator vector v⃗SvS is a bit vector \no correct stake table commitment: cmT=commit(T)cmT=commit(T) \n▪ we use Rescue-based commitment, thus all operations are native \no accumulated weighted sum exceeds threshold: ∑i∈Swi>T∑i∈Swi>T \n▪ assumption: there’s no overflow!! NOTE: outside the circuit, the client \nsoftware that’s in charge of stake table management needs to check the \naccumulated sum does not exceed modulus pp AT ALL TIMES!     \n\no signature verification (on each): Vfy(pki,m,σi)=1Vfy(pki,m,σi)=1 for all \n∀i∈[n]∀i∈[n]. \n▪ c=H(R,m,..)c=H(R,m,..): rescue-based hash to get challenge \n▪ x=gsx=gs: a fixed-base scalar mul \n▪ internally, involves bit-decomposition of ss and elliptic curve \naddition based on the bits. \n▪ y=R+pkcy=R+pkc: a variable-base scalar multiplication + an elliptic \ncurve addition. \n▪ x=?yx=?y: point equality check \nUpdating and Verifying LightClientState \n \nThe LightClientState is updated by any state prover that submits valid updates to the LightClient \ncontract via the newFinalizedState method which sets the latest finalizedState. \nCopy \nfunction newFinalizedState( \n    LightClientState memory newState, \n    IPlonkVerifier.PlonkProof memory proof \n) external { \n  //... \n} \nWe assume an altruistic, honest prover for now and leave the design of prover market to future \nwork. \nFor replicas: \n• upon receiving new QC from the leader, generate a Schnorr signature over the updated \nfinalized HotShot state, and send it over to the CDN and store it locally for a while. \n• the local storage for the Schnorr signatures (for each block) can be a sliding window of a \nfixed size where older signatures got pruned. The window size can be set based on the \nexpected interval for on-chain update plus some buffer accounting for temporarily \nfailing prover. \nFor the altruistic prover: \n• will continuously listen passively for the HotShot state changes, e.g. when a new block \nhas been decided \n• periodically requests the Schnorr signatures from the DA layer by sending a request on a \nspecific view vv. This will be the signature for the new finalized state from the consensus \nnodes \n\no For convenience of signature collection, prover will first fetch from CDN (we \nrefer to as the relay server) for the list of Schnorr signatures, if failed, then ask \neach individual replica (note: not small DA committee, or VID, but each node \nindividually). \n• Once a sufficient amount of valid signatures is collected, some provers can then \ngenerate a SNARK proof which is submitted alongside the new finalized state to the light \nclient contract. \nThe contract has the ability to be in permissioned mode where it only accepts proofs from one \nprover. For the next release, only a permissioned prover, doing the computations, will call this \nfunction. \nReplica nodes update the snapshot of the stake table at the beginning of an epoch and this \nsnapshot is used to define the set of stakers for the next epoch. The light client state must be \nupdated at least once per epoch. \nFor the next release, we are not using epochs so numBlockPerEpoch is set to type(uint32).max \nduring deployment. \nHotShot state authentication via Schnorr signatures \nWhen a set of HotShot nodes reach consensus and the finalized HotShot state has been \nupdated, they each sign a Schnorr signature on this updated HotShot state. These signatures \nassert that the signer agrees with the state of each proposed block. The signatures are stored \nlocally on the DA layer, and to save space, older signatures are pruned using a sliding window \nmechanism. The window size can be set based on the expected interval for on-chain update \nplus some buffer accounting for a temporarily failing prover. \nWhen a prover (an entity that confirms the truth of a claim) retrieves these signatures, a SNARK \nproof is then generated. This proof, is used by the LightClient contract to efficiently verify these \nSchnorr signatures. The state of the sequencer contract can be updated only if a correct SNARK \nproof is provided. This is a critical step that ensures the validity and security of state updates in \nEspresso's consensus protocol⁠. \nThe proof of the Schnorr signatures is sent to the newFinalizedState function of the LightClient \ncontract. \nVerifying the Signatures and Light Client State \nThe LightClient contract also does the work of verifying the proof that is sent by the prover on L1. \nThe verifyProof method accepts the proof and a set of public inputs (the LightClientState) to \ncheck whether the proof correctly verifies the new state being submitted. \nCopy \nfunction verifyProof(LightClientState memory state, IPlonkVerifier.PlonkProof memory proof) \n        internal \n        virtual \n    { \n        IPlonkVerifier.VerifyingKey memory vk = VkLib.getVk(); \n\n        uint256[] memory publicInput = preparePublicInput(state); \n \n        if (!PlonkVerifier.verify(vk, publicInput, proof, bytes(\"\"))) { \n            revert InvalidProof(); \n        } \n    } \nVerifying a SNARK proof requires a constant amount of space and computation, no matter how \nmany HotShot node signatures are involved. This is unlike verifying the signatures directly, \nwhich would require space and computation proportional to the number of signers. \nThe proof itself contains the HotShot state, the stake table info and the list of Schnorr \nsignatures of the HotShot nodes that formed a Quorum and came to consensus on that state. \nThis verifyProof method is executed when the newFinalizedState method is called so that the \nnew state is accepted only if the proof succeeds. \nEscape Hatch Functionality \nRollup contracts keep track of their rollup VM states and depend on our Light Client contract for \nfinalized consensus states. They usually support escape hatches in case of liveness failures on \nL2. Since different rollups impose different predicates when deciding whether L2 is down, our \nLight Client contract provides some helper functions to detect delays in HotShot updates. The \nRollup contracts can then implement their escape hatch logic based on this info. \nTwo such helper functions are: \n• function getHotShotCommitment(uint256 hotShotBlockHeight) \no Returns the root of the HotShot block commitment tree, where each leaf \ncontains the HotShot block commitment at a new height \n• function lagOverEscapeHatchThreshold(uint256 blockNumber, uint256 threshold) \no Returns whether there has been delay between updates \no Checks if the HotShot state updates lag behind the specified threshold based on \nthe provided L1 block number. \no The rollup chooses the threshold based on their liveness criteria. \no HotShot would be considered down if the gap between two consecutive updates \nwhere the provided L1 block number should have been recorded, exceeds the \nspecified threshold \nPeriodically, the light client contract is updated with the latest validated HotShot state. We store \na 10 day sliding window of historical Hotshot state roots, stateHistoryCommitments, so that \noptimistic rollups' dispute handling contracts can access required HotShot commitment data \nduring disputes (which is usually during 7 day windows). \nPublic Write Methods \n\nnewFinalizedState \nThis method updates the latest finalized light client state. It is updated per epoch. An update for \nthe last block for every epoch has to be submitted before any newer state can be accepted \nsince the stake table commitments of that block become the snapshots used for vote \nverifications later on. \nThe contract has the ability to be in permissioned mode where there is only one prover that has \nthe ability to call this function. In the next release, only a permissioned prover doing the \ncomputations will call this function \nCopy \nfunction newFinalizedState(LightClientState memory newState, IPlonkVerifier.PlonkProof \nmemory proof) \n    external; \nParameters \nName \nType \nDescription \nnewState \nLightClientState \nnew light client state \nproof \nIPlonkVerifier.PlonkProof \nPlonk proof \ncomputeStakeTableComm \nGiven the light client state, compute the short commitment of the stake table \nCopy \nfunction computeStakeTableComm(LightClientState memory state) public pure returns \n(bytes32); \nFee Token Contract \nThe source code for the fee token contract can be found on GitHub.  \nThe fee token contract enables builders to deposit ETH which allows them to pay for a data \nprocessing fee associated with HotShot. While the fee token contract facilitates deposits, \nHotShot itself manages and tracks the working balance of builder deposits. Initially, the fee \ntoken contract only supports deposits. Withdrawals are planned to be enabled in a future \nrelease. \nCopy \n\nfunction deposit(address user) public payable { \n    //... \n} \nThe contract defines a minimum and maximum amount for deposits to avoid errors and prevent \nthe fee table from being filled with dust. \nCopy \nuint256 public immutable MAX_DEPOSIT_AMOUNT = 1 ether; \nuint256 public immutable MIN_DEPOSIT_AMOUNT = 0.001 ether; \nIn the Mainnet 0 release, withdrawals are not enabled and thus the MAX_DEPOSIT_AMOUNT \naims to minimize how much ETH is locked by a builder. \nPublic Write Methods \ndeposit \nAllows anyone to deposit an ETH balance for any user \nthe deposit amount is less than a specified threshold to prevent accidental errors \nCopy \nfunction deposit(address user) public payable; \nStake Table \nThe stake table contract maintains the canonical stake table for HotShot and enables nodes to \nstake their tokens to participate in HotShot. This contract will be responsible for: \n• staking \n• unstaking \n• delegating \nHow the Stake Table Contract Works \nThe HotShot contract, which verifies and stores Espresso state updates, also stores the entirety \nof the latest HotShot stake table. By taking this to be not just a mirror of the stake table but the \ncanonical version of it, we gain a number of advantages, including the potential for the L1 itself \nto make changes to the stake table, a requirement for implementing restaking. \n \nRestaking \nRestaking is a critical part of the Espresso roadmap, since it allows Espresso to share a security \nbudget with Ethereum and aids in incentive alignment between the Ethereum operator set and \nrollups integrated with Espresso. But to enable restaking using a system like Eigenlayer, the \nrestaking contracts must be able to change the state of the Espresso stake table, such that new \nentries can be added when new L1 validators opt into restaking for Espresso. \n\n \nBy treating the stake table stored on the L1 as the canonical stake table, we allow L1 smart \ncontracts to do exactly this, merely by writing to another contract on the same L1. Espresso \nnodes will then read the updated stake table back from the L1 at the start of the next epoch and \nbegin using the stake table with the restaker's entry for validating future consensus decisions. \n \nThis does require every HotShot consensus participant to run an L1 light client in order to make \ntrustless reads from the HotShot smart contract. However, Ethereum's proof-of-stake \nconsensus enables very efficient, lightweight, trustless clients, and this Ethereum client can be \nbundled directly into the HotShot client executable for a seamless user experience. \n \nConsensus Sync \nIn order to verify consensus decisions, a HotShot client must know the stake table that was \nused to authenticate each decision, so that it can accurately account for the number of votes \nendorsing each decision. This means that, naively, if a new HotShot client or consensus \nparticipant wanted to sync with the current state of consensus, it would have to replay from \ngenesis at least every block which updated the stake table. By storing verified snapshots of the \nstake table on a trusted L1, new HotShot nodes can instead read the latest snapshot from the \nL1 and start replaying blocks from there, trusting the L1 validator set to have already verified \neach block which led up to the snapshot. This is very similar to how rollups can enable fast, \ntrustless sync for their clients by leveraging L1 state updates. \nSmart Contract Upgradeability \nThe following smart contracts are upgradeable: \n• LightClient \n• FeeContract \nThese contracts use the universally upgradeable proxy pattern (UUPS) to make it possible to \nupgrade functionality in the contract, e.g., adding a new method for a future launch. \nHow it works \nA proxy contract directs calls to the implementation contract, which contains the logic of the \nsystem. \nWhen an upgrade is needed, a new implementation contract is deployed and the proxy \ncontract's storage is updated so that it will now route requests to the new implementation. This \nallows for modifications to be made without affecting the state stored in the contract. Espresso \nusers can continue interacting with the same contract address (the address of the proxy) to \naccess the updated functionalities of the implementation contract. Careful consideration will \nbe made to ensure backward compatibility and data consistency during the upgrade process. \n ",
    "filename": "5. internal function.pdf"
  },
  {
    "id": 18,
    "content": "\n\nRollup Stacks \nDisclaimer: Espresso has launched Mainnet 0, our first production release. While the network \nis now live and being used in production, we are actively developing and upgrading both the \nnetwork and its underlying technology. We welcome rollup developers to integrate with \nEspresso, but please note that this documentation may be updated frequently and integration \nprocesses may evolve as we enhance the system. \nThe Espresso Network is a global confirmation layer that enables faster bridging, decentralized \nand shared sequencing, and low-cost data availability for rollups. As a rollup developer, you can \npermissionlessly integrate your own rollup with Espresso by making certain code changes to \ninterface with Espresso. Espresso is also working with several rollup stacks and rollup-as-a-\nservice options to provide easy deployment options. \nThis section describes the general architecture of an integration between Espresso and a typical \nrollup. It covers integrations for both ZK rollups and optimistic rollups. There may be some \nspecific details which vary between individual rollups. If you'd like more information on a \npossible integration with your specific rollup, you can contact us. As supplementary material, \nyou can also check out the demo integration of a minimal, illustrative ZK rollup. \n \nIntegrating a ZK Rollup \nThis section will develop a model of the architecture of a typical ZK rollup and describe how this \narchitecture can be adapted to use Espresso. It can serve as a guide both for adapting existing \nrollups to replace their current sequencer with Espresso and for designing new rollups intended \nto use Espresso from genesis. For a more in-depth overview please see this GitHub page. \nIf you've already familiarized yourself with the Espresso architecture and you just want a quick \nintegration checklist, you can skip ahead to the summary of changes. \nZK Rollup Architecture \nThis section develops a model of the architecture of a ZK rollup by abstracting away most of the \ninternal complexity, and focusing mainly on the interactions between the sequencer and other \ncomponents. Thus, this model should be sufficient to guide the integration of a wide variety of \nZK rollups with Espresso. \nBackground \nIn this model, a ZK rollup is a distributed state machine in which one or more agents that \ncompute state transitions (the executors or provers) use zero-knowledge proofs to convince \nadditional, non-computing agents (such as the end users) of the validity of state updates. The \nstate transitions are determined by applying the deterministic function which defines the rollup \nto a sequence of transactions, or state transition requests, starting from a known initial state. \nThis sequence is usually represented as a sequence of blocks, where each block consists of an \nordered list of many transactions. \nThe architecture model will aim to isolate the sequencer, the component of the system which \ndetermines this sequence. Modeling the sequencer's interactions with the rest of the system \ncan shed light on how to integrate with Espresso. \nComponents \n\nWe model the rollup as a collection of three components: \n• The sequencer is responsible for batching transactions from various users into ordered \nblocks, and then committing to an order of those blocks. The order can be arbitrary or \nsubject to rollup-specific constraints. The important thing is that the order is public and \nimmutable: all rollup users at all times should agree on the relative ordering of blocks. \n• The executor and prover is an abstract component which may in reality consist of \nseveral components. Its primary job is to execute the rollup's state transition function, \nusually some abstract machine in which transactions represent programs to execute. By \nexecuting this function, the executor computes and stores a VM state which is a \ndeterministic result of the ordering of blocks produced by the sequencer. \nBy virtue of maintaining and storing the state, this component is equipped to perform some \nadditional functions, including \no Running a web server which allows users to interact with the VM state (such as \nan Ethereum JSON-RPC server, if the rollup VM is Ethereum-compatible). \no Running a ZK prover which produces zero-knowledge proofs of the correctness \nof the state stored by the executor. This component proves to other, untrusting \nparticipants that the executor has correctly computed the state transition \nfunction on the sequence of blocks produced by the sequencer. \n• The rollup contract validates ZK proofs produced by the prover and stores recent, \ncertified state roots in the storage of a lower-layer blockchain (like Ethereum). This \nallows clients who neither trust the executor nor want to do the work of validating ZK \nproofs to authenticate the state of the VM, by trusting a majority of the L1 validators to \nhave correctly verified the ZK proof. The state roots stored in the contract are usually \nsuccinct digests of the VM state, like Merkle roots, so that clients can fetch large pieces \nof the state from an untrusted, off-chain source and then validate them against the state \nroot. \nThis is an optional component. Some L2s (so-called sovereign rollups) do not verify their state \nwith any other blockchain, and instead rely on each peer either computing the correct state \nthemselves or validating a ZK proof directly, rather than trusting the L1 validators to check the \nproof. \nThis model is designed to make it easy to understand what changes when the sequencer is \nreplaced by Espresso. However, in a real rollup one or more of these components may be \ncombined. For example, the sequencer and executor may be part of the same service. \nTransaction Flow \nThe figure below shows the interactions between these components as a transaction flows \nthrough the system. The dashed arrows indicate places where more than one design is possible, \nand different ZK rollups may make different choices. \n\n \nGeneralized ZK Rollup architecture \n1. The user submits a transaction. This will often be submitted to a rollup-specific service \nof the user's choice (such as a JSON-RPC server) before being forwarded to the \nsequencer. However, many rollups also allow the user to submit directly to the \nsequencer. In some rollups, the sequencer may even be combined with the rollup \nservice, so that it can execute transactions before sequencing them and filter out invalid \ntransactions. \n2. The sequencer eventually includes the transaction in a block. \n3. The block is sent to the rollup contract, where it is stored. This serves two purposes: \no The contract can act as a source of truth for the order of blocks, so that if the \nsequencer later tries to equivocate, and report a different order to different \nparticipants, all parties can take the order saved in the contract as authoritative. \n\no The layer 1 blockchain hosting the contract provides data availability for the \nrollup. Even if the sequencer goes down, users and executors/provers can read \nthe data for each sequenced block from storage in the layer 1 blockchain, which \nis presumed to be highly available. This means that the sequencer cannot stop \nany participant from reconstructing the state of the layer 2 blockchain. \n4. The executor is notified of the new block. They may get it from the rollup contract, or \ndirectly from the sequencer. In the latter case, they will authenticate the block provided \nby the sequencer against the ordering (or a succinct commitment to the ordering) stored \nin the contract. \n5. The executor executes the block and updates its local copy of the VM state. \n6. The prover produces a zero-knowledge proof that the new state is the result of correctly \napplying the state transition function to the sequenced block. \n7. The prover sends the proof and the new state root to the rollup contract, which verifies \nthe proof and stores the state root. \n8. The user observes the result of their transaction by querying the server provided by the \nexecutor. If the user does not trust the executor, they can check the response against \nthe state root stored in the contract, or they can request a zero-knowledge proof of the \nstate root and verify it themselves. \nUsing Espresso \nThis section describes the recommended architecture for a ZK rollup integrated with Espresso. \nHere we describe the architecture for a ZK rollup using Espresso for confirmations, data \navailability, and sequencing. Rollups may choose to use a separate sequencer (centralized or \ndecentralized) while still using Espresso for confirmations, with a similar system architecture. \nAdditions, deletions, and changes are highlighted in comparison with the typical ZK rollup \narchitecture. \nComponents \n\n \nGeneralized ZK rollup architecture, modified to use Espresso \nEspresso brings a few new components into the picture: \n• HotShot finalizes blocks produced by the Espresso Network in a few seconds. This \nensures that rollup transactions are confirmed quickly and in a decentralized and \nsecure fashion, owing to the many nodes participating in HotShot. \n• Tiramisu provides data availability for all rollup blocks. This availability is guaranteed by \nthe same operator set that guarantees finality, so it is reliable, and rollups have the \noption of using Tiramisu as their standalone data availability layer if they wish. Tiramisu \nis designed to provide higher throughput and lower fees than using Ethereum for data \navailability. \n• The Light client contract runs a HotShot client on the layer 1 blockchain, verifying the \nfinality of each block and storing a binding commitment to the order of blocks decided \n\nby HotShot. As described in the following sections, rollup contracts on the same layer 1 \ncan use this contract to relate their state transitions to the committed block order. \nTransaction Flow \nThe transaction flow with Espresso is very similar to the transaction flow without. The main \ndifference is in how confirmed blocks are disseminated. When not integrated with Espresso, \nrollup nodes are free to choose from where they fetch sequenced blocks: from the rollup \ncontract on layer 1, or directly from the sequencer. Espresso, however, does not send \nsequenced blocks to each rollup contract: doing so would be expensive and would not meet the \nneeds of all rollups. Instead, Espresso only sends sequenced blocks to one place—the \nsequencer contract—and it only sends a commitment to each block, at that: since Espresso \nprovides its own data availability solution, the default is not to store entire blocks on the layer 1. \nWith this new flow, rollup nodes may choose how they get notified of new blocks: either by \nstreaming them directly from Espresso or streaming events from the sequencer contract. But in \neither case, they must retrieve the block contents from the Espresso Network (see Espresso's \ndata availability API). Furthermore, rollups that still wish to use the layer 1 for data availability \nare responsible for sending the necessary data to the layer 1 once it has been sequenced. \nIt is important to note that while the transaction flow changes in terms of how transactions get \nsequenced (if Espresso is used for sequencing) and how rollups consume that sequence, there \nis no change in what a rollup does with a block after it is sequenced. This means that the \nexecution layer, which makes up the bulk of the complexity of many rollups, can be used \ncompletely unmodified with Espresso. \nThe following sections describe how the components of the rollup must be modified to support \nthe updated transaction flow and retain security. \nRollup Proofs in a Shared-Sequencer World \nThe bulk of the changes from the basic ZK rollup architecture relate to the state transition \nproofs, requiring additions to both the prover and the verifier (which may be part of the rollup \ncontract, or may be an off-chain component, in the case of sovereign rollups). The difference \nstems from the statement which needs to be proven. In a typical ZK rollup that only has to deal \nwith a single, centralized sequencer, the prover proves a statement of the form: \nGiven a block B and an initial state root S, the result of applying the state transition function is a \nnew state root S'. \nThe verifier checks both that the proof is correct, or internally consistent, and also that it is \nrelevant: that S is the current state root and B is the next block in the sequence. Only then does \nit accept S' as the new state root. The on-chain verifier can easily do this check because the \nsequencer is sending blocks (or block commitments) directly to the rollup contract, and those \nblocks contain only transactions that are relevant to this particular rollup. Therefore the \ncontract simply reads the last sequenced block directly from its own storage and compares it \nwith B (or compares a commitment to the last block with a commitment to B). The job of an off-\nchain verifier is similarly straightforward; it may also read the last sequenced block from the \ncontract, or it may fetch the block directly from the sequencer along with some kind of \nauthentication. In the case of a trusted, centralized sequencer, this authentication could be a \nsimple signature. \n\nThings are a bit more complicated when the confirmation layer is \n1. decentralized, and \n2. can be shared among multiple rollups \nas is the case in Espresso. Blocks processed by HotShot may contain transactions meant for \nother rollups, which must be filtered out, and the process of checking that a block has been \nconfirmed by HotShot is more complicated than verifying a simple signature. \nHandling Multi-Rollup Blocks \nLet's tackle the first problem first. The statement that must be proven now reads something like \nGiven a block commitment C, the block B contains all of the transactions belonging to rollup R \nin C, and only those transactions, in the same order as they appear in C. Furthermore, given an \ninitial state root S, the result of applying the state transition function to B and S is a new state \nroot S'. \nThe second part of the proof stays the same, which is good: this is where the zero-knowledge \nproof encodes the semantics of the rollup's state transition function, and, since rollup VMs tend \nto be fairly complex (e.g. EVM) this is where most of the proving complexity lies. Thus, the \nchanges required of the state transition proof will mainly be additions, and will not affect the \nsemantics of the state transition function at all. \nThe first part of the statement has changed substantially. Let's break it down: \nGiven a block commitment C \nWe work with a succinct commitment to the block, rather than the whole block, because we \ndon't want to bring transactions from other rollups into the proof. \nthe block B contains all of the transactions belonging to rollup R in C \nSince we do not have the entire block, only a commitment C, the state transition proof must \ncontain a completeness proof which shows that the block being executed, B, does not omit any \ntransactions for this rollup which were sequenced in C. Otherwise, two provers could execute a \ndifferent subset of the relevant transactions, arrive at different state roots, and successfully \nverify proofs of both conflicting states. This attack essentially gives malicious actors a way to \nfork a ZK rollup, so the completeness proof here is essential! \nTo permit an efficient proof of completeness, C is computed according to the Savoiardi VID \nscheme described in Appendix A of The Espresso Sequencing Network. The Espresso SDK, also \nunder development, will include predefined functions for working with these commitments and \ndoing proofs about them, so the complexity will be abstracted away from rollup integrations. \nand only those transactions \nIn addition to completeness, the block commitment scheme also defines a protocol for proving \ninclusion. This prevents a forking attack where a malicious prover executes some transactions \nthat were not in the original block. \nin the same order as they appear in C. \n\nThe last point is subtle. The inclusion and completeness proofs provided by the block \ncommitment scheme enforce an ordering, which prevents a forking attack where a malicious \nprover executes the right transactions in the wrong order. However, some rollups may then \napply a deterministic reordering function to ensure that the transactions are executed in an \norder which respects VM-specific constraints. For example, an EVM-compatible rollup may \ndefine the execution of a block B as first reordering the transactions in B by nonce order, and \nthen executing the result. This reordering function would be encoded in the zero-knowledge \nproof just like other rules of the rollup's execution layer. It may be beneficial to do so, because \nHotShot is agnostic to the semantics of any particular rollup and does not enforce ordering \nconstraints at the consensus level. \nHowever, most rollups can probably consider this step optional. In practice, the sequencers \nelected through Espresso (and sequencers in general) will work with builders that are aware of \nrollup-specific semantics. These builders will seek to create valuable blocks, either by requiring \nusers to pay fees or via arbitrage, and thus they will have an economic incentive to fill their \nblocks with valid transactions. In most cases, then, it will be sufficient for rollups to reject \ntransactions that are \"out of order\", and in the case of EVM rollups, this is already done, as \ntransactions with an incorrect nonce are considered invalid. Rejecting invalid transactions can \nbe somewhat simpler than unconditionally sorting or reordering a whole block. \nChecking That a Block Has Been Finalized \nThe last unsolved problem is how the rollup contract can check that a block commitment C, \nwhich has been proven to correspond to a certain list of rollup transactions, has actually been \nfinalized at a given position in the chain. \nWith a centralized sequencer, it is usually straightforward to confirm whether a given block has \nbeen sequenced at a given position in the chain. The sequencer may have a known public key, \nfor which it can produce a signature on any given block. Or, the sequencer may own the only \nEthereum account authorized to call the function on the rollup contract which stores a new \nblock on the layer 1. \nWith a decentralized system, things are a bit more complicated. Rollups that integrate with \nEspresso have their blocks processed by a decentralized consensus protocol, where \nthousands of nodes act as peers, and no one node has the privilege of unilaterally determining \nthe status of sequenced rollup blocks. A block is considered finalized if this network of peers \nreaches consensus on the decision to include that rollup block at the next available position in \nthe chain. Luckily, the process of reaching consensus produces artifacts which can be \nindependently verified by non-participants of consensus, including smart contracts. These \nartifacts are called quorum certificates, or QCs. \nA quorum certificate shows that a vote took place to include a certain block in a certain view, \nand that consensus nodes controlling a sufficient fraction of the total stake voted yes. The \ncertificate contains an aggregated signature from those that voted. Due to the nature of \ndistributed consensus, it actually requires three (in an upcoming version of HotShot this will be \nreduced to two) consecutive rounds of voting to finalize a block, so a chain of three consecutive \nvalid QCs is definitive evidence that a block (and all previous blocks) has been finalized. Thus, \nany client, such as a rollup state transition verifier, wishing to verify that a rollup block has been \nsequenced must obtain and validate a chain of three consecutive QCs. \n\nLuckily, the work required to verify finality for each block is the same across all rollups using \nEspresso, and can be shared. This is where the sequencer contract comes in. It is a single \ncontract that receives commitments to sequenced blocks along with QCs proving the finality of \nthose commitments, verifies the QCs, and stores a commitment to the finalized order of blocks. \nAnyone can append a commitment to a newly sequenced block to the contract, simply by \nproviding a valid chain of QCs, which can be obtained from any honest consensus node. \nOnce the sequencer contract has done the hard work of checking QCs to verify that a block is \nfinalized, anyone else can check finality simply by checking that the block is included in the \nsequence committed to by the contract. The contract uses a Merkle tree to commit to the \nsequence of finalized blocks, storing the root of this tree, so this check is usually done by Merkle \nproof. \nIn a typical ZK rollup, there are several parties who need to verify that a certain block has been \nsequenced: \n• The executor/prover must verify that a block has been sequenced before executing it. It \ncan do this easily by waiting for the sequencer contract to verify the block and emit an \nevent confirming that the block has been finalized. However, the executor may opt to \nconfirm the block faster than the sequencer contract (thus providing preconfirmations \nto users) by downloading and verifying the QCs itself. There are two options for verifying \nQCs: \no Use the Espresso SDK to run the same QC verification algorithm that HotShot \nconsensus uses \no Participate in consensus as a HotShot node. The HotShot node interface \nexposes a stream of verified blocks that the executor can then consume. \n• The rollup contract can read the sequence commitment directly from the sequencer \ncontract. It then has two choices: \no It can require the prover to pass in a Merkle proof showing that the block \ncommitment C from the state transition proof exists at a certain position in the \nMerkle tree \no It can make the Merkle root a public input to the state transition proof, and the \nprover can prove in zero knowledge that the commitment C exists at a certain \nposition in the Merkle tree. These choices allow a tradeoff between work done by \nthe prover (which may be slow) and work done by the rollup contract (which may \nbe expensive). In either case, the Espresso SDK will provide functionality for \nverifying these Merkle proofs on-chain and for encoding them in a zero-\nknowledge proof. \n• Rollup users may need to verify a state transition proof, especially in sovereign rollups, \nrather than relying on the rollup contract to do so. They can follow a very similar process \nas the executor/prover, either verifying QCs on their own or waiting for the sequencer \ncontract to do so. \nTransaction Format \nFrom HotShot's perspective, a transaction is just an array of bytes with an integer attached to \nidentify the rollup that the transaction belongs to. Therefore, rollups using Espresso can keep \n\ntheir existing transaction format. The only change required is that, if the rollup provides a service \nlike JSON-RPC that accepts transaction submissions, it must be modified to attach the rollup \nidentifier when forwarding the transaction to Espresso. \nThe rollup identifier works much like an EVM chain ID. Each rollup is completely free to choose \nwhatever identifier they want. However, it is strongly recommended to choose an identifier that \nno other rollup is using, because the rollup identifier determines which transactions are \nincluded in the completeness proof when filtering a multi-rollup block. Therefore, if you choose \na rollup identifier which is already in use, your rollup will be forced to execute not only its own \ntransactions but also all of those intended for the rollup with the same ID. \nDownloading Data \nOnce a block has been finalized, various rollup participants will need to download it or a subset \nof it from the Tiramisu data availability layer. We consider three main use cases: \n• A node wants to get notified when a new block is finalized \n• An end user wants a proof that a particular transaction has been included in a block, but \nthey don't want to download the entire block. This is a way of obtaining fast finality, \nbecause once a transaction is included in a finalized block, it is guaranteed that the \nrollup will eventually execute it. (This follows from completeness proofs.) \n• An executor wants to download just the subset of a block pertaining to the relevant \nrollup, with a proof that the server has provided the correct transactions in the correct \norder. \nAll of these use cases can make use of the availability API. Any HotShot node or client can \nprovide this API by plugging in the modular HotShot query service. \nNew Block Notifications \nThe availability API provides several streaming endpoints, which a client can connect to using a \nWebSockets client. These endpoints allow clients to receive information when a new block is \nfinalized or becomes available in the DA layer, without excessive polling. The streaming \nendpoints are: \n• /availability/stream/leaves/:height \nStream blocks as soon as they are finalized, starting from :height (use 0 to start from genesis). \nThe stream yields leaves, which contain metadata about finalized blocks such as the identity of \nthe node that proposed them, the signature from nodes that voted for the block, and so on. This \nis the fastest way to learn of new blocks, but because Tiramisu disseminates data \nasynchronously, the actual contents of the block may or may not be included in this stream. \n• /availability/stream/blocks/:height \nThis endpoint is similar to the leaves stream, but it waits until a block is fully available for \ndownload from Tiramisu before notifying the client. Each entry in the stream is a block with its \nfull contents. \n• /availability/stream/headers/:height \n\nThis endpoint is similar to the blocks stream, in that it will not notify the client of a new block \nuntil the full contents of that block are ready for download. However, it will not send the full \ncontents of the block. It will only send the block header, which contains metadata like the block \nheight and timestamp. This is a good way to wait until a block is finalized, at which time you can \nuse some of the finer-grained endpoints discussed below to download only a subset of the \nblock contents, saving bandwidth. \nIn the following sub-sections, it is assumed that clients of the availability API will use one of \nthese streams to wait for more blocks to be sequenced before querying for the specific data \nthey are interested in. \nSingle-Transaction Finality \nThe typical flow for this use case is \n1. A user builds a transaction using rollup-specific client software. \n2. The user saves the hash of their transaction and then submits it. \n3. The user queries the availability API for proof that a transaction with the same hash has \nbeen included in a block. \n4. The user checks that the resulting block has in fact been finalized. \n5. The user verifies the proof, at which point it is guaranteed that the rollup will eventually \nexecute the transaction. \nThe query for a proof uses the endpoint GET /availability/transaction/hash/:hash, replacing \n:hash with the tagged base 64 encoding of their transaction hash. If the requested transaction \nhas in fact been sequenced, the response is a JSON object with a key proof, containing a proof \nof inclusion in a block, as well as metadata about the block, such as height and block_hash. It \ndoes not include the full block contents, so the bandwidth usage is minimal. \nThe user can check that block_hash has been sequenced as described above: either by \nchecking for the corresponding event from the sequencer contract, or by downloading the \nrelevant QCs and verifying them manually. The QCs can be obtained using the endpoint GET \n/availability/leaf/:height, and the Espresso SDK will include functionality for verifying them. \nOnce the user has confirmed that the block is finalized, the only thing left to do is to verify the \nproof that the transaction of interest was included in that block. This is a namespace KZG \ninclusion proof just like the ones used in the state transition proofs, and the SDK will include \nfunctionality for verifying it. \nRollup's Subset of a Block \nRollup executors must download transactions relevant to their rollup in order to execute, but it \nwould be wasteful to download entire blocks, which may contain many transactions from other \nrollups. However, they do not generally want to trust the availability service to provide the \ncorrect subset of transactions. The desired flow is: \n1. An executor queries for the relevant subset of the next block. \n2. The availability service responds with the desired transactions and a proof of \ncompleteness and inclusion. \n\n3. The executor verifies that the block has been finalized, verifies the \ncompleteness/inclusion proof, and then executes the transactions. \nThe executor's query has the form GET /availability/block/:height/namespace/:rollup-id. On \nsuccess, this returns a JSON object with two keys: \n• block, a commitment to the desired block \n• proof, which includes within it a list of transactions and proves their inclusion and \ncompleteness in block. \nThe executor checks that block has been finalized at :height as described above: either by \nchecking for the corresponding event from the sequencer contract, or by downloading the \nrelevant QCs and verifying them manually. The QCs can be obtained using the endpoint GET \n/availability/leaf/:height, and the Espresso SDK will include functionality for verifying them. \nproof is a KZG namespace proof just like the ones used in the state transition proofs, and the \nSDK will include functionality for verifying it. After verifying the proof, the executor is assured \nthat block :height includes all the returned transactions, in the correct order, and no other \ntransactions with ID :rollup-id. It can then execute the transactions to compute the next rollup \nstate. \nData Availability \nTiramisu is a scalable and secure data availability solution which ensures that all sequenced \nblocks will be available for rollup participants to download and execute. This ensures that any \nparticipant can reconstruct the state of the rollup. \nWhile Tiramisu with ETH restaking can be just as secure as Ethereum DA, using it as the only \nsource of data availability technically makes a rollup into a validium. Some rollups in the \nEthereum ecosystem place a high value on persisting all of their data to Ethereum. Espresso \nsupports both approaches. Any rollup may continue to use Ethereum for DA in addition to \nTiramisu simply by having a rollup node send each block produced via Espresso to a layer 1 \ncontract. If your existing rollup already uses Ethereum DA, this is actually one less change you \nhave to make! \nSummary of Changes \nThis section summarizes the changes from the generalized ZK-rollup architecture that are \nrequired to integrate a ZK rollup with Espresso. Though the changes are presented in terms of an \nabstracted architecture, if you can map this abstraction onto your specific ZK rollup, you can \nderive a very concrete to-do list for integrating your rollup with Espresso. \n1. Modify JSON-RPC or analogous server to forward transactions from users to Espresso. \nChoose a numeric ID for your rollup and attach it to the forwarded transactions. \n2. Modify executor to stream notifications of new blocks from either: \no The streaming availability API \no Events emitted by the sequencer contract \n3. Modify the executor to download full blocks, or rollup-specific subsets of blocks, from \nthe sequencer availability API. \n\n4. Change the interface of the state transition proof, replacing the rollup block or block \ncommitment input with a HotShot block commitment. \n5. Extend the proof to encode a proof of completeness and inclusion of the rollup block \nrelative to the block commitment. For a ZK proof system formalized the usual way, in \nterms of constraints on an arithmetic circuit, this means adding more wires and \nconstraints so that the circuit checks the inclusion/completeness proof, which \nbecomes a witness to the circuit. Once released, the Espresso SDK can help define \nthese additional constraints. \n6. (Optional) Extend the proof to encode a deterministic reordering of the transactions \nprovided by Espresso. \n7. Update the prover to generate the new type of proof. \n8. (If applicable) Update the on-chain proof verifier to check the new kind of proof. This \ngenerally involves replacing the preprocessed representation of the circuit. The contract \nwill also need to accept a Merkle proof showing that the new block commitment input to \nthe proof matches the corresponding block commitment in the Merkle tree maintained \nby the sequencer contract. Alternatively, the verification of this Merkle proof can be \nembedded in the circuit. \n9. (If applicable) Update off-chain proof verifiers (validators or light clients) in the \nanalogous way. \n ",
    "filename": "6. rollup optimistic.pdf"
  },
  {
    "id": 19,
    "content": "\n\nIntegrating an Optimistic Rollup \nThis section will develop a model of the architecture of a typical optimistic rollup and describe \nhow this architecture can be adapted to use Espresso. It can serve as a guide both for adapting \nexisting rollups to integrate with Espresso and for designing new rollups intended to use the \nEspresso from genesis. \nIf you've already familiarized yourself with the Espresso architecture and you just want a quick \nintegration checklist, you can skip ahead to the summary of changes. \nOptimistic Rollup Architecture \nThis section develops a model of the architecture of an optimistic rollup by abstracting away \nmost of the internal complexity, and focusing mainly on the interactions between the sequencer \nand other components. Thus, this model should be sufficient to guide the integration of a wide \nvariety of optimistic rollups with Espresso. \nBackground \nIn this model, an optimistic rollup (OR) is a distributed state machine in which one or more \nproposers propose new state roots - the result of applying transactions to the current state - \nand optional challengers verify the state root and challenge it in case it is incorrect. The state \ntransitions are determined by applying the deterministic function which defines the rollup to a \nsequence of transactions, or state transition requests, starting from a known initial state. This \nsequence is usually represented as a sequence of blocks, where each block consists of an \nordered list of many transactions. \nThe architecture model will aim to isolate the sequencer, the component of the system which \ndetermines this sequence. Modeling the sequencer's interactions with the rest of the system \ncan shed light on how to integrate with Espresso. \nComponents \nWe model the rollup as a collection of four abstract components: \n• The sequencer is responsible for batching transactions from various users into ordered \nblocks, and then committing to an order of those blocks. The order can be arbitrary or \nsubject to rollup-specific constraints. The important thing is that the order is public and \nimmutable: all rollup users at all times should agree on the relative ordering of blocks. \n• The proposer's job is to evolve the VM state by applying the sequenced transactions and \ncommit to the new state by posting the new state root to the rollup contract. The result \nof evolving the state machine is expected to be deterministic: any party evolving the \nsame state through applying the same ordered list of transactions with the transition \nfunction of the rollup must arrive at the same evolved state. \n• The challenger's job is similar to the proposer but instead of posting new state roots to \nthe rollup contract it will challenge the state root in the rollup contract if it computes a \ndifferent VM state than the proposer. The details of how the challenge process is \nimplemented will differ from rollup to rollup. \n• The rollup contract's primary functions are to store the state roots, provide an interface \nfor storing sequenced transactions, and provide a challenge mechanism. In reality this \nfunctionality is usually split up over multiple contracts. \n\nThis model is designed to make it easy to understand what needs to change when an optimistic \nrollup integrates with Espresso. For a fully functional product other services such APIs for \nclients to connect to (e. g. a JsonRPC server for EVM rollups) are required. \nTransaction Flow \nThe figure below shows the interactions between these components as a transaction flows \nthrough the system. The dashed arrows indicate places where more than one design is possible, \nand different optimistic rollups may make different choices. \n \nGeneralized Optimistic Rollup architecture \n1. The user submits a transaction. This will often be submitted to a rollup-specific service \nof the user's choice (such as a JSON-RPC server) before being forwarded to the \nsequencer. However, many rollups also allow the user to submit directly to the \nsequencer. In some rollups, the sequencer may even be combined with the rollup \nservice, so that it can execute transactions before sequencing them and filter out invalid \ntransactions. \n2. The sequencer eventually includes the transaction in a block. \n3. The block is sent to the rollup contract, where it is stored. This serves two purposes: \no The contract can act as a source of truth for the order of blocks, so that if the \nsequencer later tries to equivocate, and report a different order to different \nparticipants, all parties can take the order saved in the contract as authoritative. \no The layer 1 blockchain hosting the contract provides data availability for the \nrollup. Even if the sequencer goes down, users, proposers and challengers can \nread the data for each sequenced block from storage in the layer 1 blockchain, \nwhich is presumed to be highly available. This means that the sequencer cannot \nstop any participant from reconstructing the state of the layer 2 blockchain. \n4. The proposer is notified of the new block. They may get it from the rollup contract, or \ndirectly from the sequencer. In the latter case, they will authenticate the block provided \nby the sequencer against the ordering (or a succinct commitment to the ordering) stored \nin the contract. \n5. The proposer executes the block and updates its local copy of the VM state. \n6. The proposer posts a commitment to the new state to the rollup contract. \n7. The challengers notice a new state being posted to the rollup contract. \n8. The challengers fetch the block and new state root from the rollup contract and use it to \nre-compute the new state root themselves. \n9. If a challenger finds the state root in the rollup contract to be incorrect it initiates a \nchallenge process. If successful it will obtain a reward and the proposer at fault will be \npunished. \n\n10. The user observes the result of their transaction by querying the server provided by the \nrollup API. If the user does not trust this API they can choose to recompute the state \nlocally (akin to what the proposer does), or wait until the challenge period has expired. \nUsing Espresso \nThis section describes the recommended architecture for an optimistic rollup integrated with \nEspresso. \nHere we describe the architecture for an optimistic rollup using Espresso for confirmations, \ndata availability, and sequencing. Rollups may choose to use a separate sequencer (centralized \nor decentralized) while still using Espresso for confirmations, with a similar system architecture. \nAdditions, deletions, and changes are highlighted in comparison with the typical optimistic \nrollup architecture. \nComponents \n \nGeneralized optimistic rollup architecture, modified to use Espresso \nEspresso brings a few new components into the picture: \n• HotShot finalizes blocks produced by the Espresso Network in a few seconds. This \nensures that rollup transactions are confirmed quickly and in a decentralized and \nsecure fashion, owing to the many nodes participating in HotShot. \n• Tiramisu provides data availability for all rollup blocks. This availability is guaranteed by \nthe same operator set that guarantees finality, so it is reliable, and rollups have the \noption of using Tiramisu as their standalone data availability layer if they wish. Tiramisu \nis designed to provide higher throughput and lower fees than using Ethereum for data \navailability. \n• The Light client contract runs a HotShot client on the layer 1 blockchain, verifying the \nfinality of each block and storing a binding commitment to the order of blocks decided \nby HotShot. As described in the following sections, rollup contracts on the same layer 1 \ncan use this contract to relate their state transitions to the committed block order. \nTransaction Flow \nThe transaction flow with Espresso is very similar to the transaction flow without. The main \ndifference is in how confirmed blocks are disseminated. When not integrated with Espresso, \nrollup nodes are free to choose from where they fetch sequenced blocks: from the rollup \ncontract on layer 1, or directly from the sequencer. Espresso, however, does not send \nsequenced blocks to each rollup contract: doing so would be expensive and would not meet the \nneeds of all rollups. Instead, Espresso only sends sequenced blocks to one place—the \nsequencer contract—and it only sends a commitment to each block, at that: since Espresso \nprovides its own data availability solution, the default is not to store entire blocks on the layer 1. \nWith this new flow, rollup nodes may choose how they get notified of new blocks: either by \nstreaming them directly from Espresso or streaming events from the sequencer contract. But in \neither case, they must retrieve the block contents from the Espresso Network (see Espresso's \n\ndata availability API). Furthermore, rollups that still wish to use the layer 1 for data availability \nare responsible for sending the necessary data to the layer 1 once it has been sequenced. \nIt is important to note that while the transaction flow changes in terms of how transactions get \nsequenced (if Espresso is used for sequencing) and how rollups consume that sequence, there \nis no change in what a rollup does with a block after it is sequenced. This means that the \nexecution layer, which makes up the bulk of the complexity of many rollups, can be used \ncompletely unmodified with Espresso. \nThe following sections describe how the components of the rollup must be modified to support \nthe updated transaction flow and retain security. \nFraud Proofs in a Shared-Sequencer World \nThe bulk of the changes from the basic optimistic rollup architecture relate to the fraud proofs, \nrequiring additions to the proposer, the challenger, and the rollup contract's challenge \nmechanism. While the details of challenge mechanisms vary widely between optimistic rollup \ninstantiations, the general concept is universal: a proposer has sent a state root to the contract, \nand a challenger has claimed that the correct state root is something different. The challenger \nmust then provide evidence that the proposed state root is incorrect, and the proposer may be \ngiven a chance to produce evidence defending their proposal. \nThere are two ways a state root can be incorrect: \n1. The proposer executed the wrong transactions. \n2. The proposer executed the right transactions incorrectly. \nTherefore, any challenge mechanism must first check that the proposer and challenger both \nagree on the correct transactions to execute, according to the ordering determined by the \nsequencer. If they do not, then whichever one has executed the right transactions trivially wins \nthe challenge. Once the proposer and challenger have agreed on the sequence of transactions \nbeing executed, the challenger must prove that the proposed state root is not the correct result \nof applying the state transition function to those transactions. \nThe first part, agreeing on the sequence of transactions, is generally a fairly straightforward \ncheck against the committed sequence. The second part, checking the proposer's computation \nof the state root, is where most of the complexity lies, since it involves proofs or interactive \nchallenge games that encompass the low-level semantics of a complex VM. Luckily, this part of \nthe challenge remains completely unchanged when integrating with Espresso, and only the \nmechanism for agreeing on the sequence of transactions needs to change. \nWith a single, centralized sequencer, reaching this agreement may only consist of a simple \nlookup, since the sequencer is sending blocks (or block commitments) directly to the rollup \ncontract, and those blocks contain only transactions that are relevant to this particular rollup. \nTherefore the contract simply reads the last sequenced block directly from its own storage and \ncompares it with the blocks claimed by the proposer and the challenger (or compares \ncommitments, in some rollups). If the proposer's claimed block does not match the expected \nblock, the contract can immediately consider the challenge successful. If the challenger's \nclaimed block does not match, the contract can immediately reject the challenge. \nThings are a bit more complicated when the confirmation layer is \n\n1. decentralized, and \n2. can be shared among multiple rollups \nas is the case in Espresso. Blocks processed by HotShot may contain transactions meant for \nother rollups, which must be filtered out, and the process of checking that a block has been \nconfirmed by HotShot is more complicated than simply reading from the rollup contract. \nHandling Multi-Rollup Blocks \nLet's tackle the first problem first. The rollup contract must be able to check that the lists of \ntransactions executed by the proposer and challenger match a certain block produced by the \nsequencer and confirmed by HotShot. However, these transactions may not be the entirety of \nany block, since HotShot will produce blocks that combine transactions for multiple rollups. \nThus, the contract needs a way to check that the proposed list of transactions contains all \ntransactions from this rollup in a particular block (\"completeness\"), only those transactions \n(\"inclusion\"), and respects the order dictated by the sequencer. It must do this without \nnecessarily having access to the whole HotShot block. \nInstead of the whole block, the contract will work with short, binding commitments to blocks. \nThe proposer and challenger can each provide a commitment C to the block they have executed \nalong with a proof of inclusion and completness relative to C for the list of transactions they \nexecuted. The contract then simply verifies that proof and checks that the commitment C was \nactually confirmed at the required position in the chain (see the next section). \nTo permit an efficient proof of completeness, C is computed according to the Savoiardi VID \nscheme described in Appendix A of The Espresso Sequencing Network. The Espresso SDK, also \nunder development, will include predefined functions for working with these commitments and \ndoing proofs about them, so the complexity will be abstracted away from rollup integrations. \nThere is a subtle point regarding ordering. The inclusion and completeness proofs provided by \nthe block commitment scheme enforce an ordering, which prevents an attack where a \nmalicious proposer executes the right transactions in the wrong order, arriving at the wrong \nstate root which was nonetheless correctly computed, and cannot be challenged. However, \nsome rollups may want to allow the proposer to execute transactions in a different order than \nthe sequencer dictated. Such rollups can have the contract check the ordering proof, and then \napply a deterministic reordering function before proceeding to the execution phase of the \nchallenge protocol, thus ensuring that the transactions are executed in an order which respects \nVM-specific constraints. It may be beneficial to do so, because HotShot is agnostic to the \nsemantics of any particular rollup and does not enforce ordering constraints at the consensus \nlevel. \nHowever, most rollups can probably consider this step optional. In practice, the sequencers \nelected through Espresso (and sequencers in general) will work with builders that are aware of \nrollup-specific semantics. These builders will seek to create valuable blocks, either by requiring \nusers to pay fees or via arbitrage, and thus they will have an economic incentive to fill their \nblocks with valid transactions. In most cases, then, it will be sufficient for rollups to reject \ntransactions that are \"out of order\" (by executing them as no-ops), which can be somewhat \nsimpler than unconditionally sorting or reordering a whole block. \nChecking That a Block Has Been Finalized \n\nThe last unsolved problem is how the rollup contract can check that a block commitment C, \nwhich has been proven to correspond to a certain list of rollup transactions, has actually been \nfinalized at a given position in the chain. \nWith a centralized sequencer, this check is usually straightforward. The sequencer may have a \nknown public key, for which it can produce a signature on any given block. Or, the sequencer \nmay own the only Ethereum account authorized to call the function on the rollup contract which \nstores a new block on the layer 1. In most cases this smart contract is used as the single source \nof truth, even if multiple parties are allowed to sequence transactions via escape hatches. \nWith a decentralized system, things are a bit more complicated. Rollups that integrate with \nEspresso have their blocks processed by a decentralized consensus protocol, where \nthousands of nodes act as peers, and no one node has the privilege of unilaterally determining \nthe status of sequenced rollup blocks. A block is considered finalized if this network of peers \nreaches consensus on the decision to include that rollup block at the next available position in \nthe chain. Luckily, the process of reaching consensus produces artifacts which can be \nindependently verified by non-participants of consensus, including smart contracts. These \nartifacts are called quorum certificates, or QCs. \nA quorum certificate shows that a vote took place to include a certain block in a certain view, \nand that consensus nodes controlling a sufficient fraction of the total stake voted yes. The \ncertificate contains an aggregated signature from those that voted. Due to the nature of \ndistributed consensus, it actually requires three (in an upcoming version of HotShot this will be \nreduced to two) consecutive rounds of voting to finalize a block, so a chain of three consecutive \nvalid QCs is definitive evidence that a block (and all previous blocks) has been finalized. Thus, \nany client, such as a rollup state transition verifier, wishing to verify that a rollup block has been \nsequenced must obtain and validate a chain of three consecutive QCs. \nLuckily, the work required to verify finality for each block is the same across all rollups using \nEspresso, and can be shared. This is where the sequencer contract comes in. It is a single \ncontract that receives commitments to sequenced blocks along with QCs proving the finality of \nthose commitments, verifies the QCs, and stores a commitment to the finalized order of blocks. \nAnyone can append a commitment to a newly sequenced block to the contract, simply by \nproviding a valid chain of QCs, which can be obtained from any honest consensus node. \nOnce the sequencer contract has done the hard work of checking QCs to verify that a block is \nfinalized, anyone else can check finality simply by checking that the block is included in the \nsequence committed to by the contract. The contract uses a Merkle tree to commit to the \nsequence of finalized blocks, storing the root of this tree, so this check is usually done by Merkle \nproof. \nIn a typical optimistic rollup, there are several parties who need to verify that a certain block has \nbeen sequenced: \n• The proposer and challenger must verify that a block has been sequenced before using \nit to compute a new state root. They can do this easily by waiting for the sequencer \ncontract to verify the block and emit an event confirming that the block has been \nfinalized. However, they may opt to confirm the block faster than the sequencer contract \n(thus providing preconfirmations to users) by downloading and verifying the QCs \nthemselves. There are two options for verifying QCs: \n\no Use the Espresso SDK to run the same QC verification algorithm that HotShot \nconsensus uses. \no Participate in consensus as a HotShot node. The HotShot node interface \nexposes a stream of verified blocks that the proposer and challenger can then \nconsume. \n• The rollup contract can read the sequence commitment directly from the sequencer \ncontract. It can then require as part of the challenge protocol that the proposer and \nchallenger provide Merkle proofs showing that the block commitment C from their \ninclusion/completeness proofs exists at a certain position in the Merkle tree. \nTransaction Format \nFrom HotShot's perspective, a transaction is just an array of bytes with an integer attached to \nidentify the rollup that the transaction belongs to. Therefore, rollups using Espresso can keep \ntheir existing transaction format. The only change required is that, if the rollup provides a service \nlike JSON-RPC that accepts transaction submissions, it must be modified to attach the rollup \nidentifier when forwarding the transaction to Espresso. \nThe rollup identifier works much like an EVM chain ID. Each rollup is completely free to choose \nwhatever identifier they want. However, it is strongly recommended to choose an identifier that \nno other rollup is using, because the rollup identifier determines which transactions are \nincluded in the completeness proof when filtering a multi-rollup block. Therefore, if you choose \na rollup identifier which is already in use, your rollup will be forced to execute not only its own \ntransactions but also all of those intended for the rollup with the same ID. \nDownloading Data \nOnce a block has been finalized, various rollup participants will need to download it or a subset \nof it from the Tiramisu data availability layer. We consider three main use cases: \n• A node wants to get notified when a new block is finalized \n• An end user wants a proof that a particular transaction has been included in a block, but \nthey don't want to download the entire block. This is a way of obtaining fast finality, \nbecause once a transaction is included in a finalized block, it is guaranteed that the \nrollup will eventually execute it. (This follows from completeness proofs.) \n• A proposer or challenger wants to download just the subset of a block pertaining to the \nrelevant rollup, with a proof that the server has provided the correct transactions in the \ncorrect order. \nAll of these use cases can make use of the availability API. Any HotShot node or client can \nprovide this API by plugging in the modular HotShot query service. \nNew Block Notifications \nThe availability API provides several streaming endpoints, which a client can connect to using a \nWebSockets client. These endpoints allow clients to receive information when a new block is \nfinalized or becomes available in the DA layer, without excessive polling. The streaming \nendpoints are: \n• /availability/stream/leaves/:height \n\nStream blocks as soon as they are finalized, starting from :height (use 0 to start from genesis). \nThe stream yields leaves, which contain metadata about finalized blocks such as the identity of \nthe node that proposed them, the signature from nodes that voted for the block, and so on. This \nis the fastest way to learn of new blocks, but because Tiramisu disseminates data \nasynchronously, the actual contents of the block may or may not be included in this stream. \n• /availability/stream/blocks/:height \nThis endpoint is similar to the leaves stream, but it waits until a block is fully available for \ndownload from Tiramisu before notifying the client. Each entry in the stream is a block with its \nfull contents. \n• /availability/stream/headers/:height \nThis endpoint is similar to the blocks stream, in that it will not notify the client of a new block \nuntil the full contents of that block are ready for download. However, it will not send the full \ncontents of the block. It will only send the block header, which contains metadata like the block \nheight and timestamp. This is a good way to wait until a block is finalized, at which time you can \nuse some of the finer-grained endpoints discussed below to download only a subset of the \nblock contents, saving bandwidth. \nIn the following sub-sections, it is assumed that clients of the availability API will use one of \nthese streams to wait for more blocks to be sequenced before querying for the specific data \nthey are interested in. \nSingle-Transaction Finality \nThe typical flow for this use case is \n1. A user builds a transaction using rollup-specific client software. \n2. The user saves the hash of their transaction and then submits it. \n3. The user queries the availability API for proof that a transaction with the same hash has \nbeen included in a block. \n4. The user checks that the resulting block has in fact been finalized. \n5. The user verifies the proof, at which point it is guaranteed that the rollup will eventually \nexecute the transaction. \nThe query for a proof uses the endpoint GET /availability/transaction/hash/:hash, replacing \n:hash with the tagged base 64 encoding of their transaction hash. If the requested transaction \nhas in fact been sequenced, the response is a JSON object with a key proof, containing a proof \nof inclusion in a block, as well as metadata about the block, such as height and block_hash. It \ndoes not include the full block contents, so the bandwidth usage is minimal. \nThe user can check that block_hash has been sequenced as described above: either by \nchecking for the corresponding event from the sequencer contract, or by downloading the \nrelevant QCs and verifying them manually. The QCs can be obtained using the endpoint GET \n/availability/leaf/:height, and the Espresso SDK will include functionality for verifying them. \nOnce the user has confirmed that the block is finalized, the only thing left to do is to verify the \nproof that the transaction of interest was included in that block. This is a namespace KZG \n\ninclusion proof just like the ones used in the state transition proofs, and the SDK will include \nfunctionality for verifying it. \nRollup's Subset of a Block \nRollup proposers and challengers must download transactions relevant to their rollup in order \nto compute new state roots, but it would be wasteful to download entire blocks, which may \ncontain many transactions from other rollups. However, they do not generally want to trust the \navailability service to provide the correct subset of transactions. The desired flow is: \n1. A proposer or challenger queries for the relevant subset of the next block. \n2. The availability service responds with the desired transactions and a proof of \ncompleteness and inclusion. \n3. The node verifies that the block has been finalized, verifies the completeness/inclusion \nproof, and then executes the transactions. \nThe node's query has the form GET /availability/block/:height/namespace/:rollup-id. On \nsuccess, this returns a JSON object with two keys: \n• block, a commitment to the desired block \n• proof, which includes within it a list of transactions and proves their inclusion and \ncompleteness in block. \nThe node checks that block has been finalized at :height as described above: either by checking \nfor the corresponding event from the sequencer contract, or by downloading the relevant QCs \nand verifying them manually. The QCs can be obtained using the endpoint GET \n/availability/leaf/:height, and the Espresso SDK will include functionality for verifying them. \nproof is a KZG namespace proof just like the ones used in the state transition proofs, and the \nSDK will include functionality for verifying it. After verifying the proof, the node is assured that \nblock :height includes all the returned transactions, in the correct order, and no other \ntransactions with ID :rollup-id. It can then execute the transactions to compute the next rollup \nstate. \nData Availability \nTiramisu is a scalable and secure data availability solution which ensures that all sequenced \nblocks will be available for rollup participants to download and execute. This ensures that any \nparticipant can reconstruct the state of the rollup. \nWhile Tiramisu with ETH restaking can be just as secure as Ethereum DA, using it as the only \nsource of data availability technically makes a rollup into a validium. Some rollups in the \nEthereum ecosystem place a high value on persisting all of their data to Ethereum. Espresso \nsupports both approaches. Any rollup may continue to use Ethereum for DA in addition to \nTiramisu simply by having a rollup node send each block produced via Espresso to a layer 1 \ncontract. If your existing rollup already uses Ethereum DA, this is actually one less change you \nhave to make! \nUsing Espresso \n\nThis section describes the recommended architecture for an optimistic rollup integrated with \nEspresso. \nHere we describe the architecture for an optimistic rollup using Espresso for confirmations, \ndata availability, and sequencing. Rollups may choose to use a separate sequencer (centralized \nor decentralized) while still using Espresso for confirmations, with a similar system architecture. \nAdditions, deletions, and changes are highlighted in comparison with the typical optimistic \nrollup architecture. \nComponents \n \nGeneralized optimistic rollup architecture, modified to use Espresso \nEspresso brings a few new components into the picture: \n• HotShot finalizes blocks produced by the Espresso Network in a few seconds. This \nensures that rollup transactions are confirmed quickly and in a decentralized and \nsecure fashion, owing to the many nodes participating in HotShot. \n\n• Tiramisu provides data availability for all rollup blocks. This availability is guaranteed by \nthe same operator set that guarantees finality, so it is reliable, and rollups have the \noption of using Tiramisu as their standalone data availability layer if they wish. Tiramisu \nis designed to provide higher throughput and lower fees than using Ethereum for data \navailability. \n• The Light client contract runs a HotShot client on the layer 1 blockchain, verifying the \nfinality of each block and storing a binding commitment to the order of blocks decided \nby HotShot. As described in the following sections, rollup contracts on the same layer 1 \ncan use this contract to relate their state transitions to the committed block order. \nTransaction Flow \nThe transaction flow with Espresso is very similar to the transaction flow without. The main \ndifference is in how confirmed blocks are disseminated. When not integrated with Espresso, \nrollup nodes are free to choose from where they fetch sequenced blocks: from the rollup \ncontract on layer 1, or directly from the sequencer. Espresso, however, does not send \nsequenced blocks to each rollup contract: doing so would be expensive and would not meet the \nneeds of all rollups. Instead, Espresso only sends sequenced blocks to one place—the \nsequencer contract—and it only sends a commitment to each block, at that: since Espresso \nprovides its own data availability solution, the default is not to store entire blocks on the layer 1. \nWith this new flow, rollup nodes may choose how they get notified of new blocks: either by \nstreaming them directly from Espresso or streaming events from the sequencer contract. But in \neither case, they must retrieve the block contents from the Espresso Network (see Espresso's \ndata availability API). Furthermore, rollups that still wish to use the layer 1 for data availability \nare responsible for sending the necessary data to the layer 1 once it has been sequenced. \nIt is important to note that while the transaction flow changes in terms of how transactions get \nsequenced (if Espresso is used for sequencing) and how rollups consume that sequence, there \nis no change in what a rollup does with a block after it is sequenced. This means that the \nexecution layer, which makes up the bulk of the complexity of many rollups, can be used \ncompletely unmodified with Espresso. \nThe following sections describe how the components of the rollup must be modified to support \nthe updated transaction flow and retain security. \nFraud Proofs in a Shared-Sequencer World \nThe bulk of the changes from the basic optimistic rollup architecture relate to the fraud proofs, \nrequiring additions to the proposer, the challenger, and the rollup contract's challenge \nmechanism. While the details of challenge mechanisms vary widely between optimistic rollup \ninstantiations, the general concept is universal: a proposer has sent a state root to the contract, \nand a challenger has claimed that the correct state root is something different. The challenger \nmust then provide evidence that the proposed state root is incorrect, and the proposer may be \ngiven a chance to produce evidence defending their proposal. \nThere are two ways a state root can be incorrect: \n1. The proposer executed the wrong transactions. \n2. The proposer executed the right transactions incorrectly. \n\nTherefore, any challenge mechanism must first check that the proposer and challenger both \nagree on the correct transactions to execute, according to the ordering determined by the \nsequencer. If they do not, then whichever one has executed the right transactions trivially wins \nthe challenge. Once the proposer and challenger have agreed on the sequence of transactions \nbeing executed, the challenger must prove that the proposed state root is not the correct result \nof applying the state transition function to those transactions. \nThe first part, agreeing on the sequence of transactions, is generally a fairly straightforward \ncheck against the committed sequence. The second part, checking the proposer's computation \nof the state root, is where most of the complexity lies, since it involves proofs or interactive \nchallenge games that encompass the low-level semantics of a complex VM. Luckily, this part of \nthe challenge remains completely unchanged when integrating with Espresso, and only the \nmechanism for agreeing on the sequence of transactions needs to change. \nWith a single, centralized sequencer, reaching this agreement may only consist of a simple \nlookup, since the sequencer is sending blocks (or block commitments) directly to the rollup \ncontract, and those blocks contain only transactions that are relevant to this particular rollup. \nTherefore the contract simply reads the last sequenced block directly from its own storage and \ncompares it with the blocks claimed by the proposer and the challenger (or compares \ncommitments, in some rollups). If the proposer's claimed block does not match the expected \nblock, the contract can immediately consider the challenge successful. If the challenger's \nclaimed block does not match, the contract can immediately reject the challenge. \nThings are a bit more complicated when the confirmation layer is \n1. decentralized, and \n2. can be shared among multiple rollups \nas is the case in Espresso. Blocks processed by HotShot may contain transactions meant for \nother rollups, which must be filtered out, and the process of checking that a block has been \nconfirmed by HotShot is more complicated than simply reading from the rollup contract. \nHandling Multi-Rollup Blocks \nLet's tackle the first problem first. The rollup contract must be able to check that the lists of \ntransactions executed by the proposer and challenger match a certain block produced by the \nsequencer and confirmed by HotShot. However, these transactions may not be the entirety of \nany block, since HotShot will produce blocks that combine transactions for multiple rollups. \nThus, the contract needs a way to check that the proposed list of transactions contains all \ntransactions from this rollup in a particular block (\"completeness\"), only those transactions \n(\"inclusion\"), and respects the order dictated by the sequencer. It must do this without \nnecessarily having access to the whole HotShot block. \nInstead of the whole block, the contract will work with short, binding commitments to blocks. \nThe proposer and challenger can each provide a commitment C to the block they have executed \nalong with a proof of inclusion and completness relative to C for the list of transactions they \nexecuted. The contract then simply verifies that proof and checks that the commitment C was \nactually confirmed at the required position in the chain (see the next section). \nTo permit an efficient proof of completeness, C is computed according to the Savoiardi VID \nscheme described in Appendix A of The Espresso Sequencing Network. The Espresso SDK, also \n\nunder development, will include predefined functions for working with these commitments and \ndoing proofs about them, so the complexity will be abstracted away from rollup integrations. \nThere is a subtle point regarding ordering. The inclusion and completeness proofs provided by \nthe block commitment scheme enforce an ordering, which prevents an attack where a \nmalicious proposer executes the right transactions in the wrong order, arriving at the wrong \nstate root which was nonetheless correctly computed, and cannot be challenged. However, \nsome rollups may want to allow the proposer to execute transactions in a different order than \nthe sequencer dictated. Such rollups can have the contract check the ordering proof, and then \napply a deterministic reordering function before proceeding to the execution phase of the \nchallenge protocol, thus ensuring that the transactions are executed in an order which respects \nVM-specific constraints. It may be beneficial to do so, because HotShot is agnostic to the \nsemantics of any particular rollup and does not enforce ordering constraints at the consensus \nlevel. \nHowever, most rollups can probably consider this step optional. In practice, the sequencers \nelected through Espresso (and sequencers in general) will work with builders that are aware of \nrollup-specific semantics. These builders will seek to create valuable blocks, either by requiring \nusers to pay fees or via arbitrage, and thus they will have an economic incentive to fill their \nblocks with valid transactions. In most cases, then, it will be sufficient for rollups to reject \ntransactions that are \"out of order\" (by executing them as no-ops), which can be somewhat \nsimpler than unconditionally sorting or reordering a whole block. \nChecking That a Block Has Been Finalized \nThe last unsolved problem is how the rollup contract can check that a block commitment C, \nwhich has been proven to correspond to a certain list of rollup transactions, has actually been \nfinalized at a given position in the chain. \nWith a centralized sequencer, this check is usually straightforward. The sequencer may have a \nknown public key, for which it can produce a signature on any given block. Or, the sequencer \nmay own the only Ethereum account authorized to call the function on the rollup contract which \nstores a new block on the layer 1. In most cases this smart contract is used as the single source \nof truth, even if multiple parties are allowed to sequence transactions via escape hatches. \nWith a decentralized system, things are a bit more complicated. Rollups that integrate with \nEspresso have their blocks processed by a decentralized consensus protocol, where \nthousands of nodes act as peers, and no one node has the privilege of unilaterally determining \nthe status of sequenced rollup blocks. A block is considered finalized if this network of peers \nreaches consensus on the decision to include that rollup block at the next available position in \nthe chain. Luckily, the process of reaching consensus produces artifacts which can be \nindependently verified by non-participants of consensus, including smart contracts. These \nartifacts are called quorum certificates, or QCs. \nA quorum certificate shows that a vote took place to include a certain block in a certain view, \nand that consensus nodes controlling a sufficient fraction of the total stake voted yes. The \ncertificate contains an aggregated signature from those that voted. Due to the nature of \ndistributed consensus, it actually requires three (in an upcoming version of HotShot this will be \nreduced to two) consecutive rounds of voting to finalize a block, so a chain of three consecutive \nvalid QCs is definitive evidence that a block (and all previous blocks) has been finalized. Thus, \n\nany client, such as a rollup state transition verifier, wishing to verify that a rollup block has been \nsequenced must obtain and validate a chain of three consecutive QCs. \nLuckily, the work required to verify finality for each block is the same across all rollups using \nEspresso, and can be shared. This is where the sequencer contract comes in. It is a single \ncontract that receives commitments to sequenced blocks along with QCs proving the finality of \nthose commitments, verifies the QCs, and stores a commitment to the finalized order of blocks. \nAnyone can append a commitment to a newly sequenced block to the contract, simply by \nproviding a valid chain of QCs, which can be obtained from any honest consensus node. \nOnce the sequencer contract has done the hard work of checking QCs to verify that a block is \nfinalized, anyone else can check finality simply by checking that the block is included in the \nsequence committed to by the contract. The contract uses a Merkle tree to commit to the \nsequence of finalized blocks, storing the root of this tree, so this check is usually done by Merkle \nproof. \nIn a typical optimistic rollup, there are several parties who need to verify that a certain block has \nbeen sequenced: \n• The proposer and challenger must verify that a block has been sequenced before using \nit to compute a new state root. They can do this easily by waiting for the sequencer \ncontract to verify the block and emit an event confirming that the block has been \nfinalized. However, they may opt to confirm the block faster than the sequencer contract \n(thus providing preconfirmations to users) by downloading and verifying the QCs \nthemselves. There are two options for verifying QCs: \no Use the Espresso SDK to run the same QC verification algorithm that HotShot \nconsensus uses. \no Participate in consensus as a HotShot node. The HotShot node interface \nexposes a stream of verified blocks that the proposer and challenger can then \nconsume. \n• The rollup contract can read the sequence commitment directly from the sequencer \ncontract. It can then require as part of the challenge protocol that the proposer and \nchallenger provide Merkle proofs showing that the block commitment C from their \ninclusion/completeness proofs exists at a certain position in the Merkle tree. \nTransaction Format \nFrom HotShot's perspective, a transaction is just an array of bytes with an integer attached to \nidentify the rollup that the transaction belongs to. Therefore, rollups using Espresso can keep \ntheir existing transaction format. The only change required is that, if the rollup provides a service \nlike JSON-RPC that accepts transaction submissions, it must be modified to attach the rollup \nidentifier when forwarding the transaction to Espresso. \nThe rollup identifier works much like an EVM chain ID. Each rollup is completely free to choose \nwhatever identifier they want. However, it is strongly recommended to choose an identifier that \nno other rollup is using, because the rollup identifier determines which transactions are \nincluded in the completeness proof when filtering a multi-rollup block. Therefore, if you choose \na rollup identifier which is already in use, your rollup will be forced to execute not only its own \ntransactions but also all of those intended for the rollup with the same ID. \n\nDownloading Data \nOnce a block has been finalized, various rollup participants will need to download it or a subset \nof it from the Tiramisu data availability layer. We consider three main use cases: \n• A node wants to get notified when a new block is finalized \n• An end user wants a proof that a particular transaction has been included in a block, but \nthey don't want to download the entire block. This is a way of obtaining fast finality, \nbecause once a transaction is included in a finalized block, it is guaranteed that the \nrollup will eventually execute it. (This follows from completeness proofs.) \n• A proposer or challenger wants to download just the subset of a block pertaining to the \nrelevant rollup, with a proof that the server has provided the correct transactions in the \ncorrect order. \nAll of these use cases can make use of the availability API. Any HotShot node or client can \nprovide this API by plugging in the modular HotShot query service. \nNew Block Notifications \nThe availability API provides several streaming endpoints, which a client can connect to using a \nWebSockets client. These endpoints allow clients to receive information when a new block is \nfinalized or becomes available in the DA layer, without excessive polling. The streaming \nendpoints are: \n• /availability/stream/leaves/:height \nStream blocks as soon as they are finalized, starting from :height (use 0 to start from genesis). \nThe stream yields leaves, which contain metadata about finalized blocks such as the identity of \nthe node that proposed them, the signature from nodes that voted for the block, and so on. This \nis the fastest way to learn of new blocks, but because Tiramisu disseminates data \nasynchronously, the actual contents of the block may or may not be included in this stream. \n• /availability/stream/blocks/:height \nThis endpoint is similar to the leaves stream, but it waits until a block is fully available for \ndownload from Tiramisu before notifying the client. Each entry in the stream is a block with its \nfull contents. \n• /availability/stream/headers/:height \nThis endpoint is similar to the blocks stream, in that it will not notify the client of a new block \nuntil the full contents of that block are ready for download. However, it will not send the full \ncontents of the block. It will only send the block header, which contains metadata like the block \nheight and timestamp. This is a good way to wait until a block is finalized, at which time you can \nuse some of the finer-grained endpoints discussed below to download only a subset of the \nblock contents, saving bandwidth. \nIn the following sub-sections, it is assumed that clients of the availability API will use one of \nthese streams to wait for more blocks to be sequenced before querying for the specific data \nthey are interested in. \nSingle-Transaction Finality \n\nThe typical flow for this use case is \n1. A user builds a transaction using rollup-specific client software. \n2. The user saves the hash of their transaction and then submits it. \n3. The user queries the availability API for proof that a transaction with the same hash has \nbeen included in a block. \n4. The user checks that the resulting block has in fact been finalized. \n5. The user verifies the proof, at which point it is guaranteed that the rollup will eventually \nexecute the transaction. \nThe query for a proof uses the endpoint GET /availability/transaction/hash/:hash, replacing \n:hash with the tagged base 64 encoding of their transaction hash. If the requested transaction \nhas in fact been sequenced, the response is a JSON object with a key proof, containing a proof \nof inclusion in a block, as well as metadata about the block, such as height and block_hash. It \ndoes not include the full block contents, so the bandwidth usage is minimal. \nThe user can check that block_hash has been sequenced as described above: either by \nchecking for the corresponding event from the sequencer contract, or by downloading the \nrelevant QCs and verifying them manually. The QCs can be obtained using the endpoint GET \n/availability/leaf/:height, and the Espresso SDK will include functionality for verifying them. \nOnce the user has confirmed that the block is finalized, the only thing left to do is to verify the \nproof that the transaction of interest was included in that block. This is a namespace KZG \ninclusion proof just like the ones used in the state transition proofs, and the SDK will include \nfunctionality for verifying it. \nRollup's Subset of a Block \nRollup proposers and challengers must download transactions relevant to their rollup in order \nto compute new state roots, but it would be wasteful to download entire blocks, which may \ncontain many transactions from other rollups. However, they do not generally want to trust the \navailability service to provide the correct subset of transactions. The desired flow is: \n1. A proposer or challenger queries for the relevant subset of the next block. \n2. The availability service responds with the desired transactions and a proof of \ncompleteness and inclusion. \n3. The node verifies that the block has been finalized, verifies the completeness/inclusion \nproof, and then executes the transactions. \nThe node's query has the form GET /availability/block/:height/namespace/:rollup-id. On \nsuccess, this returns a JSON object with two keys: \n• block, a commitment to the desired block \n• proof, which includes within it a list of transactions and proves their inclusion and \ncompleteness in block. \nThe node checks that block has been finalized at :height as described above: either by checking \nfor the corresponding event from the sequencer contract, or by downloading the relevant QCs \n\nand verifying them manually. The QCs can be obtained using the endpoint GET \n/availability/leaf/:height, and the Espresso SDK will include functionality for verifying them. \nproof is a KZG namespace proof just like the ones used in the state transition proofs, and the \nSDK will include functionality for verifying it. After verifying the proof, the node is assured that \nblock :height includes all the returned transactions, in the correct order, and no other \ntransactions with ID :rollup-id. It can then execute the transactions to compute the next rollup \nstate. \nData Availability \nTiramisu is a scalable and secure data availability solution which ensures that all sequenced \nblocks will be available for rollup participants to download and execute. This ensures that any \nparticipant can reconstruct the state of the rollup. \nWhile Tiramisu with ETH restaking can be just as secure as Ethereum DA, using it as the only \nsource of data availability technically makes a rollup into a validium. Some rollups in the \nEthereum ecosystem place a high value on persisting all of their data to Ethereum. Espresso \nsupports both approaches. Any rollup may continue to use Ethereum for DA in addition to \nTiramisu simply by having a rollup node send each block produced via Espresso to a layer 1 \ncontract. If your existing rollup already uses Ethereum DA, this is actually one less change you \nhave to make! \n ",
    "filename": "7. integrating optimistic.pdf"
  },
  {
    "id": 20,
    "content": "\n\nUsing the Espresso Network \nGuides for applications, such as rollups and bridges, to use the Global Confirmation Layer. \nBackground \nThe Espresso Network is a shared source of truth that provides strong confirmations on \ntransaction ordering and data across chains. The confirmations provided by HotShot are \nadditive, meaning that rollups can keep giving their users pre-confirmations using their own \nexisting sequencer. It’s up to the end user whether to trust the centralized sequencer or wait a \nfew seconds more for the stronger confirmation provided by Espresso. For more details see the \nsystem overview docs. . \nUsing Espresso confirmations \nUsing the Espresso Network primarily relies on three ingredients: \n• Data is sent to Espresso \n• Data is fetched from Espresso after confirmation \n• HotShot consensus is verified in the chain's state transition function (STF) \nWe recommend running an Espresso node and using its query service. This is the easiest way to \nensure that all data received is validated. For convenience you can also connect to the API \nservices of Espresso’s nodes and use the espresso-dev-node Docker image for local testing. \nSending data to Espresso \nEspresso does not interpret the data submitted to the network. As a result, chains are free to \nencode their data however they wish when sending it to Espresso. Chains are expected to \nchoose an integer namespace ID akin to the EVM chain ID. Note that anyone can submit \ntransactions with any namespace ID. If transactions from third parties are undesired those must \nbe filtered out. \nAn Espresso transaction consists of a namespace ID and payload bytes.  \nThe Espresso transactions can be sent to the submit endpoint of sequencer nodes, or to a \nbuilder. Builders may support private mempools if gossiping of transactions through the public \nmempool is not desired.  \nFetching data from Espresso \nThe blocks finalized by Espresso are divided by namespace. Transactions with a certain \nnamespace ID will be found in the corresponding namespace in the block. For a basic \nintegration, the rollup will only consider the Espresso transactions in its own namespace. \nReceiving notification about finalized Espresso blocks \nRollups can receive notifications from the Espresso query service API or the sequencer contract \non the L1. The interval at which the finalized state is proved in the sequencer contract (hours) is \norders of magnitudes slower than the Espresso block time (seconds). To acquire confirmations, \nnotifications about newly finalized blocks must be obtained via the Espresso query service API \ndirectly. \nFetching transactions confirmed by Espresso \n\nConfirmed transaction data can be obtained from the Espresso DA layer. The query service of \nEspresso nodes provides a variety of API endpoints to fetch finalized blocks, namespaces, and \ntransactions. \nVerifying Espresso consensus \nThe rollup state transition function must ensure that the rollup transactions are confirmed by \nEspresso before executing them. \nThe two main additions to the rollup STF are that the STF must verify that:  \n1. the transactions are exactly the transactions in the rollup’s namespace confirmed as \npart of the finalized Espresso block, and \n2. the Espresso block is part of the canonical Espresso chain. \nThe first check involves verifying Espresso DA's Savoiardi VID proof. To learn more about our VID \nscheme refer to our DA documentation or Appendix A of the Espresso Paper. \nFor the second check, the STF needs to verify the Espresso block Merkle proof and compare \nthat against the proven block Merkle tree commitment in the Espresso light client contract on \nthe L1. \nThe STF must also ensure that no transactions finalized by Espresso are skipped or applied \ntwice. \nArbitrum Nitro integration example \nEspresso's integration with the Arbitrum Nitro stack provides an illustration of how Espresso \nconsensus can be verified in an application's STF.  \nIf the architecture of the rollup mandates that all data is available on a specific DA layer, the \nintegration needs to ensure the necessary data is submitted to that DA layer. In the Espresso-\nArbitrum Nitro integration, a Justification (sometimes called Attestation) contains the \nnamespace and block Merkle proofs necessary for validation of Espresso consensus. The \njustification is submitted to the Nitro parent chain so that it can be used as part of the STF to \nderive the state of the child chain. \nThe Espresso-specific validation for the Arbitrum Nitro STF is performed in the replay binary \nhere. It calls out to the Rust code to verify a namespace and to verify the Espresso block Merkle \nproof. \nIn the Arbitrum Nitro “Arbitrator” smart contract, two additional functions added to the Host IO \ncontract are used to read and validate the HotShot/Espresso commitment from the sequencer \ncontract. Due to Nitro’s stateless STF, the integration also adds the corresponding Host IO \nfunctionality to supply this external piece of data to the STF when validator and staker nodes \nexecute them locally. \nIntegrating Arbitrum Orbit Chain \nArbitrum integration with Espresso \nEspresso has developed an integration with the Arbitrum Nitro tech stack that allows Arbitrum \nOrbit chains to easily integrate with Espresso. The first version of this integration enables \nEspresso confirmations for Orbit chains. The integration will later be updated with functionality \n\nto enable enhanced cross-chain interoperability, new forms of sequencing (i.e., \ndecentralized/shared sequencing), and support for Espresso DA. \nThe first production release of the Espresso-Arbitrum Nitro integration is planned for January \n2025, at which point Arbitrum Orbit chains will simply be able to download a Docker image that \nallows them to deploy onto Espresso (or ask their RaaS provider to do so on their behalf). \nIf you just want to see the steps for getting up and running, you can skip to the guide. \nIntegration overview \nThis integration makes minimal changes to the Arbitrum Nitro stack, and ensures that each \nbatch processed by the rollup is consistent with HotShot-finalized blocks within its namespace. \nTo ensure that the batch has been finalized by HotShot, the following checks are performed: \n1. Namespace validation: Ensure that the set of transactions in a rollup's batch \ncorresponds to the correct namespace. Namespacing allows multiple chains to use \nEspresso’s fast confirmation layer simultaneously by associating each chain’s \ntransactions with a unique namespace within HotShot blocks. \n2. Espresso block Merkle proof check: Confirm that the rollup's batch maps to a valid \nHotShot block. Specifically, verify that the HotShot block associated with a rollup batch \nis a valid leaf in the Merkle tree maintained by the light client contract, which stores the \nstate of HotShot on L1. \nHigh-level flow of integration \n1. The sequencer calls WriteSequencerMsg on the transaction streamer. \n2. The batcher fetches the message from the transaction streamer and submits the \ntransaction to HotShot via the transaction streamer. \n3. The batcher then calls the query API to check if the transaction has been finalized by \nHotShot. \n4. Once the transaction is finalized, the batcher performs batch consistency checks. \n5. The batcher signs the transaction calldata that would be sent to the Sequencer Inbox \ncontract. \n6. The Sequencer Inbox contract is modified to verify the batcher’s signature. \nThis approach involves running a Nitro node with only the batcher enabled, operating in a TEE \nenvironment (such as Intel SGX). The batcher will sign the transaction calldata. In case the TEE \nis broken, the batch poster can't impact the safety of the Orbit chain. It could, however, \ntemporarily halt the chain's progress, thereby breaking liveness. Bridges relying on Espresso \nconfirmations for faster settlement need to trust the TEE as well in this integration. In a future \nupdate, the dependency on TEEs will be removed entirely. \n ",
    "filename": "8. integrating arb orbit chain.pdf"
  },
  {
    "id": 21,
    "content": "\n\nQuickstart with Arbitrum Nitro Rollups \nTL;DR \nThis guide provides a step-by-step approach to deploying a rollup or Arbitrum Orbit chain with \nthe Espresso Network. It emphasizes testing the integration in a controlled environment and \nincludes guidance for mainnet deployment. The instructions cover both local and cloud setups \nusing Docker. \nNote: This guide assumes certain trust conditions and may not be suitable for production. \nBackground \nThe Espresso Network is a confirmation layer that provides chains with information about the \nstate of their own chain and the states of other chains, which is important for cross-chain \ncomposability. Espresso confirmations can be used in addition to the soft confirmations from a \ncentralized sequencer, are backed by the security of the Espresso Network, and are faster than \nwaiting for Ethereum finality (12-15 minutes). \nHow It Works \nIn a regular chain, the transaction lifecycle will look something like this: \n1. A user transacts on an Arbitrum chain. \n2. The transaction is processed by the chain's sequencer, which provides a soft-\nconfirmation to the user, and the transactions are packaged into a block. \n3. The sequencer, responsible for collecting these blocks, compressing, and submitting, \nsubmits the transactions to the base layer. \no If the base layer is Arbitrum One or Ethereum, the transaction will take at least \n12-15 minutes to finalize, or longer, depending on how frequently the sequencer \nposts to the base layer. \no In this transaction lifecycle, the user must trust that the chain's sequencer \nprovided an honest soft-confirmation and will not act maliciously. There are \nlimited ways to verify that the sequencer and batcher acted honestly or did not \ncensor transactions. \nThis reliance on trust is a strong assumption, and it's where the Espresso Network provides \nsignificant benefits. When the chain is integrated with the Espresso Network, the following \nenhancements occur: \n• The sequencer provides a soft confirmation to the user, while the transactions are also \nsent to the Espresso Network to receive a stronger confirmation secured by Byzantine \nFault Tolerance (BFT) consensus. \n• A software component of the sequencer, called the batch poster (henceforth referred to \nas \"batcher\"), operates inside a Trusted Execution Environment (TEE) and must honor \nthe Espresso Network confirmation. It cannot change the ordering or equivocate. \n• This setup provides a strong guarantee that the transaction will ultimately be included \nand finalized by the base layer. \n\nWhile the user must still trust that the chain's sequencer has provided an honest soft \nconfirmation, the Espresso Network offers a stronger confirmation that holds the sequencer \naccountable and prevents it from equivocating or acting maliciously. The initial implementation \nof the batch poster is permissioned, and the user must trust that it will not reorder blocks \nproduced by the sequencer. \nFor a comprehensive overview of how the Espresso Network integrates with your rollup—\nincluding details on the architecture, component interactions, and overall flow—please refer to \nthis integration guide. \nRunning Your Own \nIntegrating with the Espresso Network requires minimal changes to Arbitrum Nitro's existing \nrollup design. The Espresso Team has already done that, and in the following sections we will \nprovide a comprehensive guide for running your own instance and building on Espresso! \nComponents \nWe model the rollup as a collection of three components: \n• The sequencer \n• The batcher \n• The TEE contract (which we mock in this example) \nDeploying The Cloud Arbitrum Orbit Chain \nPlease note that these documents are to facilitate the deployment of a testable instance of the \nArbitrum Orbit Chain with Espresso and should not be assumed to be production-ready \ninfrastructure. \nNote: This guide is based on deploying your on own rollup on Arbitrum Sepolia. A dedicated \nsection at the end of the guide outlines all the modifications needed for deployment on \nArbitrum One. \n0. Install Requisite Dependencies \nEnsure you have Node.js 16, yarn, foundry, git and build-essential tools installed on your system \nbefore proceeding. \n1. Deploy the Contracts \nFirst, clone the contracts repository and set up the development environment: \nCopy \ngit clone https://github.com/EspressoSystems/nitro-contracts.git \ncd nitro-contracts \ngit checkout develop \nInstall the dependencies and build the project (this may take several minutes): \nCopy \nyarn install && forge install \n\nyarn build:all \nCreate a .env file with the following variables: \nCopy \nARBISCAN_API_KEY=\"YOUR_KEY_HERE\" \nDEVNET_PRIVKEY=\"YOUR_PRIVATE_KEY\" \nESPRESSO_TEE_VERIFIER_ADDRESS=\"0x8354db765810dF8F24f1477B06e91E5b17a408bF\" \nYou can get your Arbiscan API key by going to here. You need an account in order to get one. As \nfor the private key, choose any address you want to use as the owner of the rollup. The amount \nrequired to deploy all the rollup contracts is arount 0.15 ETH. \nThis contract above is a mock TEE verifier that will be used to test the rollup by always returning \ntrue for any input. In this guide, we are thus assuming that the batch poster will not act \nmaliciously because it is not operating inside a TEE. \n2. Configure Deployment \nThere is a config.example.ts file in the scripts folder that show you how the config file should \nlook like. There is also a config.template.ts file that you can use to create your own config file. \n1. Rename config.template.ts to config.ts 2.Update the following values in config.ts: \nCopy \nowner: \"OWNER_ADDRESS\", \nchainId: ethers.BigNumber.from('YOUR_CHAIN_ID'), // Update with your desired chain ID \n// Chain configuration \nchainConfig: { \n    \"chainId\": ChainID, // Update with the same chain ID, \n    \"InitialChainOwner\": \"YOUR_OWNED_ADDRESS\", \n} \nvalidators: [\"AN_OWNED_ADDRESS\"], \nbatchPosterAddress: [\"ANOTHER_OWNED_ADDRESS\"], \nbatchPosterManager: \"ANOTHER_OWNED_ADDRESS\", \nImportant Notes: \n• chainId: Ensure that the chainId values in both configuration fields are identical and \nunique. \n• initialChainOwner: The initialChainOwner should be the same as the owner address. \nDon't forget to modify this value at the end of the chain configuration. \n\n• Validators/Stackers: The validators array only requires one address, though you may \nadd more if needed. These addresses need a minimal amount of funds (approximately \n0.00003 ETH ~ ArbSepolia) each time they stake. \n• Batch Poster: The batchPosterAddress and batchPosterManager can be the same, but \nthey should differ from the validators. A very small amount of funds (approximately \n0.00001 ETH ~ ArbSepolia) is required for posting batches. \n3. Run Deployment \nExecute the deployment script: \nCopy \nnpx hardhat run scripts/deployment.ts --network arbSepolia \nNote: You can ignore the message \"env var ESPRESSO_LIGHT_CLIENT_ADDRESS not set...\" - \nthis is only needed for RollupCreator deployment. \nAdd deployed rollup creator address to .env \nThe previous deployment script will output the address of the rollup creator. Add this address to \nyour .env: \nCopy \nROLLUP_CREATOR_ADDRESS=\"DEPLOYED_ADDRESS\" \nDeploy the Rollup Proxy Contract \nCopy \nnpx hardhat run scripts/createEthRollup.ts --network arbSepolia \nNote: You can keep the terminal opened with the logged addresses and the block number. This \nis the easiest way to find the deployed contract addresses when configuring the chain in the \nlatest sections of this guide. \nFind the Deployed Contract Addresses \nTo find the addresses of your deployed contracts: \n1. Go to Arbiscan Sepolia (or Arbiscan for mainnet) \n2. Search for the address associated with your DEVNET_PRIVKEY from the .env file \n3. In the transactions list, look for a transaction with the method name Create Rollup \n4. Click on the transaction to view its details \n5. Click on the \"Logs(x)\" tab and scroll down to the bottom of the page to find all deployed \ncontract addresses. \n          Note: You can also find most of the contract addresses in espresso-\ndeployments/arbSepolia.json. The upgrade executor contract address from this JSON file will be \nneeded for the next section. \nConfiguring and Running the Chain \n\nThe docker configuration can be found in the espresso-build-something-real repository. \n1. Clone and Configure the Repository \nCopy \ngit clone https://github.com/EspressoSystems/espresso-build-something-real \ncd espresso-build-something-real \n2. Update Configuration Files \nYou'll need to modify two configuration files with the deployment addresses, keys, ids and rpc \nurl from the previous steps: \nFiles to update: \n• config/full_node.json \n• config/l2_chain_info.json \nRequired updates: \n• In config/l2_chain_info.json: \no Set chainId under chain-config to match your rollup's chain ID \no Set InitialChainOwner to the address of the rollup owner \no Set the rollup smart contract addresses from the previous deployment \no Update deployed-at to the block number where rollup proxy was created \n• In config/full_node.json: \no Add the rpc provider's arbitrum URL with your API key to the url field (e.g., infura, \nalchemy, etc.) \no Set id under chain to match your rollup's chain ID \no Update private-key for both stacker (validator) and batch poster addresses \nNote: Make sure you do not push your private keys to your repository if it is public. Or use \nenvironment variables to store your private keys. \n3. Run the Chain \nFor those seeking to evaluate their infrastructure and to get a clearer picture of what a \"working\" \nimplementation looks like, we have made available a Docker Compose configuration for local \ndevelopment. The configuration included in this repository is ready to use as is – it will run your \nrollup locally. For cloud deployment details, see the Cloud Configuration section at the end of \nthe guide. \nCopy \nversion: '2.2' \nservices: \n\n  nitro: \n    image: ghcr.io/espressosystems/nitro-espresso-integration/nitro-node:integration \n    container_name: nitro-node \n    ports: \n      - \"8547:8547\" \n      - \"8548:8548\" \n      - \"8549:8549\" \n    command: --conf.file /config/full_node.json \n    volumes: \n      - ./config:/config \n      - ./wasm:/home/user/wasm/ \n      - ./database:/home/user/.arbitrum \n    depends_on: \n      - validation_node \n \n  validation_node: \n    image: ghcr.io/espressosystems/nitro-espresso-integration/nitro-node:integration \n    container_name: validation_node \n    ports: \n      - \"8949:8549\" \n    volumes: \n      - ./config:/config \n    entrypoint: /usr/local/bin/nitro-val \n    command: --conf.file /config/validation_node_config.json \nStart the chain using Docker: \nCopy \ndocker compose up -d \nUnderstanding the Startup Process \nDuring startup, you'll see various logs and warnings. Here's what to expect and how to interpret \nthem: \n1. Initial Staker Warnings \n\nCopy \nWARN [02-11|18:54:23.102] error acting as staker \nerr=\"error advancing stake from node 5: block validation is still pending\" \nWARN [02-11|18:55:00.911] Large gap between last seen and current block number, skipping \ncheck for reverts  \nlast=122,990,253 current=122,990,485 \nThis is normal, this means that staker doesnt have any new nodes to stake on. \n2. Batch Validation Process \nCopy \nINFO [02-11|19:33:42.369] Batch validation status: hasBatchBeenValidated=false \nINFO [02-11|18:55:52.264] Fetching Merkle Root at hotshot height=1,173,100 \nThese logs show the batch poster working to validate and process transactions. \n3. Successful Batch Processing When you see the following logs, it indicates successful \nbatch processing: \nCopy \nINFO [02-11|19:36:28.868] Batch validation status: hasBatchBeenValidated=true \nINFO [02-11|19:36:29.688] DataPoster sent transaction \nINFO [02-11|19:36:29.691] BatchPoster: batch sent \nImportant Notes: \no Update to Batch posting can take from 1-30 minutes after a user has sent the \ntransaction. \no Verify successful operation by checking the sequencerInbox contract on the \nArbitrum Sepolia explorer. \no Occasional staker warnings occur when there are no new nodes to stake on. \n4. RPC Rate Limiting Issues \nYou may encounter errors indicating you've exceeded the RPC provider's rate limits: \nCopy \n2025-02-27 11:12:54 INFO [02-27|16:12:54.607] rpc response method=eth_getLogs logId=199 \nerr=\"Too Many Requests\" result=null \n2025-02-27 11:12:54 WARN [02-27|16:12:54.609] error reading inbox err=\"Too Many Requests\" \nThese errors can occur when resyncing the entire rollup history after deleting the database \nfolder or when using an RPC provider with strict rate limits. \nSolutions: \n\no Adjust polling intervals in configuration files to reduce request frequency \no Use one provider for initial sync, then switch to another for ongoing operations \no Upgrade to a higher tier with your RPC provider \n4. Testing the Chain \nTo verify your chain is running correctly: \n1. Check Confirmed Nodes by the Validator/Staker \nCopy \ncast call --rpc-url https://arbitrum-sepolia-rpc.publicnode.com \nYOUR_ROLLUP_PROXY_ADDRESS \"latestConfirmed()(uint256)\" \n1. Test bridge functionality: \nCopy \ncast send --rpc-url https://arbitrum-sepolia-rpc.publicnode.com \nYOUR_INBOX_CONTRACT_ADDRESS 'depositEth() external payable returns (uint256)' --private-\nkey YOUR_PRIVATE_KEY  --value 10000000000 -vvvv \nNote: Bridging transactions can take up to 15 minutes to finalize. \n1. Verify your balance: \nCopy \ncast balance YOUR_PRIVATE_KEY_PUBLIC_ADDRESS --rpc-url http://127.0.0.1:8547 \n1. Test sending transactions: \nCopy \ncast send ANY_ADDRESS --value 1 --private-key YOUR_PRIVATE_KEY_WITH_FUNDS --rpc-url \nhttp://127.0.0.1:8547 \nFor a more consistent test, you can also continuously send transactions to the rollup. This \napproach simulates a more realistic environment by continually submitting transactions, \nallowing you to see how the system handles ongoing activity. (See the next section for details.) \n1. Check recipient balance: \nCopy \ncast balance ANY_ADDRESS --rpc-url http://127.0.0.1:8547 \nIf successful, the recipient's balance should show 1 wei or the amount you sent if different. \nTransaction Flow Generator \nIf you want to generate test transactions on your rollup, navigate to the tx-generator repository \nsubfolder and follow the README instructions: \nCopy \n\ncd tx-generator \nThis script continuously generates transactions to help you evaluate your rollup and the \nEspresso Network. \nHotshot Query Tool \nYou can also use this project in conjunction with the transaction generator to verify that the \ntransactions you generate are properly submitted to Hotshot. By inputting the correct chain ID \nin the config, the Hotshot Query Tool—a simple Go project—fetches and prints namespace \ntransactions from the Hotshot query service. This tool sends HTTP requests and can be easily \nadapted for other API endpoints as needed. \nDeploying Your Rollup on Mainnet \nTo deploy your rollup on Arbitrum mainnet, update your configuration files with the appropriate \nparameters and follow the guide. Below are the key changes you need to make, along with \nreferences to the relevant sections of this guide: \n• TEE Verifier Address: Set the mock Espresso TEE verifier address to: \no Mainnet: 0xE68c322e548c3a43C528091A3059F3278e0274Ed \no Testnet: 0x8354db765810dF8F24f1477B06e91E5b17a408bF Refer to Deploy the \nContracts. \n• Network Selection: Change the network in the nitro-contracts repository from \narbSepolia to arb1 when running smart contract and deployment scripts. Refer to Run \nDeployment. \n• Batch Poster Settings: Update the batch-poster configuration in the \nconfig/full_node.json file by: \no Changing the hotshot URL to https://query.main.net.espresso.network/v0 \no Setting the light-client address to \n0x61f627c6785503b6d83e4d59611af7361210ae64. Refer to Update \nConfiguration Files. \n• Parent Chain ID: In l2_chain_info.json, change the parent-chain-id from 421614 to \n42161 and optionally adjust the chain-name. Refer to Update Configuration Files. \n• RPC Endpoint: \no In full_node.json, update the RPC URL to the mainnet endpoint. \no When testing, change the RPC URL from https://arbitrum-sepolia-\nrpc.publicnode.com to https://arbitrum-one-rpc.publicnode.com. Refer to both \nUpdate Configuration Files and Testing the Chain. \nApply these modifications to ensure your rollup is properly configured for mainnet deployment. \nCloud Configuration \nInfo: If you want to get up and running with this and you already know about cloud \nconfiguration, you can take the docker compose file and modify it as needed. If you aren't \n\nfamiliar with cloud environments, read on. (Note: This setup is for AWS, but it should work with \nany cloud.) \nBooting A Chain on EC2 \nThe first step is to launch an ec2 instance, which is a simple process. First, go into the console \nand either search for ec2, or select it from the quick select if you've used it before. \n \nThe AWS Home Page \nFrom here, we can configure the EC2 launch configuration. You can leave everything default, but \nfeel free to change the settings if you've done this before. Under Instance Type you can select \nt3.medium or t3.large, but any cloud instance with at least 4 gigabytes of RAM and 2 CPU cores \nshould be sufficient. \n \n\nInfo: Please note that in our testing t3.medium seems to meet the requirements, but if you \nencounter instability, you might need to upgrade to the larger instance. \nFrom here, make sure you configure your key pair, otherwise you will not have SSH access to the \nmachine. You can use the auto generated security group for the instance. Make sure you allow \nSSH traffic as well. The following image should mostly reflect your configuration: \n \nSecurity Note: Keep your key pair secure and do not share it with others. Ensure that the \npermissions on your key file are set to be readable only by you (e.g., chmod 400 your-key.pem). \nSetting up CloudWatch logging (Optional) \nIf you want to have access to the logs of your rollup without having to each time SSH into the \ninstance, you can enable CloudWatch logging and send the logs to a CloudWatch log group. You \nwill then be able to view the logs in the CloudWatch console. Follow the steps below to enable \nthis: \n1. Create IAM Role: \n• In the advanced details section, create a new IAM profile to enable logging if you don't \nhave one already. \n\n \n• Click on Create role then select EC2 as the use case in the next screen. \n \n• Click \"Next\" then select the AmazonAPIGatewayPushToCloudWatchLogs policy in the \nnext screen. \n\n \n\n• Name your role in the next screen (e.g., \"CloudWatchLogsRole\") and create the role. \n1. Attach Role to EC2: \n• Return to the EC2 instance creation screen. \n• In Advanced settings, select the role you just created and click Launch instance. \n1. Create Log Group: \no Navigate to the CloudWatch console and create two log groups for your rollup: \n\n \n\no You can name them both nitro-node-logs and validation-node-logs or something \nsimilar. This is the name we give them in the docker-compose.yml file at the \nfollowing step. \n2. Update Docker Configuration: Modify your docker-compose.yml to include \nCloudWatch logging: \nCopy \nversion: '2.2' \nservices: \n  nitro: \n    image: ghcr.io/espressosystems/nitro-espresso-integration/nitro-node:integration \n    container_name: nitro-node \n    ports: \n      - \"8547:8547\" \n      - \"8548:8548\" \n      - \"8549:8549\" \n    command: --conf.file /config/full_node.json \n    volumes: \n      - ./config:/config \n      - ./wasm:/home/user/wasm/ \n      - ./database:/home/user/.arbitrum \n    depends_on: \n      - validation_node \n    # ===== CloudWatch Configuration Start ===== \n    logging: \n      driver: \"awslogs\" \n      options: \n        awslogs-region: \"us-east-2\" # Update to your EC2 instance's region \n        awslogs-group: \"nitro-node-logs\" \n        awslogs-stream: \"nitro-node\" \n    # ===== CloudWatch Configuration End ===== \n \n  validation_node: \n\n    image: ghcr.io/espressosystems/nitro-espresso-integration/nitro-node:integration \n    container_name: validation_node \n    ports: \n      - \"8949:8549\" \n    volumes: \n      - ./config:/config \n    entrypoint: /usr/local/bin/nitro-val \n    command: --conf.file /config/validation_node_config.json \n    # ===== CloudWatch Configuration Start ===== \n    logging: \n      driver: \"awslogs\" \n      options: \n        awslogs-region: \"us-east-2\" # Update to your EC2 instance's region \n        awslogs-group: \"validation-node-logs\" \n        awslogs-stream: \"validation-node\" \n    # ===== CloudWatch Configuration End ===== \n          Note: Make sure to update the awslogs-region to match your EC2 instance's region. \nPreparing Your Environment \n1. Move Key to .ssh Folder: \no Move your key from the downloads folder or any other folder to the .ssh folder: \nCopy \nmv ~/Downloads/'your-key.pem' ~/.ssh/ \n2. Test Your Connection: \no Use ping to test your connection with the IPv4 address: \nCopy \nping 'your-host' \no If you encounter \"Connection refused\" or \"Communication prohibited by filter,\" \ntry using a different network, like mobile data. \n3. Connect to Your EC2 Instance: \no Run this command to connect to your EC2 instance using your key: \nCopy \n\nssh -i ~/.ssh/'your-key.pem' ec2-user@'your-host' \nNote: Replace your-host with either the public DNS or IP address of your EC2 instance. \nTransferring Files to Your EC2 Instance \n1. Create Config Folder: \no On another terminal, run the following to create a config folder on your instance: \nCopy \nssh -i ~/.ssh/'your-key.pem' ec2-user@'your-host' \"mkdir -p ~/rollup/config\" \nNote: For this first step, you can also run the mkdir command from the instance terminal. \n2. Move Config Files to Instance: \no From the root of your git repo, transfer your config files to the instance's config \nfolder: \nCopy \nscp -i ~/.ssh/'your-key.pem' config/* ec2-user@'your-host':~/rollup/config/ \n3. Move Docker File to Instance: \no Similarly, transfer the Docker Compose file to your instance: \nCopy \nscp -i ~/.ssh/'your-key.pem' docker-compose.yml ec2-user@'your-host':~/rollup/ \nConfiguring Docker \nFrom inside your instance, install docker with the following steps: \nCopy \nsudo yum update -y && \\ \n    sudo yum install -y docker && \\ \n    sudo service docker start && \\ \n    sudo usermod -aG docker ec2-user \nYou can now log out and back in, and your user, ec2-user, should have Docker access without \nneeding sudo. The last thing we need is Docker Compose. Unfortunately, at the time of writing, \nAmazon Linux 2023 has an older distribution of Docker, which does not yet support the \ncompose subcommand. To access Docker Compose, you need to download it by executing the \nfollowing steps: \nNote: Before continuing, exit and reconnect to your instance by running exit in the terminal. \nCopy \n# First, pull docker compose \n\nsudo curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-\n$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose \n \n# Then, add execute permissions \nsudo chmod +x /usr/local/bin/docker-compose \n \n# Verify \ndocker-compose version \nRunning Docker Compose \n1. Connect to EC2 Instance: \no Return to the terminal connected to your EC2 instance, or reconnect if you have \nlogged out. You can find the connection command in the Preparing Your \nEnvironment section. \n2. Run Docker Compose: \no From the rollup folder, execute the following command to start your services: \nCopy \ndocker-compose up \nNote: You can add the -d flag to run the containers in the background. \n3. Handle Permission Errors: \no If you encounter a permission error like: \nCopy \nnitro-node | Fatal configuration error: unable to create chain directory: mkdir \n/home/user/.arbitrum/local: permission denied \no Create the necessary folders and set permissions: \nCopy \nmkdir -p database wasm \nsudo chown -R 1000:1000 database wasm \no This gives Docker container's user permission to write to these directories. \nTesting the Connection \nYou can now that the connection to your rollup by for example checking your balance: \nCopy \ncast balance 'your-address' --rpc-url http://your-host:8547 ",
    "filename": "9. quickstart arb rollup.pdf"
  }
]